{
    "summary": "Lambda Reserved Cloud is now available with the new NVIDIA GH200 Grace Hopper\u2122\nSuperchip. Learn more\n\n  * Cloud Show submenu for Cloud\n\n    * __Cloud Sign-In\n    * __On Demand Cloud\n    * __Reserved Cloud\n  * Datacenter Show submenu for Datacenter\n\n    * **Echelon Clusters** Large scale GPU clusters designed for AI. GPUs, storage, and InfiniBand networking.\n    * **Hyperplane Server** NVIDIA Tensor Core GPU server with up to 8x A100 or H100 GPUs, NVLink, NVSwitch, and InfiniBand.\n    * **Scalar Server** PCIe server with up to 8x customizable NVIDIA Tensor Core GPUs and dual Xeon or AMD EPYC processors.\n    * **NVIDIA DGX Systems** NVIDIA's latest generation of infrastructure for enterprise AI.\n    * **NVIDIA GH200** Breakthrough design that forms a high-bandwidth connection between the NVIDIA Grace\u2122 CPU and Hopper\u2122 GPU.\n    * **NVIDIA H100 & H200**New, next-generation Tensor Core GPUs based on the Hopper architecture.\n    * **GPU Colocation** Data center colocation for your GPU cluster.\n  * Desktops Show submenu for Desktops\n\n    * **Vector GPU Workstation** Lambda's GPU workstation designed for AI. Up to four NVIDIA GPUs.\n    * **Vector One GPU Desktop** Lambda's single GPU desktop. Configured with a single NVIDIA RTX 4090.\n    * **Tensorbook GPU Laptop** Lambda's portable GPU laptop. Beautifully designed in white aluminum by RAZER.\n  * Company Show submenu for Company\n\n    * About\n    * Careers\n    * Professional Services\n    * Partners\n  * Resources Show submenu for Resources\n\n    * __GPU Benchmarks\n    * __Blog\n    * __Lambda Stack\n    * __Documentation\n    * __Forum\n    * __Research\n    * __Technical Support\n  * __\n\nOpen main navigation Close main navigation\n\n  * Cloud Show submenu for Cloud\n\n    * Cloud Sign-In\n    * On-Demand Cloud\n    * Reserved Cloud\n  * Datacenter Show submenu for Datacenter\n\n    * Echelon Clusters\n    * Hyperplane Server\n    * NVIDIA DGX Systems\n    * Scalar Server\n    * Colocation\n    * NVIDIA GH200\n    * NVIDIA H100 & H200\n  * Desktops Show submenu for Desktops\n\n    * Vector Workstation\n    * Vector One Desktop\n    * Tensorbook Laptops\n  * Company Show submenu for Company\n\n    * About\n    * Careers\n    * Professional Services\n    * Partners\n  * Resources Show submenu for Resources\n\n    * GPU Benchmarks\n    * Blog\n    * Lambda Stack\n    * Documentation\n    * Forum\n    * Research\n  * Support\n  * +1 (866) 711-2025 \n\n__\n\n+1 (866) 711-2025\n\n**Hyperplane**\n\n  * Overview\n  * Support\n\n  * Overview\n  * Support\n\nGet pricing\n\nLambda Cloud GPUs from $0.50/hour ___\n\n#  The world's fastest server for AI research\n\n### Now available with  \nNVIDIA H100 Tensor Core GPUs\n\nGet pricing\n\n10,000+ research teams trust Lambda\n\nNOW AVAILABLE\n\n##  Lambda Reserved Cloud with NVIDIA H100 GPUs and AMD EPYC 9004 series CPUs\n\n#### NOW AVAILABLE\n\nLambda\u2019s Hyperplane HGX server, with NVIDIA H100 GPUs and AMD EPYC 9004 series\nCPUs, is now available for order in Lambda Reserved Cloud, starting at $1.89\nper H100 per hour! By combining the fastest GPU type on the market with the\nworld\u2019s best data center CPU, you can train and run inference faster with\nsuperior performance per dollar.\n\nReserve now\n\n###\n\n## Reserve Your Cloud Cluster\n\nA member from our team will be in touch.\n\nFirst name*\n\nLast name*\n\nBusiness Email*\n\nPhone number*\n\nCompany name*\n\nPlease select an _estimated_ number of H100 GPUs you would like to reserve.\n\nNumber of H100 GPUs\n\n1x8x16x32x64x128x248x504x1016x2040x>2040x\n\nForm Name\n\nLast Offer Source\n\nutm_source\n\nutm_medium\n\nutm_campaign\n\nutm_content\n\nProducts\n\nPrimary Product Interest\n\nLast Quote ID\n\nquote_pdf_url\n\nquote_amt\n\nproduct_name\n\nproduct_description\n\nLead source\n\nProduct Type\n\nBy clicking submit below, you consent to allow Lambda to store and process the\npersonal information submitted above to provide you the content requested.\nPlease review our privacy policy for more information.\n\n  * I agree to receive other communications from Lambda .\n\nSpec Highlights\n\n##  Engineered for your workload\n\nTell us about your research and we'll design a machine that's perfectly\ntailored to your needs.\n\nTalk to an engineer\n\n###\n\n## Talk to an engineer\n\nFill out the form below and we'll be in touch shortly.\n\nFirst name*\n\nLast name*\n\nBusiness Email*\n\nPhone number*\n\nCompany name*\n\nPrimary Product Interest*\n\nCloud Cluster (Rsv) - GH200 OnlyCloud Cluster (Rsv) - H200 OnlyReserved Cloud\nClusterColocation ServicesEchelon - GPU HPC ClusterHyperplane - H100 SXM\nServerLambda GPU CloudNVIDIA DGX\u2122 Systems with LambdaScalar - H100 PCle\nServerVector - GPU WorkstationVector One - GPU Workstation\n\nJob Role*\n\nAlgorithm EngineerAI ConsultantAI EngineerAI ResearcherComputer Vision\nEngineerDeep Learning EngineerDeep Learning ResearcherDirector of AIDirector\nof Machine LearningHead of AIHead of Machine LearningIT System\nAdministratorMachine Learning ConsultantMachine Learning EngineerMachine\nLearning ResearcherManager of AIManager of Machine LearningNatural Language\nProcessing EngineerSenior Machine Learning EngineerVP of Artificial\nIntelligenceVP of Machine LearningChief AI OfficerChief ML OfficerCIOOther\n\nContact Us Reasons\n\nForm Name\n\nLast Offer Source\n\nutm_source\n\nutm_medium\n\nutm_campaign\n\nutm_content\n\nLead source\n\nBy clicking submit below, you consent to allow Lambda to store and process the\npersonal information submitted above to provide you the content requested.\nPlease review our privacy policy for more information.\n\n  * I agree to receive other communications from Lambda .\n\n  | Hyperplane A100 | Hyperplane H100  \n---|---|---  \n**GPUs** |  4x or 8x A100 80GB SXM4 | 8x H100 80GB SXM5  \n**CPUs** | Up to 128 cores and 256 threads | Up to 192 cores and 384 threads  \n**Memory** | Up to 8 TB DDR4 | Up to 8 TB DDR5  \n**Storage** | Up to 184 TB of NVMe SSDs | Up to 492 TB of NVMe SSDs  \n**Networking** | Up to 200 Gb/s | Up to 400 Gb/s  \n  \nEchelon Clusters\n\n##  Easily scale from server to cluster\n\nAs your team's compute needs grow, Lambda's in-house HPC engineers and AI\nresearchers can help you integrate Hyperplane and Scalar servers into GPU\nclusters designed for deep learning.\n\n  * __\n\nCompute\n\nScaling to 1000s of GPUs for distributed training or hyperparameter\noptimization.\n\n  * __\n\nStorage\n\nHigh-performance parallel file systems optimized for ML.\n\n  * __\n\nNetworking\n\nCompute and storage fabrics for GPUDirect RDMA and GPUDirect Storage.\n\n  * __\n\nSoftware\n\nFully integrated software stack for MLOps and cluster management.\n\nLearn More\n\nLambda Premium Support\n\n##  Service and support by technical experts who specialize in machine\nlearning\n\nLambda Premium Support includes:\n\n  * Up to 5 year extended warranty with advanced parts replacement\n  * Live technical support from Lambda's team of ML engineers\n  * Support for ML software included in Lambda Stack: PyTorch\u00ae, Tensorflow, CUDA, CuDNN, and NVIDIA Drivers\n\nLearn more\n\nLambda Stack\n\n##  Plug in. Start training.\n\nOur servers include Lambda Stack, which manages frameworks like PyTorch\u00ae and\nTensorFlow. With Lambda Stack, you can stop worrying about broken GPU drivers\nand focus on your research.\n\n  * __\n\nZero configuration required\n\nAll your favorite frameworks come pre-installed.\n\n  * __\n\nEasily upgrade PyTorch\u00ae and TensorFlow\n\nWhen a new version is released, just run a simple upgrade command.\n\n  * __\n\nNo more broken GPU drivers\n\nDrivers will \"just work\" and keep compatible with popular frameworks.\n\nLearn More\n\nColocation\n\n##  Your servers. Our datacenter.\n\nLambda Colocation makes it easy to deploy and scale your machine learning\ninfrastructure. We'll manage racking, networking, power, cooling, hardware\nfailures, and physical security. Your servers will run in a Tier 3 data center\nwith state-of-the-art cooling that's designed for GPUs. You'll get remote\naccess to your servers, just like a public cloud.\n\nLearn more\n\n__\n\nFast support\n\nIf hardware fails, our on-premise data center engineers can quickly debug and\nreplace parts.\n\n__\n\nOptimal performance\n\nOur state-of-the-art cooling keeps your GPUs cool to maximize performance and\nlongevity.\n\n__\n\nHigh availability\n\nOur Tier 3 data center has redundant power and cooling to ensure your servers\nstay online.\n\n__\n\nNo network set up\n\nWe handle all network configuration and provide you with remote access to your\nservers.\n\nResearch\n\n## Explore our research\n\nView all research __\n\nICCV 2021\n\n### Multiple Pairwise Ranking Networks for Personalized Video Summarization\n\nWe propose a model for personalized video summaries by conditioning the\nsummarization process with predefined categorical labels.\n\nLearn more __\n\nICCV 2019\n\n### HoloGAN: Unsupervised Learning of 3D Reps. from Natural Images\n\nWe propose a novel generative adversarial network (GAN) for the task of\nunsupervised learning of 3D representations from natural images.\n\nLearn more __\n\nNeurIPS 2018\n\n### RenderNet: A Deep ConvNet for Differentiable Rendering from 3D Shapes\n\nWe present a differentiable rendering convolutional network with a novel\nprojection unit that can render 2D images from 3D shapes.\n\nLearn more __\n\nSIGGRAPH Asia 2019\n\n### Adversarial Monte Carlo Denoising with Conditioned Aux. Feature Modulation\n\nWe demonstrate that GANs can help denoiser networks produce more realistic\nhigh-frequency details and global illumination.\n\nLearn more __\n\nTech Specs\n\n## Technical Specifications\n\nHyperplane 4-A100  Hyperplane 8-A100  Hyperplane 8-H100\n\n###  GPUs\n\n4 NVIDIA\u00ae A100 SXM4 GPUs (80 GB) \u00b7 NVLink and PCIe 4.0 GPU-to-GPU interconnect\n\n  * 4 NVIDIA A100 SXM4 GPUs (80 GB)\n  * NVLink and PCIe 4.0 GPU-to-GPU interconnect\n\n###  Processors\n\nTwo AMD EPYC\u2122 or Intel Xeon Processors \u00b7 AMD EPYC 7003 (Milan) Series\nProcessors with up to 112 cores...\n\n2 AMD EPYC\u2122 or Intel Xeon Processors\n\n  * **AMD EPYC 7003** (Milan) Series Processors with up to 112 cores total\n  * **Intel Xeon 3rd Gen** (Ice Lake) Scalable Processors with up to 80 cores total\n\n###  System memory\n\nUp to 8 TB of 3200 MHz DDR4 ECC RAM in 32 DIMM slots\n\n  * Up to 8 TB of 3200 MHz DDR4 ECC RAM in 32 DIMM slots\n\n###  Storage\n\nUp to 122.88 TB of storage via 4 hot-swappable U.2 NVMe SSDs...\n\n  * Up to 122.88 TB of storage via 4 hot-swappable U.2 NVMe SSDs\n\n###  Networking\n\n2 RJ45 10 Gbps (aggregate) BASE-T LAN ports \u00b7 1 RJ45 1 Gbps BASE-T LAN out-of-\nband management port...\n\n**Built-in networking:**\n\n  * 2 RJ45 10 Gbps (aggregate) BASE-T LAN ports\n  * 1 RJ45 1 Gbps BASE-T LAN out-of-band management port\n\n**Up to 4 high-speed NICs for GPUDirect RDMA. Options include:**\n\n  * NVIDIA ConnectX-6 200 Gb/s HDR InfiniBand/VPI Adapter, QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-6 100 Gb/s HDR100 InfiniBand/VPI Adapter, 1x QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-6 Dx EN 200 Gb/s Ethernet Adapter, QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-6 Dx EN 100 Gb/s Ethernet Adapter, QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-5 EN 100 Gb/s Ethernet Adapter, QSFP28, PCIe 3.0 x16\n\n###  Power\n\nTwo hot-swappable 2200W 80 PLUS\u2122 Platinum PSUs \u00b7 1 + 1 redundancy...\n\n  * 2 hot-swappable 2200W 80 PLUS\u2122 Platinum PSUs\n  * 1 + 1 redundancy\n\n**AC input:**  \n\n  * 220-240Vac / 11-10A / 50-60 Hz\n\n**Max output per PSU:**  \n\n  * 220-240Vac / 2200 watts\n\n###  Front I/O\n\nPower button and LED \u00b7 Reset button \u00b7 Information LED \u00b7 LAN activity LED...\n\n  * Power button and LED\n  * Reset button\n  * Information LED\n  * LAN activity LED (one for each port)\n  * HDD LED\n  * Power fail LED\n\n###  Rear I/O\n\nUID button and LED \u00b7 VGA port \u00b7 2 USB 3.0 ports \u00b7 2 RJ45 10 Gbps (aggregate)\nBASE-T LAN ports...\n\n  * UID button and LED\n  * VGA port\n  * 2 USB 3.0 ports\n  * 2 RJ45 10 Gbps (aggregate) BASE-T LAN ports\n  * 1 RJ45 1 Gbps BASE-T LAN out-of-band management (IPMI) port\n\n###  Accessories\n\nRackmounting kit \u00b7 2 configurable C19 power cables\n\n  * Rackmounting kit\n  * 2 configurable C19 power cables\n\n###  Physical\n\nForm factor: 2U rackmount \u00b7 Width: 17.2 inches (437 mm) \u00b7 Height: 3.5 inches\n(89 mm) \u00b7 Depth: 32.7 inches (830.3 mm)...\n\n  * **Form factor:** 2U rackmount\n  * **Width:** 17.2 inches (437 mm)\n  * **Height:** 3.5 inches (89 mm)\n  * **Depth:** 32.7 inches (830.3 mm)\n  * **Server weight:** 78.5 lbs (35.6 kg)\n  * **Rackmounting kit weight:** 5 lbs (2.3 kg)\n  * **Total weight with packaging:** 88.5 lbs (40.1 kg)\n\n###  GPUs\n\n8 NVIDIA A100 SXM4 GPUs (80 GB) \u00b7 NVLink and NVSwitch GPU-to-GPU fabric\n\n  * 8 NVIDIA\u00ae A100 SXM4 GPUs (80 GB)\n  * NVLink and NVSwitch GPU-to-GPU fabric\n\n###  Processors\n\nTwo AMD EPYC\u2122 or Intel Xeon Processors \u00b7 AMD EPYC 7003 (Milan) Series\nProcessors with up to 112 cores...\n\n2 AMD EPYC\u2122 or Intel Xeon Processors\n\n  * **AMD EPYC 7003** (Milan) Series Processors with up to 112 cores total\n  * **Intel Xeon 3rd Gen** (Ice Lake) Scalable Processors with up to 80 cores total\n\n###  System memory\n\nUp to 8 TB of 3200 MHz DDR4 ECC RAM in 32 DIMM slots\n\n  * Up to 8 TB of 3200 MHz DDR4 ECC RAM in 32 DIMM slots\n\n###  Storage\n\nUp to 7.68 TB of storage via 2 onboard M.2 NVMe SSDs...\n\n  * Up to 7.68 TB of storage via 2 onboard M.2 NVMe SSDs\n  * Up to 184.32 TB of storage via 6 hot-swappable U.2 NVMe SSDs\n\n###  Networking\n\n2 RJ45 10 Gbps BASE-T LAN ports \u00b7 1 RJ45 1 Gbps BASE-T LAN out-of-band\nmanagement port...\n\n**Built-in networking:**\n\n  * 2 RJ45 10 Gbps BASE-T LAN ports\n  * 1 RJ45 1 Gbps BASE-T LAN out-of-band management port\n\n**Up to 9 high-speed NICs for GPUDirect RDMA and Storage. Options include:**\n\n  * NVIDIA ConnectX-6 200 Gb/s HDR InfiniBand/VPI Adapter, QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-6 100 Gb/s HDR100 InfiniBand/VPI Adapter, 1x QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-6 Dx EN 200 Gb/s Ethernet Adapter, QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-6 Dx EN 100 Gb/s Ethernet Adapter, QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-5 EN 100 Gb/s Ethernet Adapter, QSFP28, PCIe 3.0 x16\n\n###  Power\n\nFour hot-swappable 3000W 80 PLUS\u2122 Titanium PSUs \u00b7 2 + 2 redundancy...\n\n  * 4 hot-swappable 3000W 80 PLUS\u2122 Titanium PSUs\n  * 2 + 2 redundancy\n\n**AC input:**\n\n  * 210-240Vac / 16-14.5A / 50-60 Hz\n\n**Max output per PSU:**\n\n  * 210-240Vac / 3000 watts  \n  \n\n###  Front I/O\n\nPower button and LED \u00b7 UID button and LED \u00b7 BMC reset button \u00b7 Information LED\n\u00b7 LAN activity LED...\n\n  * Power button and LED\n  * UID button and LED\n  * BMC reset button\n  * Information LED\n  * LAN activity LED\n  * VGA port\n  * 2 USB 3.0 ports\n  * 1 RJ45 1 Gbps BASE-T LAN out-of-band management (IPMI) port\n\n###  Rear I/O\n\n2 RJ45 10 Gbps BASE-T LAN ports\n\n  * 2 RJ45 10 Gbps BASE-T LAN ports\n\n###  Accessories\n\nRackmounting kit \u00b7 4 configurable C19 power cables\n\n  * Rackmounting kit\n  * 4 configurable C19 power cables\n\n###  Physical\n\nForm factor: 4U rackmount \u00b7 Width: 17.6 inches (446 mm) \u00b7 Height: 6.9 inches\n(174 mm) \u00b7 Depth: 35.4 inches (900 mm)...\n\n  * **Form factor:** 4U rackmount\n  * **Width:** 17.6 inches (446 mm)\n  * **Height:** 6.9 inches (174 mm)\n  * **Depth:** 35.4 inches (900 mm)\n  * **Server weight:** 166 lbs (75.3 kg)\n  * **Rackmounting kit weight:** 5 lbs (2.3 kg)\n  * **Total weight with packaging:** 225 lbs (102.1 kg)\n\n\n\n###  GPUs\n\n8 NVIDIA\u00ae H100 SXM5 GPUs (80 GB) \u00b7 NVLink and NVSwitch GPU-to-GPU fabric\n\n  * 8 NVIDIA\u00ae H100 SXM5 GPUs (80 GB)\n  * NVLink and NVSwitch GPU-to-GPU fabric\n\n###  Processors\n\nTwo AMD EPYC\u2122 or Intel Xeon Processors \u00b7 AMD EPYC 7004 & 9004 (Genoa &\nBergamo) Series Processors with up to 256 cores...\n\n2 AMD EPYC\u2122 or Intel Xeon Processors\n\n  * **AMD EPYC 7004 & 9004** (Genoa & Bergamo) Series Processors with up to 256 cores\n  * I **ntel Xeon 4th Gen** (Sapphire Rapids) Scalable Processors with up to 112 cores total\n\n###  System memory\n\nUp to 8 TB of 4800 MHz DDR5 ECC RAM in 32 DIMM slots\n\n  * Up to 8 TB of 4800 MHz DDR5 ECC RAM in 32 DIMM slots\n\n###  Storage\n\nUp to 7.68 TB of storage via 2 onboard M.2 NVMe SSDs \u00b7 Up to 491.52 TB of\nstorage...\n\n  * Up to 7.68 TB of storage via 2 onboard M.2 NVMe SSDs\n  * Up to 491.52 TB of storage via 16 hot-swappable U.2 NVMe SSDs\n\n###  Networking\n\n2 RJ45 10 Gbps BASE-T LAN ports \u00b7 1 RJ45 1 Gbps BASE-T LAN out-of-band\nmanagement port\n\n**Built-in networking:**\n\n  * 2 RJ45 10 Gbps BASE-T LAN ports\n  * 1 RJ45 1 Gbps BASE-T LAN out-of-band management port\n\n**Up to 9 high-speed NICs for GPUDirect RDMA and Storage. Options include:**\n\n  * NVIDIA ConnectX-7 400 Gb/s NDR InfiniBand Adapter, OSFP56, PCIe 5.0 x16\n  * NVIDIA ConnectX-7 200 Gb/s NDR200 InfiniBand Adapter, OSFP56, PCIe 5.0 x16\n  * NVIDIA ConnectX-7 200 Gb/s NDR200 InfiniBand/VPI Adapter, QSFP112, PCIe 5.0 x16\n  * NVIDIA ConnectX-6 200 Gb/s HDR InfiniBand/VPI Adapter, QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-6 100 Gb/s HDR100 InfiniBand/VPI Adapter, 1x QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-6 Dx EN 200 Gb/s Ethernet Adapter, QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-6 Dx EN 100 Gb/s Ethernet Adapter, QSFP56, PCIe 4.0 x16\n  * NVIDIA ConnectX-5 EN 100 Gb/s Ethernet Adapter, QSFP28, PCIe 3.0 x16\n\n###  Power\n\n6 hot-swappable 3000W 80 PLUS Titanium PSUs \u00b7 3 + 3 redundancy\n\n  * 6 hot-swappable 3000W 80 PLUS Titanium PSUs\n  * 3 + 3 redundancy\n\n**AC input:**\n\n  * 210-240Vac / 16-14.5A / 50-60 Hz\n\n**Max output per PSU:**\n\n  * 210-240Vac / 3000 watts  \n  \n  \n  \n\n###  Front I/O\n\nPower button \u00b7 Reset button \u00b7 Information LED \u00b7 LAN activity LED (one for each\nport)...\n\n  * Power button and LED\n  * Reset button\n  * Information LED\n  * LAN activity LED (one for each port)\n  * HDD LED\n  * Power fail LED\n\n###  Rear I/O\n\nUID button and LED \u00b7 VGA port \u00b7 2 USB 3.0 ports...\n\n  * UID button and LED\n  * VGA port\n  * 2 USB 3.0 ports\n  * 2 RJ45 10 Gbps BASE-T LAN ports\n  * 1 RJ45 1 Gbps BASE-T LAN out-of-band management (IPMI) port\n\n###  Accessories\n\nRackmounting kit \u00b7 6 configurable C19 power cables\n\n  * Rackmounting kit\n  * 6 configurable C19 power cables\n\n###  Physical\n\n8U rackmount \u00b7 17.2 inches (437 mm) \u00b7 14 inches (356 mm) \u00b7 31.5 inches (800\nmm)...\n\n  * **Form factor** : 8U rackmount\n  * **Width** : 17.2 inches (437 mm)\n  * **Height** : 14 inches (356 mm)\n  * **Depth** : 31.5 inches (800 mm)\n\nResources\n\nGPU Benchmarks  Blog  Lambda Stack  Documentation  Forum  Research\n\nCompany\n\nAbout  Careers  Professional Services  Partners\n\nSupport\n\nTechnical Support  Partner Portal\n\nContact\n\nContact Us  P. 1 (866) 711-2025\n\n* * *\n\n\u00a9 2023 All rights reserved.\n\nTerms of Service     Privacy Policy  \n\nFollow us on Facebook Follow us on Twitter Follow us on LinkedIn Lambda's\nYouTube Channel Lambda's Blog RSS Feed\n\n",
    "links": "[{\"link\": \"https://lambdalabs.com/?hsLang=en\", \"text\": \"\"}, {\"link\": \"https://lambdalabs.com/deep-learning/servers/hyperplane\", \"text\": \"\"}, {\"link\": \"https://lambdalabs.com/deep-learning/servers/hyperplane\", \"text\": \"Hyperplane Server\"}, {\"link\": \"https://lambdalabs.com/cart?hsLang=en\", \"text\": \"\"}, {\"link\": \"https://lambdalabs.com/deep-learning/servers/hyperplane?hsLang=en\", \"text\": \"Hyperplane\"}, {\"link\": \"https://lambdalabs.com/deep-learning/servers/hyperplane\", \"text\": \"Overview\"}, {\"link\": \"https://lambdalabs.com/deep-learning/servers/hyperplane/support\", \"text\": \"Support\"}, {\"link\": \"https://lambdalabs.com/deep-learning/servers/hyperplane\", \"text\": \"Overview\"}, {\"link\": \"https://lambdalabs.com/deep-learning/servers/hyperplane/support\", \"text\": \"Support\"}, {\"link\": \"https://lambdalabs.com/service/gpu-cloud?hsLang=en\", \"text\": \"\"}, {\"link\": \"https://lambdalabs.com/deep-learning/servers/hyperplane/support?hsLang=en\", \"text\": \"\\n          Learn more\\n        \"}, {\"link\": \"https://lambdalabs.com/?hsLang=en\", \"text\": \"\"}]"
}