{
    "summary": "|  | **Hacker News** new | past | comments | ask | show | jobs | submit |\nlogin  \n---|---|---  \n|  |\n\n| Cloud GPU Resources and Pricing (fullstackdeeplearning.com)  \n---|---|---  \n|  184 points by nihit-desai 6 months ago | hide | past | favorite | 59\ncomments  \n  \n  \n| |\n\n|\n\ntedivm 6 months ago | next [\u2013]\n\n  \n\nI love the full stack deep learning crew, and took their course several years\nago in Berkeley. I highly recommend it.\n\nOne thing that always blows my mind is how much it is just not worth it to\ntrain LLMs in the cloud if you're a startup (and probably even less so for\nreally large companies). Compared to 36 month reserved pricing the break even\npoint was 8 months if you bought the hardware and rented out some racks at a\ncolo, and that includes the on hands support. Having the dedicated hardware\nalso meant that researchers were willing to experiment more when we weren't\ndoing a planned training job, as it wouldn't pull from our budget. We spent a\nsizable chuck of our raise on that cluster but it was worth every penny.\n\nI will say that I would not put customer facing inference on prem at this\npoint- the resiliency of the cloud normally offsets the pricing, and most\ninference can be done with cheaper hardware than training. For training though\nyou can get away with a weaker SLA, and the cloud is always there if you\nreally need to burst beyond what you've purchased.  \n  \n---|---|---  \n| |\n\n|\n\nericpauley 6 months ago | parent | next [\u2013]\n\n  \n\n> Compared to 36 month reserved pricing the break even point was 8 months if\n> you bought the hardware and rented out some racks at a colo, and that\n> includes the on hands support. Having the dedicated hardware also meant that\n> researchers were willing to experiment more when we weren't doing a planned\n> training job, as it wouldn't pull from our budget.\n\nThat\u2019s having it both ways, of course. You can\u2019t both recoup the hardware cost\nin 8 months and have \u201cfree\u201d downtime.\n\nUnder this pricing you need at least 25%ish duty cycle to break even (in 3\nyears) so probably still favoring buying, but for some people that might not\nadd up. Pricing also varies drastically between providers, so this may depend\non choice there.  \n  \n---|---|---  \n| |\n\n|\n\nrfoo 6 months ago | root | parent | next [\u2013]\n\n  \n\nThe thing is, this is compared to 36 months reserved instance, which also\nassumes 100% usage. For true on-demand you pay much more.  \n  \n---|---|---  \n| |\n\n|\n\nmoonchrome 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nReserved pricing is not on-demand.  \n  \n---|---|---  \n| |\n\n|\n\nbradleyjg 6 months ago | parent | prev | next [\u2013]\n\n  \n\nIt\u2019s the first widespread thing where build your own makes sense in a while.\nMost prior workflows either needed high SLAs, were bursty, or just didn\u2019t add\nup to much.  \n  \n---|---|---  \n| |\n\n|\n\nfpgaminer 6 months ago | prev | next [\u2013]\n\n  \n\nMy experience with many of these services renting mostly A100s:\n\nLambdaLabs: For on-demand instances, they are the cheapest available option.\nTheir offering is straightforward, and I've never had a problem. The downside\nis that their instance availability is spotty. It seems like things have\ngotten a little better in the last month, and 8x machines are available more\noften than not, but single A100s were rarely available for most of this year.\nAnother downside is lack of persistent storage, meaning you have to transfer\nyour data every time you start a new instance. They have some persistent\nstorage in beta, but it's effectively useless since it's only in one region\nand there's no instances in that region that I've seen.\n\nJarvis: Didn't work for me when I tried them a couple months ago. The\ninstances would never finish booting. It's also a pre-paid system, so you have\nto fill up your \"balance\" before renting machines. But their customer service\nwas friendly and gave me a full refund so _shrug_.\n\nGCP: This is my go-to so far. A100s are $1.1/hr interruptible, and of course\nyou get all the other Google offerings like persistent disks, S3, managed SQL,\ncontainer registry, etc. Availability of interruptible instances has been\nconsistently quite good, if a bit confusing. I've had some machines up for a\nweek solid without interruption, while other times I can tear down a stack of\nmachines and immediately request a new one only to be told they are out of\navailability. The downsides are the usual GCP downsides: poor documentation,\nsometimes weird glitches, and perhaps the worst billing system I've seen\noutside of the healthcare industry.\n\nVast.ai: They can be a good chunk cheaper, but at the cost of privacy,\nsecurity, support, and reliability. Pre-load only. For certain workloads and\nif you're highly cost sensitive this is a good option to consider.\n\nRunPod: Terrible performance issues. Pre-load only. Non-responsive customer\nsupport. I ended up having to get my credit card company involved.\n\nSelf-hosted: As a sibling comment points out, self hosting is a great option\nto consider. In particular \"Having the dedicated hardware also meant that\nresearchers were willing to experiment more\". I've got a couple cards in my\nlab that I use for experimentation, and then throw to the cloud for big runs.  \n  \n---|---|---  \n| |\n\n|\n\npavelstoev 6 months ago | parent | next [\u2013]\n\n  \n\nplease consider CoreWeave - great experience so far.  \n  \n---|---|---  \n| |\n\n|\n\nfpgaminer 6 months ago | root | parent | next [\u2013]\n\n  \n\nFor 2 to 3 times the cost of Lambda on demand and GCP preempt?  \n  \n---|---|---  \n| |\n\n|\n\nsorcer 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\n+1 to this  \n  \n---|---|---  \n| |\n\n|\n\npavelstoev 6 months ago | prev | next [\u2013]\n\n  \n\nMay also suggest the suite of open source DeepView tools and which are part of\nPyPi. Profile and predict your specific model training performance on a\nvariety of GPUs. I wrote a linked in post with usage GIF here:\nhttps://www.linkedin.com/posts/activity-7057419660312371200-...\n\nAnd links to PyPi:\n\n=> https://pypi.org/project/deepview-profile/ =>\nhttps://pypi.org/project/deepview-predict/\n\nAnd you can actually do it in browser for several foundational models (more to\ncome): => https://centml.ai/calculator/\n\nNote: I have personal interests in this startup.  \n  \n---|---|---  \n| |\n\n|\n\nstuckkeys 6 months ago | prev | next [\u2013]\n\n  \n\n\"Those god damn AWS charges\" -Silicon Valley. Might as well build your own GPU\nfarm. Some of these cards, used you can probably get for 6K (guestimating).  \n  \n---|---|---  \n| |\n\n|\n\nx-complexity 6 months ago | parent | next [\u2013]\n\n  \n\nThat would imply that the current AI cycle would be able to persist at its\ncurrent levels of frothiness indefinitely: In the in-between lull periods,\nthese GPU farms would be seen as something to sell off. This doesn't even take\ninto account the eventual depreciation of the GPUs in question, as better\nGPUs/accelerators come into the market.\n\nMost companies have an AWS account that they can throw on more money at for\n'AI research & implementation'. With such an account existing in the first\nplace, along with said price depreciations, the company in question would have\nto be certain that they'll use said GPUs all the time to make up for the\nupfront costs they'll be putting up with.  \n  \n---|---|---  \n| |\n\n|\n\nmirekrusin 6 months ago | root | parent | next [\u2013]\n\n  \n\nYour own hardware can be rented out if you're not using it through vast.ai for\nexample.\n\nWhen new hardware comes out, you can sell old one to recover some of the cost.  \n  \n---|---|---  \n| |\n\n|\n\nstuckkeys 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nSure bud. Sure. To each their own. When these GPU enter the consumer market is\nwhen the AWS cost becomes irrelevant. =)  \n  \n---|---|---  \n| |\n\n|\n\npavelstoev 6 months ago | parent | prev | next [\u2013]\n\n  \n\nOften you are better off running certain workloads on lesser GPUs. But this\nrequires certain tricky compiler-level optimizations. For example, can run\ncertain LLM inference with comparable latency on cheaper A40s vs running on\nA100s. Could also run on 3090s (sometimes even faster). This helps with\noperating costs but may also resolve availability constraints.  \n  \n---|---|---  \n| |\n\n|\n\nsorcer 6 months ago | root | parent | next [\u2013]\n\n  \n\nA40 / A6000 and A5000 is a great GPU for single GPU inference and training,\nprovides better price/performance than A100 for models that fit.  \n  \n---|---|---  \n| |\n\n|\n\n1024core 6 months ago | prev | next [\u2013]\n\n  \n\nDoesn't anybody use TPUs from Google?\n\nGiven the heterogeneous nature of GPUs, RAM, tensor cores, etc. it would be\nnice to have a direct comparison of, say, number of teraflops-hour per dollar,\nor something like that.  \n  \n---|---|---  \n| |\n\n|\n\nTradingPlaces 6 months ago | prev | next [\u2013]\n\n  \n\nLambda has a very interesting benchmark page https://lambdalabs.com/gpu-\nbenchmarks\n\nIf you look through the throughput/$ metric, the V100 16GB looks like a great\ndeal, followed by H100 80GB PCIe 5. For most benchmarks, the A100 looks worse\nin comparison  \n  \n---|---|---  \n| |\n\n|\n\ndkobran 6 months ago | prev | next [\u2013]\n\n  \n\nFWIW, Paperspace has a similar GPU comparison guide located here\nhttps://www.paperspace.com/gpu-cloud-comparison\n\nDisclosure: I work on Paperspace  \n  \n---|---|---  \n| |\n\n|\n\nsva_ 6 months ago | parent | next [\u2013]\n\n  \n\nIt seems very honest of you to release this list, considering you're at the\nvery bottom of it by $/hour.  \n  \n---|---|---  \n| |\n\n|\n\nakiselev 6 months ago | root | parent | next [\u2013]\n\n  \n\nDemand outstrips supply by such a wide margin it doesn\u2019t matter one bit how\nthey compare.\n\nThe cheapest 8x A100 (80GB) on the list is LambdaLabs @ $12/hour on demand,\nand I\u2019ve only once seen any capacity become available in three months of using\nit. AWS last I checked was $40/hr on demand or $25/hr with 1 year reserve,\nwhich costs more than a whole 8xA100 hyperplane from Lambda.\n\nThe pricing on these things is nuts right now  \n  \n---|---|---  \n| |\n\n|\n\nslavik81 6 months ago | parent | prev | next [\u2013]\n\n  \n\nI was very hopeful when I saw \"AMD Support\" listed on a few of those\nproviders, but that appears to only refer only to AMD CPUs. It is,\nunfortunately, very difficult to find public cloud providers for AMD GPUs.  \n  \n---|---|---  \n| |\n\n|\n\npavelstoev 6 months ago | parent | prev | next [\u2013]\n\n  \n\nVery nice. Can we chat briefly ?  \n  \n---|---|---  \n| |\n\n|\n\nKeplerBoy 6 months ago | prev | next [\u2013]\n\n  \n\nwhy does no one rent out AMD GPUs?\n\nI know those cards are second class citizens in the world of deep learning,\nbut they have had (experimental) pytorch support for a while now, where are\nthe offerings?  \n  \n---|---|---  \n| |\n\n|\n\nmkaic 6 months ago | parent | next [\u2013]\n\n  \n\n> I know those cards are second class citizens in the world of deep learning,\n\nIt's worse than that. AMD cards aren't second class citizens, they're not even\non the same playing field. ROCm can't compete with CUDA and its ecosystem _at\nall_ , the most popular deep learning frameworks are only _experimentally_\nsupported, _and_ Nvidia ships more dedicated tensor processing cores for AI\nacceleration on their cards. Nvidia has a near monopoly in AI not because\nthey're particularly amazing, but because it seems like AMD is just\nuninterested in competing.  \n  \n---|---|---  \n| |\n\n|\n\narvinsim 6 months ago | root | parent | next [\u2013]\n\n  \n\nI don't think that AMD is uninterested in competing.\n\nIt is just that the mindshare is swallowed up by Nvidia that it is really\ndifficult to use something else even if you want to.  \n  \n---|---|---  \n| |\n\n|\n\nkouteiheika 6 months ago | root | parent | next [\u2013]\n\n  \n\n> I don't think that AMD is uninterested in competing.\n\nI also think they're not interested. Either that or just simply incompetent.\n\nFor example, just look at this issue and see the huge mess:\n\nhttps://github.com/RadeonOpenCompute/ROCm/issues/1714\n\nWith NVidia I can just buy _any_ random GPU and expect it to work for\neverything I throw at it (at long as it has enough VRAM). With AMD it's a\nroulette, and only a handful of very expensive server/workstation GPUs (8 in\ntotal if I'm counting it right) are actually officially supported. It's a\njoke.\n\nThey need to better support their own products, and they need to officially\nsupport _all_ of their consumer GPUs to expand their mindshare. They're not\ndoing that. From what I can see they only seem to be interested in the\ntraditional HPC space.  \n  \n---|---|---  \n| |\n\n|\n\ndetaro 6 months ago | parent | prev | next [\u2013]\n\n  \n\nWhy would anyone offer a strictly worse product unless it were lots cheaper to\noffer, which it isn't? (Even for non-AI use cases, I don't think AMD has much\nthat's more attractive in servers?)  \n  \n---|---|---  \n| |\n\n|\n\nxbmcuser 6 months ago | parent | prev | next [\u2013]\n\n  \n\nThey are simply put not price competitive in instruction per dollar at the\nhigh end though they are starting to catch up. But to me the biggest reason is\nsoftware wise they are behind Nvidia. NVidia might be considered a hardware\ncompany because of its gpus but they are underappreciated as a the software\ncompany that build tools for other to utilise its gpus.  \n  \n---|---|---  \n| |\n\n|\n\nkkielhofner 6 months ago | root | parent | next [\u2013]\n\n  \n\nThey\u2019re not price competitive anywhere in the line.\n\nBy the time you spend hours/days/weeks constantly dealing with random edge\ncases and issues with the poor software support of AMD you could have bought\n2-3x the Nvidia hardware (minimum) and still come out ahead.  \n  \n---|---|---  \n| |\n\n|\n\nnihit-desai 6 months ago | prev | next [\u2013]\n\n  \n\nA comprehensive list of GPU options and pricing from cloud vendors. Very\nuseful if you're looking to train or deploy large machine learning/deep\nlearning models.  \n  \n---|---|---  \n| |\n\n|\n\nfreediver 6 months ago | prev | next [\u2013]\n\n  \n\nCreated this a while ago for the same purpose https://cloudoptimizer.io  \n  \n---|---|---  \n| |\n\n|\n\njustherefornews 6 months ago | parent | next [\u2013]\n\n  \n\nNice man, ty!  \n  \n---|---|---  \n| |\n\n|\n\ndotBen 6 months ago | prev | next [\u2013]\n\n  \n\nLooking to run a cloud instance of Stable Diffusion for personal\nexperimentation. Looking at cloud mostly because I don't have a GPU or desktop\nhardware at home, and my Mac M1 is too slow. But also needing to contend with\nconstant switching on/off the instance several times a week to use it.\n\nWondering which vendors other HN'ers are using to achieve this?  \n  \n---|---|---  \n| |\n\n|\n\nShakahs 6 months ago | parent | next [\u2013]\n\n  \n\nAWS spot g5 or Vultr fractional A40 with an Ansible playbook to set up Stable\nDiffusion, drivers, etc.  \n  \n---|---|---  \n| |\n\n|\n\nvictorbjorklund 6 months ago | parent | prev | next [\u2013]\n\n  \n\nHave you tried google colab? Runs pretty well on it  \n  \n---|---|---  \n| |\n\n|\n\nhcarlens 6 months ago | prev | next [\u2013]\n\n  \n\nI built a slightly less detailed version of this, which also also lists free\ncredits: https://cloud-gpus.com/\n\nOpen to any feedback/suggestions! Will be adding 4090/H100 shortly.  \n  \n---|---|---  \n| |\n\n|\n\nandrewstuart 6 months ago | prev | next [\u2013]\n\n  \n\nGo buy a GPU from the local computer store.\n\nConsumers GPUs are much more available, much cheaper and much faster.  \n  \n---|---|---  \n| |\n\n|\n\nkookamamie 6 months ago | parent | next [\u2013]\n\n  \n\nNVIDIA's EULA specifically forbids using consumer GPUs for data-center -like\ncomputation purposes. Even if that wasn't an issue, there are a number of\nissues with both providing power to multiple consumer GPUs and just fitting\nthem into a case/chassis, as they vary in their physical sizes - often being\n1.5-2x in width compared to the similar level enterprise GPU.  \n  \n---|---|---  \n| |\n\n|\n\nandrewstuart 6 months ago | root | parent | next [\u2013]\n\n  \n\n>> NVIDIA's EULA specifically forbids using consumer GPUs for data-center\n-like computation purposes\n\nI cannot see that in the wording here:\n\nhttps://www.nvidia.com/en-us/drivers/geforce-license/  \n  \n---|---|---  \n| |\n\n|\n\ndikei 6 months ago | root | parent | next [\u2013]\n\n  \n\n> No Datacenter Deployment. The SOFTWARE is not licensed for datacenter\n> deployment, except that blockchain processing in a datacenter is permitted.\n\nIt's this clause.  \n  \n---|---|---  \n| |\n\n|\n\nthrowaway3672 6 months ago | parent | prev | next [\u2013]\n\n  \n\nWhy just not rent a 3090 from vast ai? This is literally 0.2$ per hour.  \n  \n---|---|---  \n| |\n\n|\n\ndeserialized 6 months ago | root | parent | next [\u2013]\n\n  \n\nDepends on your requirements.\n\nVast is a grey-market with very few security measures in place. A host can\nsnoop on your workloads very easily for example because they have full access\nto the docker host.\n\nYou're also limited in that you can only deploy a single container so if you\njust wanted some spot nodes to scale out your existing Dagster cluster etc..\nYou're not going to be able to cleanly plug Vast into your existing\ninfrastructure deployment process.\n\nBut if you're just playing around or don't have major security/data\nrestrictions and only want a jhub notebook or nvidia-glx-desktop then yeah\nit's a very hard deal to beat.  \n  \n---|---|---  \n| |\n\n|\n\nvhcr 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nYou can purchase a 3090 for $1000, assuming you're going to use it 24/7, at\n450W you would use about 1000kWh, at $0.10 / kWh, it would pay itself in about\n3 months.  \n  \n---|---|---  \n| |\n\n|\n\ndikei 6 months ago | root | parent | next [\u2013]\n\n  \n\nThat's not a typical use case during development though. People need fast\nfeedback loops: they'd rather rent multiple GPUs and have the result the next\nmorning, than waiting for days with no guarantee of success.\n\nSo unless you have stable tasks that need to run continuously, or have enough\nusers to keeps your GPU clusters busy, your GPUs usage would be quite bursty:\nsome period of high activity then a lot of time idling.  \n  \n---|---|---  \n| |\n\n|\n\noceanplexian 6 months ago | root | parent | next [\u2013]\n\n  \n\nI'd argue the opposite. Having to spin up and down instances for development\nis a huge PITA, the tooling sucks, and the instance might not even be\navailable the next time you need them. It also stresses me out personally,\nbecause I'm worrying about getting productive use out of every minute. Whereas\nmy little GPU cluster (Despite a big upfront cost) costs nothing but\nelectricity and runs 24/7.  \n  \n---|---|---  \n| |\n\n|\n\n1024core 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nAre you sure about your calculation? At $0.20/hr, in 3 months (90*24 hours),\nyou would spend about $432.\n\nAnd your electricity bill alone would be $100. So you'd be spending $1100 over\nthose 3 months purchasing the card, -vs- spending $432 in the cloud.  \n  \n---|---|---  \n| |\n\n|\n\nYetAnotherNick 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nI agree that buying is a good choice, but your calculation is wrong. 3 months\nof renting will take 0.2*24*30*3=$432, which won't even cover half of the base\nprice.  \n  \n---|---|---  \n| |\n\n|\n\naunty_helen 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nSo you save 500$ on cloud costs, spend 4k on developer time and delay a 250k\ndollar app launch by 2 weeks.\n\nWhen you're just messing around, this is great. Home brew gpu labs are fun and\ncool. As soon as you bring economics into it and start valuing time, it's a\nnon-starter.  \n  \n---|---|---  \n| |\n\n|\n\narvinsim 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nThe value proposition is very different between one who is just starting to\nlearn AI/ML and one who already knows what to train.  \n  \n---|---|---  \n| |\n\n|\n\nrlupi 6 months ago | prev | next [\u2013]\n\n  \n\nOut of curiosity. Do mostly use/want one GPU or the full server with all GPUs\n(8x A100 80GB, or 16x A100 40GB but I think only Google Cloud has those)? or a\nmix?  \n  \n---|---|---  \n| |\n\n|\n\ncrucifiction 6 months ago | prev | next [\u2013]\n\n  \n\nMissing Oracle Cloud which has a massive GPU footprint -\nhttps://www.oracle.com/cloud/compute/gpu/  \n  \n---|---|---  \n| |\n\n|\n\nranguna 6 months ago | parent | next [\u2013]\n\n  \n\nIt's missing quite a few vendors, but they are open to PRs :)  \n  \n---|---|---  \n| |\n\n|\n\nhislaziness 6 months ago | prev | next [\u2013]\n\n  \n\nI do not see any of the AI processors like the Google TPU. Would they be\ncheaper?  \n  \n---|---|---  \n| |\n\n|\n\nbrucethemoose2 6 months ago | parent | next [\u2013]\n\n  \n\nTPU and other accelerator performance varies by application... And even in a\nhugely popular config (like finetuning LLaMA with JAX) its hard to find a good\nbenchmark. But generally speaking Google charges a pretty penny for TPUs\n\nAccelerators outside TPUs are exotic. Off the top of my head... Cerebras only\noffers their WS2 as a 1st party \"pay for a specific training job\" kinda thing.\nIntel Gaudi 2 is supposedly good but is mysterious to me, and Ponte Vecchio\nhas barely started shipping. Graphcore and Tenstorrent chips in the wild seem\nkinda long in the tooth for big training jobs. The AMD MI300 is not shipping,\nand the older AMD Instincts are difficut to find in cloud services (maybe\nbecause they got eaten up for HPC?)\n\nLots of other promising accelerators (with my personal favorite being the\nCentaur x86 \"accelerated\" CPUs, perfect for dirt cheap LLM inference/LORA\ntraining) died on the vine because of the CUDA moat, and I think more will\nshare the same fate.  \n  \n---|---|---  \n| |\n\n|\n\naeturnum 6 months ago | prev | next [\u2013]\n\n  \n\nThis isn't my area of expertise so I can't get too deep but this is an\nextremely well organized and straightforward resource.  \n  \n---|---|---  \n| |\n\n|\n\nMacsHeadroom 6 months ago | prev | next [\u2013]\n\n  \n\nIt's interesting that AWS is a full 5x more expensive than the leading low\ncost providers, with Google close behind AWS.  \n  \n---|---|---  \n| |\n\n|\n\nIceHegel 6 months ago | prev [\u2013]\n\n  \n\nDo we have numbers from H100  \n  \n---|---|---  \n| |\n\n|\n\nsorcer 6 months ago | parent [\u2013]\n\n  \n\nThe only H100s available to get is the \"little brother\" PCIe version with\nHBM2e memory instead of HBM3. Very very few clouds, or people, in general,\nhave H100 deployed at any scale. You will see when the MLPerf benchmark\nresults come out.  \n  \n---|---|---  \n  \n  \n  \n|  \n---  \n  \nGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact  \n  \nSearch:\n\n",
    "links": "[]",
    "priceAndPlans": "|  | **Hacker News** new | past | comments | ask | show | jobs | submit |\nlogin  \n---|---|---  \n|  |\n\n| Show HN: Oblivus GPU Cloud \u2013 Affordable and scalable GPU servers from\n$0.29/hr (oblivus.com)  \n---|---|---  \n|  193 points by oblivuslimited 6 months ago | hide | past | favorite | 129\ncomments  \n|\n\nGreetings HN!\n\nThis is Doruk from Oblivus, and I'm excited to announce the launch of our\nplatform, Oblivus Cloud. After more than a year of beta testing, we're excited\nto offer you a platform where you can deploy affordable and scalable GPU\nvirtual machines in as little as 30 seconds! https://oblivus.com/cloud\n\n\\- What sets Oblivus Cloud apart?\n\nAt the start of our journey, we had two primary goals in mind: to democratize\nHigh-Performance Computing and make it as straightforward as possible. We\nunderstand that maintaining GPU servers through major cloud service providers\ncan be expensive, with hidden fees adding to the burden of running and\nmaintaining servers. Additionally, the cloud can sometimes be overly complex\nfor individuals who don't have much knowledge but still require powerful\ncomputing resources. That's why we decided to create a platform that offers\naffordable pricing, easy usability, and high-quality performance.\n\n\\- Features\n\n1\\. Fully customizable infrastructure that lets you switch between CPU and GPU\nconfigurations to suit your needs.\n\n2\\. Transparent and affordable per-minute-based Pay-As-You-Go pricing with no\nhidden fees. Plus, free data ingress and egress. (Pricing:\nhttps://oblivus.com/pricing/)\n\n3\\. Optimized cost with storage and IP address-only billing when the virtual\nmachine is shut down.\n\n4\\. Each virtual machine comes with 10Gbps to 40Gbps public network\nconnectivity.\n\n5\\. NVMe ($0.00011/GB/hr) and HDD ($0.00006/GB/hr) storage that is 3x\nreplicated to fulfill your storage needs.\n\n6\\. Choose from a variety of cutting-edge CPUs and 10 state-of-the-art GPU\nSKUs. (Availability: https://oblivus.com/availability/)\n\n7\\. OblivusAI OS images come with pre-installed ML libraries, so you can start\ntraining your models right away without the hassle of installing and\nconfiguring the necessary libraries.\n\n8\\. If you're working with a team, utilize our organization feature to\nsimplify the billing process. Everyone in your organization uses the same\nbilling profile, so you don't need to keep track of multiple accounts.\n\n9\\. No quotas or complex verification processes. Whether you represent a\ncompany, an institution, or you're a researcher, you have full access to our\ninfrastructure without any limitations.\n\n10\\. Easy-to-use API with detailed documentation so that you can integrate\nyour code with ours.\n\n\\- Pricing\n\nAt Oblivus Cloud, we provide pricing that is affordable, transparent, and up\nto 80% cheaper than major cloud service providers. Here is a breakdown of our\npricing:\n\n1\\. CPU-based virtual machines starting from just $0.019/hour.\n\n2\\. NVIDIA Quadro RTX 4000s starting from $0.27/hour.\n\n3\\. Tesla V100s starting from $0.51/hour.\n\n4\\. NVIDIA A40s and RTX A6000s starting from $1.41/hour.\n\nWe also offer 6 other GPU SKUs to help you accurately size your workloads and\nonly pay for what you need. Say goodbye to hidden fees and unpredictable\ncosts.\n\nIf you represent a company, be sure to register for a business account to\naccess even better pricing rates.\n\n\\- Promo Code\n\nJoin us in celebrating the launch of Oblivus Cloud by claiming your $1 free\ncredit! This may sound small, but it's enough to get started with us and\nexperience the power of our platform. With $1, you can get over 3 hours of\ncomputing on our most affordable GPU-based configuration, or over 50 hours of\ncomputing on our cheapest CPU-based configuration.\n\nTo redeem this free credit, simply use the code HN_1 on the 'Add Balance' page\nafter registration.\n\nRegister now at https://console.oblivus.com/register\n\n\\- Quick Links\n\nWebsite: https://oblivus.com/\n\nConsole: https://console.oblivus.com/\n\nCompany Documentation: https://docs.oblivus.com/\n\nAPI Documentation: https://documenter.getpostman.com/view/21699896/UzBtoQ3e\n\nIf you have any questions, feel free to post them below and I'll be happy to\nassist you. You can also directly email me at doruk@oblivus.com!  \n  \n  \n  \n| |\n\n|\n\nNickHoff 6 months ago | next [\u2013]\n\n  \n\nOne of the issues is availability. Lambdalabs and Paperspace never have\ninstances available. AWS and GCP have the per-account quota set to 0 by\ndefault for GPU instances and deny requests to raise it, even to 1. (One of my\nAWS accounts was granted 1 GPU after some back-and-forth with support.) I'm\nnot upset, I understand that demand is high, but how will oblivus handle\navailability?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nThank you for your question!\n\nFirstly, unlike AWS, GCP, or Azure, there are no restrictions on the number of\nGPUs you can deploy with us. As long as you have sufficient account balance,\nyou can deploy hundreds of GPUs simultaneously.\n\nIn terms of availability, we maintain a diverse range of resources from\nvarious vendors in our on-demand stock. While we have our own infrastructure,\nwe also leverage infrastructure from other vendors to meet the growing demand.\n\nCurrently, we have more than 3000 GPUs in stock, with over 2600 of them\navailable for deployment. You can find more detailed information on our\navailability page at https://oblivus.com/availability/.\n\nI hope this helps!  \n  \n---|---|---  \n| |\n\n|\n\nyanslookup 6 months ago | root | parent | next [\u2013]\n\n  \n\n> you can deploy hundreds of GPUs simultaneously.\n\n> with over 2600 of them available for deployment.\n\nguessing you mean 2600 in total.\n\nFWIW we ran a workload recently on AWS that required a few thousand g4\ninstances in a single AWS region. We ended up scavenging and using g3s as well\ndue to capacity constraints.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nThat's quite an impressive workload!\n\nIf you're using our on-demand service and intend to terminate the machines\nonce your work is completed, we currently have 2600 available GPUs. However,\nif you have an ongoing need for these machines, we also have reserved\ninstances with additional stock, which brings our total capacity to an\nestimated 7000 GPUs as of today.\n\nBut of course these numbers could easily change in the future.  \n  \n---|---|---  \n| |\n\n|\n\nwilliamstein 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nI think they mean that almost 2600 are literally available right now for you\nto rent - or if you requested them, then you would get them right now, and\nthen there would - for a while - be 0 that are available, because you have\nthem all.  \n  \n---|---|---  \n| |\n\n|\n\nanonymousDan 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nOut of interest, was this workload for training or serving?  \n  \n---|---|---  \n| |\n\n|\n\nandrewstuart 6 months ago | prev | next [\u2013]\n\n  \n\nLots of people seem to think they have no choice but to rent GPUs from cloud\nservice providers.\n\nRemember retail GPUs are much more powerful, much cheaper and much more\navailable than cloud GPUs.\n\nJust go to the store and buy one.  \n  \n---|---|---  \n| |\n\n|\n\nzamnos 6 months ago | parent | next [\u2013]\n\n  \n\nThen I'd only have one. Instead, I can borrow a dozen of them and use them in\nparallel and I'll return them when I'm done. I don't know how many I'll need,\nor for how long, and cloud enables my bad habit of not planning.  \n  \n---|---|---  \n| |\n\n|\n\nNickHoff 6 months ago | parent | prev | next [\u2013]\n\n  \n\n> retail GPUs are much more powerful\n\nI agree with your basic point about building vs renting, but retail GPUs\naren't simply more powerful. Retail GPUs are just as fast but have a lot less\nVRAM and memory bandwidth. The RTX4090 has 24 GB of VRAM with 1 TB/s of\nbandwidth. The A100 has 80 GB at 2 TB/s. For some tasks, the memory and\nbandwidth are the strongest constraints.  \n  \n---|---|---  \n| |\n\n|\n\nseanw444 6 months ago | root | parent | next [\u2013]\n\n  \n\nAI being a primary example, and that's probably the thing most people are\nrenting these for nowadays anyways.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | prev | next [\u2013]\n\n  \n\nThank you for sharing your thoughts.\n\nAcquiring the latest technology, such as an A100 GPU, individually from a\nstore can indeed be quite expensive, with prices around $10,000 per unit.\nAdditionally, setting up and maintaining a home lab to scale from one GPU to\nthousands can be a significant investment in terms of both cost and resources.\n\nIn contrast, our platform provides access to the latest GPU technology without\nthe need for users to individually purchase and manage the hardware. We offer\nthe scalability to deploy multiple GPUs as needed and scale back down when\nrequired, making it a more cost-effective and flexible solution compared to\nsetting up and maintaining a personal lab.\n\nFurthermore, it's important to note that our GPUs are directly dedicated to\neach virtual machine, ensuring that the power and performance are not\ncompromised by sharing resources. This ensures that our GPUs provide the full\ncapability and performance expected, making them just as powerful as\nindividual units.  \n  \n---|---|---  \n| |\n\n|\n\ndougSF70 6 months ago | root | parent | next [\u2013]\n\n  \n\nI have just bought 4 servers with 10 GPU (Titan X Pascal) and 384 GB of RAM\nfor the price of a macbook. I found a solar powered CoLo facility to put them\nin. The economics of owning vs renting from AWS are worth it. My server set up\nis the same price as running one model on AWS.  \n  \n---|---|---  \n| |\n\n|\n\ncrabbone 6 months ago | root | parent | next [\u2013]\n\n  \n\nIf memory serves, NVidia's drivers can only be used for Tesla / Ampere family,\nif you are using them in a datacenter. Titan / GeForce are not allowed. So,\nthere's that (but maybe I'm missing something).\n\nNow, \"running\" may also mean different things. For example, you may want to do\nthings s.a. performance diagnostics (in order to understand if your code uses\nresources efficiently), and then you'd need stuff like NVML, DCGM and co.\nConsumer-grade hardware might not be supported, or might not in principle\nsupport diagnostics collection / instrumentation. Or, if you have multiple\nGPU-dependent workloads that cannot saturate your resources -- you might think\nof MIG as being a way to address that... and, again, consumer-grade GPUs won't\nhelp you here...\n\nI'm not saying you shouldn't try self-hosting. I'm actually all for it. But,\nyou also need to be mindful of the pros and cons. NVidia _must_ have some\nreason to pitch the datacenter family of GPUs to, well, datacenters. They\naren't _just_ blowing up prices.\n\nAlso, to give some sense of comparison:\n\nTitan X ~= 3.5K CUDA cores.\n\nV100 ~= 5K CUDA cores.\n\nA100 ~= 7K CUDA cores.\n\nH100 ~= 18.5K CUDA cores.\n\nIt probably doesn't translate directly into H100 being six times as fast as\nTitan X, but, I hear that these GPU workloads might be lengthy...  \n  \n---|---|---  \n| |\n\n|\n\ndougSF70 6 months ago | root | parent | next [\u2013]\n\n  \n\nI bought these from a Chinese internet firm. They dismantled their data center\nand I am buying it.  \n  \n---|---|---  \n| |\n\n|\n\ndougSF70 6 months ago | root | parent | next [\u2013]\n\n  \n\nAlso, Datacenter GPUs only have passive cooling, presumably to allow for more\nCuda cores. I get that what I have is older tech but for the cost of _one_\nH100 GPU I can have 60 of these servers (600 GPUs) plus some change to pay for\nCoLo fees.  \n  \n---|---|---  \n| |\n\n|\n\ncrabbone 6 months ago | root | parent | next [\u2013]\n\n  \n\nI'm not trying to discourage, and I don't have concrete numbers on hand, but\nthere are other factors, beside the price of h/w. Like, obviously, electricity\nuse, data locality / moving it around (with more smaller units you'd have to\nmove it more, also, not sure if consumer-grade GPUs support NVLink, and even\nif they do, then at what bandwidth?)\n\nFor some of these, you could obviously pay with your time. Sometimes that time\nis very valuable, and sometimes you have a lot to spare.\n\nAlso, the amount of VRAM (3x)... Sometimes having too little of it means\nhaving to re-write the program, or it could dramatically impact the speed.\nSimilarly for bandwidth (8x). But, again, if the kind of workload you have\nisn't constrained by either, then you could probably win by running on more\nsmaller / older GPUs.  \n  \n---|---|---  \n| |\n\n|\n\n0x008 6 months ago | root | parent | next [\u2013]\n\n  \n\nNVLink was discontinued with 40 series nvidia consumer GPUs.  \n  \n---|---|---  \n| |\n\n|\n\nlostmsu 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nI estimate $1.5k of electricity per GPU for a 5y lifetime assuming 100% load.\nNot including other components.  \n  \n---|---|---  \n| |\n\n|\n\n_boffin_ 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nUhh\u2026 can you share details\u2026 this seems like price point of things when they\u2019ve\nfallen off the back of a truck.  \n  \n---|---|---  \n| |\n\n|\n\niJohnDoe 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nCan you please share where to buy this from? I would like to purchase one.\nThanks!  \n  \n---|---|---  \n| |\n\n|\n\nandrewstuart 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nIn the tests I\u2019ve done, retail GPUs are much faster than data center GPUs.  \n  \n---|---|---  \n| |\n\n|\n\n0x008 6 months ago | root | parent | next [\u2013]\n\n  \n\nThey are, but data center GPU (ampere and above) are twice as good in\nperformance per watt.  \n  \n---|---|---  \n| |\n\n|\n\nmoralestapia 6 months ago | root | parent | next [\u2013]\n\n  \n\nAside from being green and that, why would a consumer care about a _leased_\nprocessor's performance per watt?  \n  \n---|---|---  \n| |\n\n|\n\npjlegato 6 months ago | root | parent | next [\u2013]\n\n  \n\nThe retail price of renting the GPU includes, among other things, the cost of\nthe power bill that the cloud service pays (or else they'd lose money.) The\nmore efficient GPU will, all else the same, be cheaper to lease.  \n  \n---|---|---  \n| |\n\n|\n\nlate2part 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nGiven a relative amount of benefit, with a ratio of Watts/Benefit, a lower\nratio of Watts/Benefit should cost less to the consumer.\n\nSo the consumer would care because the service is lower cost.  \n  \n---|---|---  \n| |\n\n|\n\nmoralestapia 6 months ago | root | parent | next [\u2013]\n\n  \n\nI'm truly confused by your argument and @pjlegato's ...\n\nIt's more expensive to rent a GPU than to buy it.\n\nThis whole comment thread started because GP wrote:\n\n>Remember retail GPUs are much more powerful, much cheaper and much more\navailable than cloud GPUs.\n\nSo, still, why would a consumer care about performance per watt?  \n  \n---|---|---  \n| |\n\n|\n\nrat9988 6 months ago | root | parent | next [\u2013]\n\n  \n\nBecause you'll have to include its price when you buy it, but not when you\nrent it. And how the economics plays out then is not obvious without maths.  \n  \n---|---|---  \n| |\n\n|\n\nmoralestapia 6 months ago | root | parent | next [\u2013]\n\n  \n\nI can assure you it's cheaper to buy. Otherwise they wouldn't be leasing it.\n\n(ofc assuming you're not going to use it for like a day)  \n  \n---|---|---  \n| |\n\n|\n\nupbeat_general 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nLower wattage allows more GPUs per system. For distributed training, that\u2019s\nallows 8 GPUs per system which would otherwise be very difficult with retail\nGPUs.\n\nAnd GPU-GPU interconnects are important for this type of training so putting\nas many GPUs as possible close together is necessary.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nWe use the same graphic buses, such as SXM4 and PCI Express 4.0 x 16, as you\nwould find in your system. Theoretically, there should be no difference in\nperformance or compatibility at all.\n\nIf you have conducted your tests on shared instances and have observed such\ndifferences, then it's understandable. However, I want to emphasize once again\nthat we provide dedicated virtual machines, which means that the resources are\nexclusively allocated to each user and not shared.  \n  \n---|---|---  \n| |\n\n|\n\njosefx 6 months ago | root | parent | next [\u2013]\n\n  \n\n> Theoretically, there should be no difference in performance or compatibility\n> at all.\n\nAt least in the past there was a tendency by NVIDIA to run their professional\nGPUs at a lower frequency, offering reliability and correctness guarantees as\ntradeoff. Meanwhile most of the retail/gaming GPUs came overclocked and almost\noutright guaranteeing that they would start to crash and fail if you tried to\nrun them 24/7.  \n  \n---|---|---  \n| |\n\n|\n\netherael 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nEven so, an A100 is not actually faster than an RTX 4090, right? So basically\nthe reason you would want to use something like this instead is because you\nneed the VRAM?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nYes, 0x008 makes a valid point regarding this topic. Data center GPUs and RTX\nSeries use different technologies and optimizations for different purposes.\n\nOur intention is not to discourage anyone from purchasing their own GPUs. We\noffer our GPU cloud services as an alternative for those who may not have the\nresources or prefer the convenience and flexibility of renting GPUs on-demand.\nWe apologize if the discussion took a different direction, and we appreciate\nyour understanding.  \n  \n---|---|---  \n| |\n\n|\n\nupbeat_general 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nThat\u2019s not the case. It is significantly faster, at least for deep learning\nworkloads. Check out Lambda Lab\u2019s benchmarks.  \n  \n---|---|---  \n| |\n\n|\n\netherael 6 months ago | root | parent | next [\u2013]\n\n  \n\nhttps://lambdalabs.com/gpu-benchmarks for those who are curious, ends up being\nquite the difference no matter which way you slice it, up to 60% better. Of\ncourse, this is probably due to the increased interconnect bandwidth and\nmemory rather than raw compute horsepower, but for the workloads in question\nthat's relevant.  \n  \n---|---|---  \n| |\n\n|\n\nthe-dude 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nWould you be so kind to post these results? How much faster, which models etc.  \n  \n---|---|---  \n| |\n\n|\n\npitched 6 months ago | root | parent | next [\u2013]\n\n  \n\nI found some charts here showing A100 roughly doubling a 3090: https://bizon-\ntech.com/blog/best-gpu-for-deep-learning-rtx-3...\n\nWith the 4090 being a fair amount more powerful, it might give the a100 a\nchallenge for about 10% of the cost. That\u2019s still about 10k hours of renting\none with this service though.  \n  \n---|---|---  \n| |\n\n|\n\nYetAnotherNick 6 months ago | root | parent | next [\u2013]\n\n  \n\nSure if you are using tiny net(by today's standard) and small batch sizes,\nA100 is just twice as fast compared to 3090 due to overhead of framework and\nmemory movement. A100 has 312 FP16 TFlops[1], compared to 40 and 82.6 of 3090\nTi and 4090 respectively[2], and has 3x memory. Also it has 10x inter GPU\ncommunication bandwidth.\n\n[1]: https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Cent...\n\n[2]: https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/n...  \n  \n---|---|---  \n| |\n\n|\n\nlostmsu 6 months ago | root | parent | next [\u2013]\n\n  \n\nI believe you are wrong. The relevant metric is tensor flops and 4090 has 330.\nI believe 312 you quoted for A100 is tensor flops.  \n  \n---|---|---  \n| |\n\n|\n\ndtx1 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nEating your own dogfood I see: https://imgur.com/IXPWOSk  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nAs someone whose native language is not English, I find it helpful to use AI\nto grammatically correct and improve my texts. There's no shame in asking for\nhelp or using tools to improve your language skills at all.  \n  \n---|---|---  \n| |\n\n|\n\ndtx1 6 months ago | root | parent | next [\u2013]\n\n  \n\nSame here and don't interpret it as an accusation, more of an \"huh, that's\ninteresting\", mostly cause I had the suspicion that this was partly written by\nChatGPT and was able to detect it, I'm not entirely sure what made me realize\nit.  \n  \n---|---|---  \n| |\n\n|\n\nmodeless 6 months ago | parent | prev | next [\u2013]\n\n  \n\nRetail GPUs do not have the maxed out VRAM that is desirable for machine\nlearning. And it's cheaper to run them, but it's not free.  \n  \n---|---|---  \n| |\n\n|\n\nimmibis 6 months ago | parent | prev | next [\u2013]\n\n  \n\nPresuming you don't want an A100 for just a few hours.  \n  \n---|---|---  \n| |\n\n|\n\nprotortyp 6 months ago | parent | prev | next [\u2013]\n\n  \n\nI agree. Even just a bunch of RTX 3060s go a long way[1]. Also, in the case of\nmy startup we can use our local storage vs uploading dozens of terabytes of\ndata to the cloud and deal with data privacy issues.\n\n[1] https://timdettmers.com/2023/01/16/which-gpu-for-deep-learni...  \n  \n---|---|---  \n| |\n\n|\n\ncookieperson 6 months ago | parent | prev | next [\u2013]\n\n  \n\nThis is the actual answer. Why rent when you can own. The beefy GPUs of today\nwill still carry their weight years into the future. If you take it even\nremotely seriously you'll save money and have resale value  \n  \n---|---|---  \n| |\n\n|\n\ncircuit10 6 months ago | root | parent | next [\u2013]\n\n  \n\nBecause you\u2019ll probably be only be using it for a small fraction of the time\nyou have it, so sharing can be much more cost effective depending on how much\nyou use it  \n  \n---|---|---  \n| |\n\n|\n\ncookieperson 6 months ago | root | parent | next [\u2013]\n\n  \n\nI dropped a couple hundred bucks on a GPU years ago. It's done a lot of work\nfor me and has years to go. True if you don't plan on using it no point in\nbuying it. Same goes for literally anything  \n  \n---|---|---  \n| |\n\n|\n\nmeinheld111 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nTrue. As with most rental models, there is a usage threshold from wich on it\u2019s\nmore cost effective.  \n  \n---|---|---  \n| |\n\n|\n\ncrabbone 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\n> Why rent when you can own.\n\nThis is a very silly question... the answer is the same as with virtually\nanything else: real estate, transportation, wedding dresses...\n\nI mean, you need to compare the price that will ultimately depend on the kind\nof thing you are doing... that's all. Sometimes it makes sense to rent, other\ntimes it makes sense to buy. There's no one right answer.  \n  \n---|---|---  \n| |\n\n|\n\nraphaelj 6 months ago | prev | next [\u2013]\n\n  \n\nStill pretty expensive IMO.\n\nI'm building a GPU accelerated signal processing web-app, and I'm currently\nplanning to deploy it on a $180 Nvidia Jetson. Performances are a little bit\nlow, but mostly enough to make the app usable.\n\nI tried to find GPU instances for < $50/month, but couldn't find any. The only\nalternative would have been to rent Apple M1 instances at Scaleway, but it's\nstill way more expensive (\u20ac80/month) than hosting my 10 watts Jetson at home.\n\nDoes anybody know cheaper GPU instances? I don't need much computing power,\nabout 0.5Tflops to 1TFlops  \n  \n---|---|---  \n| |\n\n|\n\nsabalaba 6 months ago | parent | next [\u2013]\n\n  \n\nLambda Cloud has A100 40GB for $1.10/hr\u2014less than half the price of OP). We\nalso just launched H100s for $2.40/hr.\n\nhttps://lambdalabs.com/service/gpu-cloud  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nEngaging in self-promotion without taking the time to understand the context\nis not helpful. Moreover, the estimated cost of approximately $800 per month\nwould still apply, which may not be within the desired budget.  \n  \n---|---|---  \n| |\n\n|\n\nVeen 6 months ago | root | parent | next [\u2013]\n\n  \n\nHow dare someone self-promote in the midst of your self-promotion?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nI am answering to the version before you edited it.\n\nYou are absolutely right, and I apologize if I came across as dismissive. We\nmay have different viewpoints, and it's important to respect and acknowledge\neach other's perspectives. I appreciate your input, and I'm here to listen and\naddress any further questions or concerns.\n\nLet's keep the conversation friendly and open.  \n  \n---|---|---  \n| |\n\n|\n\nReDeiPirati 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nwhat do you mean with \"the estimated cost of approximately $800 per month\"?  \n  \n---|---|---  \n| |\n\n|\n\njermaustin1 6 months ago | root | parent | next [\u2013]\n\n  \n\nParent is saying $1.10/hr = $792/mo of GPU leasing. GGP is looking for $50/mo.\nObviously $792 is a lot higher than the GGP's request for $50/mo.\n\nWhether or not $50/mo is an unreasonable ask (it is), they have a budget and\nfeel that even $85/mo is too far out of budget for their application.  \n  \n---|---|---  \n| |\n\n|\n\nnordsieck 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\n> what do you mean with \"the estimated cost of approximately $800 per month\"?\n\nPresumably they mean\n\n$1.1/hr * 24hr/day * 30day/month = $792/month  \n  \n---|---|---  \n| |\n\n|\n\nYetAnotherNick 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\n> We also just launched H100s for $2.40/hr.\n\nWhat's the availability like? I had tried it once and had problem getting any\nGPU.  \n  \n---|---|---  \n| |\n\n|\n\nsabalaba 6 months ago | root | parent | next [\u2013]\n\n  \n\nWe have lots of H100s available right now.  \n  \n---|---|---  \n| |\n\n|\n\nwilliamstein 6 months ago | root | parent | next [\u2013]\n\n  \n\nThanks! Is there a status page like https://oblivus.com/availability/ for\nLambda Cloud? I got very excited about building a product on top of Lambda\ncloud for cocalc.com, but after read lots of docs, when I tried Lambda cloud,\nI just got the message: \"We are currently out of capacity for all instances.\nPlease check again in a few hours.\" This was on April 27. I immediately\nthought: \"There is no possible way I can build a product on this for my users,\nbut it may be very useful for other tasks, e.g., training.\" If there were\nsomething like https://oblivus.com/availability/, especially with historical\ndata, it would be very useful to appropriately set my expectations about how\nLambda Cloud can best be used.\n\nBy the way, Lambda Stack is VERY impressive. Thanks for maintaining that!  \n  \n---|---|---  \n| |\n\n|\n\nalex_sf 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nLambda availability is awful.  \n  \n---|---|---  \n| |\n\n|\n\ntehsauce 6 months ago | parent | prev | next [\u2013]\n\n  \n\nOn vast.ai I rent 3090s for $0.12 an hour. Nothing comes close in price.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nThey operate a model known as a \"marketplace\" or \"community\" cloud. Unlike us,\nthey utilize servers hosted by individuals in their homes. This is something\ncompletely different.\n\nWhile they may offer lower prices, the infrastructure may have limitations in\nterms of reliability, scalability, and security compared to data center-based\nproviders. Additionally, the performance and availability of the services vary\ndepending on the hosts' hardware and internet connection.  \n  \n---|---|---  \n| |\n\n|\n\nKomoD 6 months ago | root | parent | next [\u2013]\n\n  \n\nNo, it's not just individuals, and not just at home.  \n  \n---|---|---  \n| |\n\n|\n\nmlboss 6 months ago | root | parent | prev | next [\u2013]\n\n  \n\nBiggest problem with vast.ai is data security. It is running on somebody's PC.\nAnybody can get hold of your data.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | prev | next [\u2013]\n\n  \n\nThank you for your feedback!\n\nIf you're working with Jetson, the RTX 4000 may be more powerful than you need\nright now, and it's the least expensive we've got. It's worth exploring if\nthere are cloud service providers that offer lower-end GPUs, as that could\npotentially be a better fit for your requirements and budget.  \n  \n---|---|---  \n| |\n\n|\n\nGC_tris 6 months ago | parent | prev | next [\u2013]\n\n  \n\nLess than 50$ will be really hard, at least in any form of professional setup\n(so not hosted in a random basement ;) ).\n\nOur lowest at Genesis Cloud at this time are instances with an RTX 3060Ti for\n0.20$/hour which adds up to 146$/month (\nhttps://www.genesiscloud.com/pricing#nvidia3060ti ) Though, this includes free\nstorage, no egress fees and has a lot more power than a Jetson.\n\nIf you need to optimize for low cost hosting, did you already check whether\nyou actually must have a GPU for your use case? Modern CPU have some\nimpressing capabilities.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nHere we have another instance of self-promotion that does not align with what\nGGP mentioned. Thank you, Genesis and Lambda, for promoting yourselves in a\nstartup thread. Given your long-standing presence in this industry, I would\nexpect better engagement from you.  \n  \n---|---|---  \n| |\n\n|\n\nzamnos 6 months ago | root | parent | next [\u2013]\n\n  \n\nYou were doing _so_ well, self promoting and engaging with the community on\nyour post earlier. I didn't expect to see you stoop to this level of\ncommenting.\n\nMaybe it's time to step away from the keyboard for a while?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nI appreciate and respect every user who contributes by asking questions,\nproviding feedback, or sharing suggestions. However, it is disappointing and\nunreasonable to witness self-promotion from companies that have been\nestablished in this industry for a considerable period of time under a startup\nthread.\n\nMoreover, the fact that their self-promotion does not align with the intention\nof the original discussion and GGP explains their purpose. Their primary goal\nis not genuinely assisting or finding a solution.\n\nIn such cases, as you can imagine, it's challenging for me to maintain\nrespect.  \n  \n---|---|---  \n| |\n\n|\n\nzamnos 6 months ago | root | parent | next [\u2013]\n\n  \n\nIf you're finding it too challenging, might I reiterate my suggestion to take\na break from the keyboard? it's not a good look.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nYou make a valid point, and I appreciate your suggestion.\n\nI apologize if my previous comment came across as dismissive. I believe I have\nexpressed my viewpoint clearly, but I'm open to further discussion.\n\nLet's continue this conversation in a friendly and respectful manner, but\nafter I come back from my break as you have suggested. :)  \n  \n---|---|---  \n| |\n\n|\n\nmschuster91 6 months ago | prev | next [\u2013]\n\n  \n\n> 9\\. No quotas or complex verification processes. Whether you represent a\n> company, an institution, or you're a researcher, you have full access to our\n> infrastructure without any limitations.\n\nGiven how easy it is to shoot oneself in the foot with stolen credentials or\nbad Terraform code, I'd think it should be possible for users to set their own\nquota that is locked for, say, 24 hours to at least put a cap on abuse\nscenario costs.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nThank you for sharing your idea with us!\n\nJust wanted to clarify; currently, our platform operates on a pre-paid system\nwhere users are required to make a deposit to initiate server usage. We do\noffer the option for users to deposit smaller amounts, starting from $5, which\nhelps minimize potential losses in the event of stolen credentials.\nAdditionally, we have an Auto Top-Up system that can automatically replenish\nyour balance based on the settings you specify. We also send notification\nemails to ensure that any transaction made is intentional and authorized.\n\nAs a result, even if you have full access to our platform, you would need an\naccount balance to deploy or maintain virtual machines.\n\nBut we genuinely appreciate your suggestion and will give serious\nconsideration to incorporating such a security feature into our platform.\nThank you once again for your valuable input!  \n  \n---|---|---  \n| |\n\n|\n\nmydriasis 6 months ago | prev | next [\u2013]\n\n  \n\nGreat time to be \"selling umbrellas\" :) good luck on your business! Exciting\nstuff!  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nThank you so much! :)  \n  \n---|---|---  \n| |\n\n|\n\nmkl 6 months ago | prev | next [\u2013]\n\n  \n\nI feel like I'm missing some things on your pricing page. In the GPU Instances\ntable, what are the units of \"System RAM\"? In the CPU Instances table, what\ndoes \"the cost of RAM is included in the per vCPU price\" mean, in terms of how\nmuch RAM?\n\nIn the Comparison table, some of the savings percentages seem to be calculated\nincorrectly, as the percentage of the price left instead of the percentage of\nthe price saved (e.g. 3.99/5.88 instead of (5.88-3.99)/5.88).\n\nThe pricing is transparent but complicated. I think you need calculator that\nlets you select different options and tells you the price. Maybe you have such\na thing? I didn't find it.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nThank you for your questions, and I apologize for any confusion caused.\n\nWe offer two types of virtual machines: GPU-based and CPU-based. The pricing\nstructure differs slightly for each.\n\nFor GPU-based VMs: The vCPU/GPU and RAM/GPU ratio is set at a minimum of\n1-to-1. This means that if you deploy a VM with one GPU, you will need at\nleast 1 vCPU and 1GB of RAM. As our platform is fully customizable, we bill\nyou for the specific amount of RAM ($0.006/hr per GB) and the number of vCPUs\n($0.011/hr per vCPU) you select.\n\nFor CPU-based VMs: The RAM/vCPU ratio is fixed at 4. In this case, only the\nvCPU is customizable, and you will only be billed for the number of vCPUs you\nchoose. The cost of RAM is already included in the price you pay for the vCPU.\n\nHere's an example: A GPU-based VM with 1x Quadro RTX 4000 ($0.27/hr), 1 vCPU\n($0.011/hr), 1GB of RAM ($0.006/hr), and a 40GB NVMe drive (40 x $0.00011/hr).\n\nAnother example: A CPU-based VM with 1x AMD EPYC Rome ($0.033/hr), 4GB of RAM\n(FREE/INCLUDED), and a 40GB NVMe drive (40 x $0.00011/hr).\n\nWe also have a calculator available on the VM deployment page, where you can\nsee detailed information about the specific configuration you want to deploy.\nOnce you register on our console, you can access the calculator at\nhttps://console.oblivus.com/dashboard/oblivuscloud/deploy/.\n\nPlease note that I have forwarded the comparison calculations internally, and\nthey should be fixed soon. Thank you for bringing it to our attention!  \n  \n---|---|---  \n| |\n\n|\n\nmkl 6 months ago | root | parent | next [\u2013]\n\n  \n\nI think you should have the calculator on the pricing page for everyone, near\nthe top. It would be a good marketing tool to help people decide to sign up.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nDefinitely a great idea, we will do it in the near future. Thank you once\nagain!  \n  \n---|---|---  \n| |\n\n|\n\nvictor106 6 months ago | prev | next [\u2013]\n\n  \n\nSo this is about 30% cheaper than GCP. But GCP sustained use discounts give\nyou a 30% discount if used for the whole month.\n\nAzure and AWS pricing for GPU\u2019s is insane.\n\nlambdalabs seems to be the cheapest but I haven\u2019t used it. Anyone has\nexperience with it?  \n  \n---|---|---  \n| |\n\n|\n\nprotortyp 6 months ago | parent | next [\u2013]\n\n  \n\nThe issue with lambda is that they're always booked out in my experience.  \n  \n---|---|---  \n| |\n\n|\n\nljlolel 6 months ago | root | parent | next [\u2013]\n\n  \n\nNobody goes there any more, it\u2019s impossible to get a table  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | prev | next [\u2013]\n\n  \n\nHello, thank you for providing valuable feedback.\n\nIf you require sustained usage, we offer long-term reservations with discounts\nof up to 50% off our regular on-demand prices. For assistance in selecting the\nmost suitable option for your specific needs, please feel free to email us at\nbusiness@oblivus.com. We'll be happy to assist you.  \n  \n---|---|---  \n| |\n\n|\n\nKomoD 6 months ago | prev | next [\u2013]\n\n  \n\nI suggest using vast.ai instead, much cheaper than this.\n\n\"oblivus\": 1x RTX A5000 for $0.84/hr vast: 4x RTX A5000 for $1.08/hr, 1x RTX\nA5000 for $0.20/hr, and much better and larger selection.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nThis is a duplicate, so I'm pasting my previous answer here as well.\n\n\"They operate a model known as a \"marketplace\" or \"community\" cloud. Unlike\nus, they utilize servers hosted by individuals in their homes. This is\nsomething completely different.\n\nWhile they may offer lower prices, the infrastructure may have limitations in\nterms of reliability, scalability, and security compared to data center-based\nproviders. Additionally, the performance and availability of the services vary\ndepending on the hosts' hardware and internet connection.\"  \n  \n---|---|---  \n| |\n\n|\n\nKomoD 6 months ago | root | parent | next [\u2013]\n\n  \n\nOnce again, no, not just individuals in their homes  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nOkay, then let me paraphrase it differently... While it's possible for\nindividuals or companies to host servers in data centers or actively\nparticipate in the system, this doesn't alter the fact that Vast.ai operates\nas a community cloud. The essential point remains unchanged: they offer a\ndistinct and different service compared to us.  \n  \n---|---|---  \n| |\n\n|\n\nwongarsu 6 months ago | prev | next [\u2013]\n\n  \n\nOn equivalent instances it seems a good bit more expensive than e.g.\nlambdalabs [1], but in turn you get a lot more flexibility. Both in terms of\nhaving more GPU models to choose from, and having much more flexible machine\nconfigurations. Seems like a worthwhile tradeoff.\n\n1: https://lambdalabs.com/service/gpu-cloud#pricing\n\nhttps://oblivus.com/pricing/  \n  \n---|---|---  \n| |\n\n|\n\nkbumsik 6 months ago | parent | next [\u2013]\n\n  \n\nI am using an 8*A100 machine from Lambda Labs, but I honestly cannot\nunderstand how it can be so cheap. I haven't found a cheaper one than this. It\nis even cheaper than most of spot-like interruptible offerings.  \n  \n---|---|---  \n| |\n\n|\n\nYetAnotherNick 6 months ago | root | parent | next [\u2013]\n\n  \n\nAgreed. Even by their on demand price, cost per A100 per year is <$10k. Even\nif they somehow make maintenance and electricity cost to close to 0(which\nlikely won't be the case), and their GPU usage 100% they are running it with\noperational loss for the first year.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | prev | next [\u2013]\n\n  \n\nThank you for your valuable feedback!\n\nWe often receive comparisons to Lambda, but it ultimately boils down to your\nspecific requirements and preferences. As you mentioned, our platform is\ntailored for personalized virtual machine configurations, distinguishing us\nfrom other cloud service providers that offer pre-set configurations. We\nstrongly believe that this level of flexibility is a key differentiator and\ndelivers enhanced value to our customers.  \n  \n---|---|---  \n| |\n\n|\n\nSoftTalker 6 months ago | prev | next [\u2013]\n\n  \n\nInteresting name choice. I read it as \"oblivious\" even after looking at it a\nfew times.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHahaha, yeah,\n\nEven though Oblivus doesn't have a specific meaning, we initially didn't make\nthe connection with \"oblivious\". When we attended Cloudfest recently, many\npeople also pronounced it as \"oblivous,\" so you're not alone in that regard, I\nsuppose. :)  \n  \n---|---|---  \n| |\n\n|\n\narjvik 6 months ago | prev | next [\u2013]\n\n  \n\nDefinitely submit a PR to add Oblivus to https://cloud-gpus.com/  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nWill do! Thank you very much for the suggestion!  \n  \n---|---|---  \n| |\n\n|\n\nbrancz 6 months ago | prev | next [\u2013]\n\n  \n\nIn a world of shared GPUs, what tools do people use to profile their GPU\nworkloads. Are there code level profilers like for CPUs? I typically hear that\npeople don\u2019t often have issues with the code running on the GPU itself but\nrather ensuring it is fed information quickly enough to stay saturated. How do\npeople measure this and find out what to improve, especially in a shared\nsetting?  \n  \n---|---|---  \n| |\n\n|\n\ncagataygurturk 6 months ago | prev | next [\u2013]\n\n  \n\nWhat is your hypervisor technology? How do you ensure isolation between\ntenants?\n\nPS: I see that your VMs are created in CoreWeave datacenters. I wonder the\nrelationship between your company and CoreWeave. Are you just a reseller?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHello,\n\nWe utilize the KVM hypervisor to ensure that all resources are dedicatedly\nallocated to the virtual machine.\n\nThey are among the vendors we collaborate with within the United States. To\nreiterate what I previously mentioned to another user, our on-demand stock\ncomprises a diverse range of resources obtained from multiple vendors. In\naddition to our own infrastructure, we leverage resources from these vendors\nto effectively address the increasing demand for our services.  \n  \n---|---|---  \n| |\n\n|\n\n_0ffh 6 months ago | prev | next [\u2013]\n\n  \n\nRe the pricing I see all prices given per hour (which I think is sensible),\nbut I can't find what the granularity of usage measurement is.\n\nAssume I try something out on a GPU server and I realize I made some wrong\nassumption in my code and I need to think things over, so I shut down the\ninstance after 2h 18min 24s of usage. What do I pay for?\n\nEd. Nevermind, I found the answer here\nhttps://docs.oblivus.com/billing/payment-plans, seems to be per minute, so I\nassume 2h 19min.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHello,\n\nYes, we bill our services on a minute basis, so you will only be charged for\nthe actual usage duration, which in this case would be 139 minutes (2 hours\nand 19 minutes). You can find detailed information about the resources you\nhave used and their respective costs on the Billing > Invoices page.\n\nI hope this helps!  \n  \n---|---|---  \n| |\n\n|\n\ncoleca 6 months ago | prev | next [\u2013]\n\n  \n\nI always thought there were license restrictions from Nvidia preventing the\nuse of their consumer grade GPUs in datacenters / cloud environments. Is this\nnot the case?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHello, thank you for the question!\n\nTo clarify, we don't offer any consumer cards (RTX 3080, RTX 4090, etc)\nmentioned in the thread. Instead, we provide professional cards from the RTX\nSeries such as the RTX 4000 and RTX 5000.  \n  \n---|---|---  \n| |\n\n|\n\nmattpavelle 6 months ago | prev | next [\u2013]\n\n  \n\nWhile vast.ai (see other comments) may not be a direct competitor, Runpod is.\nAnd Runpod is significantly cheaper https://www.runpod.io/gpu-instance/pricing\n\nRunpod: A100 80GB for $1.990/hr\n\nvs\n\nOblivus: A100 80GB for $2.41/hr\n\nHow do you justify being significantly more expensive?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHello, thank you for your question!\n\nIt's important to note that our platform offers full customizability, while\nthey only offer pre-set configurations. This level of flexibility comes with a\ntrade-off in terms of pricing.\n\nRegarding their infrastructure and system, I don't have specific information.\nHowever, I couldn't find any details about long-term reservations on their\nwebsite. In comparison, -for example- we offer reserved instances with A100s\nstarting as low as $1.2 per hour.\n\nThere are also other factors to consider. We prioritize investing in data\ncenter infrastructure, security, quality, and reliability. For instance, we\nprovide up to 40Gbps public, 200Gbps private network connectivity for each\nvirtual machine and offer 3x replication for storage. These features come with\ntheir own trade-offs, and each company has its own pricing structure.\n\nI hope this clarifies the situation!  \n  \n---|---|---  \n| |\n\n|\n\njayonsoftware 6 months ago | prev | next [\u2013]\n\n  \n\nCan I put an instance to sleep and just pay for the storage ?Could not find\nthe info on your site.\n\nAlso for Windows instance, what do people use to connect. RDP seems slow. My\nprimary use case is software development trying to learn CUDA programing.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHello, thank you for your questions!\n\nAt the moment, when you shut down a server, you are only billed for the\nstorage and the IP address associated with it. However, we are working on\nimplementing a feature in the near future that will allow you to detach the IP\naddress, enabling you to pay solely for the storage usage.\n\nRegarding Windows servers, the default software for remote access is RDP.\nHowever, you have the freedom to install and use any other remote access\nsoftware of your choice on our platform.\n\nHope this clarifies things for you!  \n  \n---|---|---  \n| |\n\n|\n\ndr_kiszonka 6 months ago | prev | next [\u2013]\n\n  \n\nHow does cloud gaming work from a customer perspective? I see it mentioned on\nthe main page but I haven't found any additional information about it (on your\nmobile page).  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nThank you for your question!\n\nWe offer Windows (BYOL) images on our virtual machines. If you have a good\nlatency to our servers and sufficient bandwidth to handle the streaming, you\ncan enjoy cloud gaming using any software of your choice.\n\nTo get started, you can deploy a CPU-based Windows virtual machine for as low\nas $0.019/hr. Once you have installed your games, you can stop the virtual\nmachine and modify it into a GPU-based VM for as low as $0.29/hr. While your\nserver is running, you will be billed for all the components. However, if you\nshut down the server, you will only be billed for the storage and the IP\naddress. Your data will remain safe as long as you don't delete the server,\nallowing you to start playing again whenever you want.\n\nIn the coming days, we will be implementing a feature that allows you to\ndetach the IP address, resulting in more cost-effective billing, where you\nwill only be billed for the storage.\n\nI hope this information helps!  \n  \n---|---|---  \n| |\n\n|\n\ndr_kiszonka 6 months ago | root | parent | next [\u2013]\n\n  \n\nNeat - thanks! What I would find helpful is a calculator to estimate the total\ncost per day, including storage, ram, etc. It would be great if it allowed me\nto easily answers questions like this one.\n\nAssuming that:\n\n\\- I game 2 hours a day,\n\n\\- I need 50 GB for Windows and another 200 GB for games,\n\n\\- I need 16 GB of ram and an equivalent of RTX 4080,\n\nwhat would my total cost per day would be?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nWe have plans to add a calculator in the coming days to make it easier for you\nto estimate costs. Currently, the console deploy page is the only place where\nyou can see the hourly pricing.\n\nAs for finding a GPU equivalent to the RTX 4080, it's challenging as it is a\nnew release and we don't have benchmarks for it yet. However, we can provide\npricing estimates for the RTX 5000 and RTX 4000 GPUs.\n\nFor the RTX 4000, the approximate cost would be around $24, while for the RTX\n5000, it would be around $45. Please keep in mind that these are estimates and\nthe actual pricing may vary. Additionally, the optimization we are planning to\nimplement for IP addresses in the near future will further improve the overall\npricing.  \n  \n---|---|---  \n| |\n\n|\n\n71a54xd 6 months ago | prev | next [\u2013]\n\n  \n\nCurious how this compares to Tensordock? I've had great experiences with their\nservers + support. Vastly preferable to other first party / distributed\noptions.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHi there, thank you for your inquiry!\n\nUpon review of their system, it appears that although there may be\nsimilarities, there are some notable differences between their service and\nours.\n\nSpecifically, it seems that their CPU-based virtual machines come with preset\nconfigurations, lacking full customizability. Additionally, I was unable to\nlocate an additional storage solution, reserved instances, organization and\nauto top-up system on their platform.\n\nOn the other hand, we do not plan on offering anything similar to their\n\"marketplace cloud\" with individual hosts.  \n  \n---|---|---  \n| |\n\n|\n\n71a54xd 6 months ago | root | parent | next [\u2013]\n\n  \n\nThanks!  \n  \n---|---|---  \n| |\n\n|\n\nAlifatisk 6 months ago | prev | next [\u2013]\n\n  \n\nI\u2019m new to this cloud gpu thing, how do I use this?\n\nDo I get access to a vm with powerful gpus?\n\nI would love to be able to execute dockerfiles over the cli that runs gpu\nintensive executables.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHello!\n\nTo begin, simply create an account at https://console.oblivus.com and add a\nminimum of $5 to your account balance. Alternatively, you can utilize the\npromo code HN_1 on the same page for testing purposes.\n\nOnce you deploy your server, you'll have access to a dedicated virtual\nmachine, providing you with the capability to perform any tasks you would\ntypically carry out on your own computer. You also have the freedom to execute\nDocker within your virtual machine.\n\nPlease have a look at our documentation at https://docs.oblivus.com, where we\ntried to explain everything with images directly from our console. If you have\nany other questions, I'd be happy to answer them!  \n  \n---|---|---  \n| |\n\n|\n\ncollaborative 6 months ago | root | parent | next [\u2013]\n\n  \n\nThat page is not mobile friendly FYI :-)  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | root | parent | next [\u2013]\n\n  \n\nThank you for bringing this to our attention.\n\nOur team will look into this matter internally to ensure that everything is\nfunctioning properly. We appreciate your feedback!  \n  \n---|---|---  \n| |\n\n|\n\ncheptsov 6 months ago | prev | next [\u2013]\n\n  \n\nLooks really cool. Congrats on the launch! A quick question. Do you allow to\nbuild custom public images? We\u2019d love to integrate dstack.ai with Oblivus.  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nThank you very much!\n\nWe are currently working on developing a feature that will allow users to\ncreate snapshots of their system disks and use those images to create multiple\ninstances. We expect this feature to be available to everyone in a few weeks.\n\nIf you don't mind, could you please send an email to doruk@oblivus.com so that\nwe can discuss this further?  \n  \n---|---|---  \n| |\n\n|\n\nteelelbrit 6 months ago | prev | next [\u2013]\n\n  \n\nHey, we produce AI tutorials on youtube - it'd be cool if we could maybe\nproduce a video showcasing your GPU cloud?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHello!\n\nThat sounds like an excellent suggestion. Would you mind sending an email to\ndoruk@oblivus.com with a brief description of the video and details about your\nchannel?\n\nThanks!  \n  \n---|---|---  \n| |\n\n|\n\ntransformi 6 months ago | prev | next [\u2013]\n\n  \n\nwhat is the differentiation from RunPod? (they have pretty much same prices\nbut with more services..)  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHello, thank you for your question.\n\nThe main difference between our platform and theirs is that we offer fully\ncustomizable configurations. From vCPU to RAM, Disk, and GPU, you have the\nflexibility to customize each aspect according to your specific needs. Unlike\npre-set configurations offered by other providers, our platform allows you to\ntailor your virtual machines to your exact requirements.  \n  \n---|---|---  \n| |\n\n|\n\nmichele_f 6 months ago | prev | next [\u2013]\n\n  \n\nIs there any chance you\u2019re going to offer the service in EU?  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent | next [\u2013]\n\n  \n\nHello, thank you so much for the question!\n\nWe have plans to expand to the EU, potentially in the third quarter of 2023.\n\nBuilding our current infrastructure in the EU is quite difficult, especially\nwith the high electricity prices in EU data centers. However, we have been in\ndiscussions with various providers to establish an infrastructure that meets\nour requirements.\n\nAdditionally, we are actively working on a solution called Oblivus Edge\nDeployment, which aims to offer low-latency and high-bandwidth connectivity to\nEU users, even if the server is located in the US. We expect to release this\ntechnology in the next few weeks.  \n  \n---|---|---  \n| |\n\n|\n\nA6gYPfxNas 6 months ago | prev [\u2013]\n\n  \n\nGreat to see another option in the space! Good luck!  \n  \n---|---|---  \n| |\n\n|\n\noblivuslimited 6 months ago | parent [\u2013]\n\n  \n\nThank you so much! :)  \n  \n---|---|---  \n  \n  \n  \n|  \n---  \n  \nGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | Contact  \n  \nSearch:\n\n"
}