{
    "summary": "Products\n\nNVIDIA HGX H100GPU ComputeCPU ComputeKubernetesVirtual\nServersStorageNetworking\n\nSolutions\n\nVFX & RenderingMachine Learning & AIInference ServicePixel Streaming\n\nWhy CoreWeavePricingDocumentation\n\nCompany\n\nAbout UsBlogEvents & WebinarsIn the NewsContact SalesCareersStatusAccelerator\nProgram\n\nContact Us\n\nLogin\n\nX\n\nNVIDIA HGX H100s are here, starting at $2.23/hr. Learn More\n\n\u2715\n\n\u2715\n\nCoreWeave Cloud\n\n# The GPU Cloud.\n\nCoreWeave is a specialized cloud provider, delivering a massive scale of GPUs\non top of the industry\u2019s fastest and most flexible infrastructure.\n\nGet in Touch\n\nModern Infrastructure, Best-in-Class Tech Stack\n\n## CoreWeave Cloud Architecture\n\nA modern, Kubernetes native cloud that\u2019s purpose-built for large scale, GPU-\naccelerated workloads. Designed with engineers and innovators in mind,\nCoreWeave offers unparalleled access to a broad range of compute solutions\nthat are up to 35x faster and 80% less expensive than legacy cloud providers.\n\n35x\n\nFaster  \n\u200d\n\n80%\n\nLESS EXPENSIVE  \n\u200d\n\n50%\n\nREDUCTION IN LATENCY  \n\u200d\n\n5\n\nTier 4 Data Centers in  \nNorth America\n\n##  **CoreWeave\u2019s Infrastructure**\n\nEach component of our infrastructure has been carefully designed to help our\nclients access the scale and variety of compute they need to create and\ninnovate.\n\n* * *\n\n### Need GPUs on-demand at scale?\n\n#### GPU Compute\n\nThe industry\u2019s broadest range of NVIDIA GPUs, highly configurable and highly\navailable.\n\n  * 11+ NVIDIA GPU SKUs\n\n  * Available on demand\n\nLearn More\n\n### Have a general-purpose compute project?\n\n#### CPU Compute\n\nMassive scale of CPU-only instances when GPU acceleration isn\u2019t required.\n\n  * Intel Xeon and AMD Epyc\n\n  * Scale in seconds\n\nLearn More\n\n### Prefer containerized deployments?\n\n#### Kubernetes\n\nFully managed Kubernetes, delivering the performance of bare-metal without the\ninfrastructure overhead.\n\n  * Spin-up new instances in seconds\n\n  * Responsive auto-scaling across thousands of GPUs\n\nLearn More\n\n### Need to deploy in Virtual Machines?\n\n#### Virtual Servers\n\nEasily deploy and manage NVIDIA GPU accelerated and CPU-only Virtual Servers.\n\n  * Linux, Windows or bring your own ISO\n\n  * Out-of-the-box desktop streaming via Teradici and Parsec\n\nLearn More\n\n### Fed up with inflexible storage?\n\n#### Storage\n\nDistributed and fault tolerant storage, with triple replication, managed\nseparately from compute.\n\n  * Easily resize volumes and scale capacity\n\n  * IOPS and throughput optimized\n\nLearn More\n\n### Ready for networking that won\u2019t slow you down?\n\n#### Networking\n\nEndless horizontal scaling with routing, switching, firewalling and load-\nbalancing built into the network fabric. And we won\u2019t charge you for egress\nlike the other guys.\n\n  * Built to power HPC workloads with ease\n\n  * Scale to 100Gbps+\n\nLearn More\n\n##  **Compute Resources for Any Use Case**\n\n###  **Machine Learning & AI**\n\nAccess compute resources that match the complexity of your models on an\ninfrastructure that empowers you to run inference at scale.\n\n###  **VFX & Rendering**\n\nAccelerate your team\u2019s workflows with a cloud-based production pipeline or\naccess render capacity at scale.\n\n###  **Pixel Streaming**\n\nServe new users 35x faster at half the cost without the burden of extensive\nresource planning.\n\n## What Others Say\n\nFrom our clients to our partners, we strive to provide best-in-class solutions\nto  \ndrive innovation and fast, flexible experiences.\n\nRead case study \u2192\n\n\u201cCoreWeave provides us with virtual workstations that have high-end NVIDIA\nGPUs. Not just for the individual artists, but also for rendering on the queue\nas well. If we need to provision hundreds of GPU\u2019s for a long sequence, we are\nable to do that quickly and easily \u2013 and that\u2019s been awesome.\u201d\n\nRajesh Sharma\n\n,\n\nVP of Engineering at Spire Animation Studios\n\nRead case study \u2192\n\n\u201cCoreWeave is a valued, dedicated partner of NVIDIA. As such, they were named\nour first Elite Cloud Solutions Provider for Compute in the NVIDIA Partner\nNetwork. By offering their clients a tremendously broad range of compute\noptions - from A100s to A40s - at unprecedented scale and their commitment to\ndelivering world-class results in AI, machine learning, visual effects and\nmore. NVIDIA is a proud supporter of CoreWeave.\u201d\n\nMatt McGrigg\n\n,\n\nGlobal Director Business Development, Cloud & Strategic Partners, NVIDIA\n\nRead case study \u2192\n\n\u201cAlways having access to GPUs on-demand has been a huge sanity saver. The\navailability and reliability of CoreWeave\u2019s service allowed us to serve our\ncurrent models and continuously build and test new ideas.\u201d\n\nYasu Seno\n\n,\n\nCEO, Bit192, Inc.\n\nRead case study \u2192\n\n\u201cAfter a few months of struggling to keep up with demand at mega-cloud prices,\nwe were able to seamlessly move our cloud infrastructure over to CoreWeave.\nUsing CoreWeave\u2019s Inference Optimized V100 instances, our inference latencies\ndropped by 50% and the cost savings from our partnership allowed us to\ncontinue delivering our free-tier experience and substantially reduce our\ncost-per-user.\u201d\n\nNick Walton\n\n,\n\nFounder & CEO of Latitude, the creator of AI Dungeon\n\nRead case study \u2192\n\n\u201cBy pairing the NVIDIA Quadro RTX and RTX A6000 GPUs with PCoIP technology\nfrom Teradici, artists get access to ultra-low latency, high powered GPU-\naccelerated Virtual Workstations that allow them to work at \u2018human-speed.\u2019\nNow, we can tackle projects that in the past, our hardware could have never\nhandled, and our team can truly showcase their talents without being held back\nby tech restraints.\u201d\n\nAndrew Bly\n\n,\n\nCEO at Molecule VFX\n\nRead case study \u2192\n\n\u201cCoreWeave\u2019s deployment architecture enables us to scale up extremely fast\nwhen there is more demand. We are able to serve requests 3x faster after\nmigrating to CoreWeave, leading to a much better user experience while saving\n75% in cloud costs. For the users, this means the generation speeds will never\nslow down, even when there is peak load.\u201d\n\nEren Do\u011fan\n\n,\n\nCEO, NovelAI\n\nRead case study \u2192\n\n\u201cWith my previous platform, I couldn't choose the hardware \u2013 I was locked into\na Tesla K80. With CoreWeave, I\u2019m less worried about the service scaling up if\nwe do a big marketing campaign or get mentioned by a big influencer, which has\ngiven me a lot of confidence and means we can think bigger.\u201d\n\nAngus Russell\n\n,\n\nFounder, NightCafe\n\nRead case study \u2192\n\n\u201cCoreWeave is an anchor provider of compute infrastructure for our transcoding\nnetwork. The ability to Right-Size our workloads across CoreWeave\u2019s diverse\ninfrastructure set allows us to substantially reduce our compute costs and\npass those savings along to our clients.\u201d\n\nDoug Petkanics\n\n,\n\nCEO, Livepeer\n\n## In the News\n\nVAST Data Powers the Storage for CoreWeave AI InfrastructureVAST Data Powers\nthe Storage for CoreWeave AI Infrastructure\n\nSeptember 27, 2023\n\nA Startup in the New Jersey Suburbs Is Battling the Giants of Silicon ValleyA\nStartup in the New Jersey Suburbs Is Battling the Giants of Silicon Valley\n\nAugust 25, 2023\n\nHow Nvidia Built a Competitive Moat Around A.I. ChipsHow Nvidia Built a\nCompetitive Moat Around A.I. Chips\n\nAugust 21, 2023\n\nNvidia\u2019s Best AI Chips Sold Out Until 2024, Says Leading Cloud GPU\nProviderNvidia\u2019s Best AI Chips Sold Out Until 2024, Says Leading Cloud GPU\nProvider\n\nAugust 9, 2023\n\nSee More\n\n## Ready to get started?  \n\nGet in Touch\n\nCoreWeave is a specialized cloud provider, delivering a massive scale of GPUs\non top of the industry\u2019s fastest and most flexible infrastructure.  \n  \n101 Eisenhower Pkwy Ste 106  \nRoseland NJ 07068-1050  \n\n  * Why CoreWeave\n  * Documentation\n  * Pricing\n  * Contact Sales\n  * About Us\n  * Support\n\n  * Careers\n  * Blog\n  * Events & Webinars\n  * Login\n  * Status\n  * Terms of Service\n\nX\n\nNVIDIA HGX H100s are here, starting at $2.23/hr. Learn More\n\n\u2715\n\n\u2715\n\n",
    "links": "[{\"link\": \"https://www.coreweave.com/\", \"text\": \"\"}, {\"link\": \"https://www.coreweave.com/products/hgx-h100\", \"text\": \"NVIDIA HGX H100\"}, {\"link\": \"https://www.coreweave.com/products/gpu-compute\", \"text\": \"GPU Compute\"}, {\"link\": \"https://www.coreweave.com/products/cpu-compute\", \"text\": \"CPU Compute\"}, {\"link\": \"https://www.coreweave.com/products/kubernetes\", \"text\": \"Kubernetes\"}, {\"link\": \"https://www.coreweave.com/products/virtual-servers\", \"text\": \"Virtual Servers\"}, {\"link\": \"https://www.coreweave.com/products/storage\", \"text\": \"Storage\"}, {\"link\": \"https://www.coreweave.com/products/networking\", \"text\": \"Networking\"}, {\"link\": \"https://www.coreweave.com/solutions/vfx-rendering\", \"text\": \"VFX & Rendering\"}, {\"link\": \"https://www.coreweave.com/solutions/machine-learning-ai\", \"text\": \"Machine Learning & AI\"}, {\"link\": \"https://www.coreweave.com/solutions/inference-service\", \"text\": \"Inference Service\"}, {\"link\": \"https://www.coreweave.com/solutions/pixel-streaming\", \"text\": \"Pixel Streaming\"}, {\"link\": \"https://www.coreweave.com/why-coreweave\", \"text\": \"Why CoreWeave\"}, {\"link\": \"https://www.coreweave.com/gpu-cloud-pricing\", \"text\": \"Pricing\"}, {\"link\": \"https://www.coreweave.com/about-us\", \"text\": \"About Us\"}, {\"link\": \"https://www.coreweave.com/blog\", \"text\": \"Blog\"}, {\"link\": \"https://www.coreweave.com/events\", \"text\": \"Events & Webinars\"}, {\"link\": \"https://www.coreweave.com/press\", \"text\": \"In the News\"}, {\"link\": \"https://www.coreweave.com/contact-us\", \"text\": \"Contact Sales\"}, {\"link\": \"https://www.coreweave.com/careers\", \"text\": \"Careers\"}, {\"link\": \"https://www.coreweave.com/coreweave-accelerator-program\", \"text\": \"Accelerator Program\"}, {\"link\": \"https://www.coreweave.com/contact-us\", \"text\": \"Contact Us\"}, {\"link\": \"https://www.coreweave.com/products/hgx-h100\", \"text\": \"Learn More\"}, {\"link\": \"https://www.coreweave.com/contact-us\", \"text\": \"Get in Touch\"}, {\"link\": \"https://www.coreweave.com/products/gpu-compute\", \"text\": \"Learn More\"}, {\"link\": \"https://www.coreweave.com/products/cpu-compute\", \"text\": \"Learn More\"}, {\"link\": \"https://www.coreweave.com/products/kubernetes\", \"text\": \"Learn More\"}, {\"link\": \"https://www.coreweave.com/products/virtual-servers\", \"text\": \"Learn More\"}, {\"link\": \"https://www.coreweave.com/products/storage\", \"text\": \"Learn More\"}, {\"link\": \"https://www.coreweave.com/products/networking\", \"text\": \"Learn More\"}, {\"link\": \"https://www.coreweave.com/solutions/machine-learning-ai\", \"text\": \"\"}, {\"link\": \"https://www.coreweave.com/solutions/vfx-rendering\", \"text\": \"\"}, {\"link\": \"https://www.coreweave.com/solutions/pixel-streaming\", \"text\": \"\"}, {\"link\": \"https://www.coreweave.com/blog/coreweave-empowers-real-time-collaborative-filmmaking-with-spire-animation-studios\", \"text\": \"Read case study \u2192\"}, {\"link\": \"https://www.coreweave.com/blog/coreweave-becomes-nvidias-first-elite-cloud-services-provider-for-compute\", \"text\": \"Read case study \u2192\"}, {\"link\": \"https://www.coreweave.com/blog/coreweave-and-bit192-inc-help-gpt-neox-20b-reach-japan\", \"text\": \"Read case study \u2192\"}, {\"link\": \"https://www.coreweave.com/blog/how-ai-dungeons-cloud-adventure-with-coreweave-improved-3-critical-kpis\", \"text\": \"Read case study \u2192\"}, {\"link\": \"https://www.coreweave.com/blog/how-molecule-vfx-accelerated-their-vfx-pipeline-with-coreweave\", \"text\": \"Read case study \u2192\"}, {\"link\": \"https://www.coreweave.com/blog/how-coreweave-helped-novelai-grow-from-0-to-40k-users-in-3-months\", \"text\": \"Read case study \u2192\"}, {\"link\": \"https://www.coreweave.com/blog/pushing-the-boundaries-of-ai-powered-art-with-nightcafe\", \"text\": \"Read case study \u2192\"}, {\"link\": \"https://www.coreweave.com/blog/livepeer-scaling-batch-processing-workloads-with-cost-effective-cloud-infrastructure\", \"text\": \"Read case study \u2192\"}, {\"link\": \"https://www.coreweave.com/blog/vast-data-powers-the-storage-for-coreweave-ai-infrastructure\", \"text\": \"VAST Data Powers the Storage for CoreWeave AI Infrastructure\"}, {\"link\": \"https://www.coreweave.com/blog/vast-data-powers-the-storage-for-coreweave-ai-infrastructure\", \"text\": \"\"}, {\"link\": \"https://www.coreweave.com/blog/a-startup-in-the-new-jersey-suburbs-is-battling-the-giants-of-silicon-valley\", \"text\": \"A Startup in the New Jersey Suburbs Is Battling the Giants of Silicon Valley\"}, {\"link\": \"https://www.coreweave.com/blog/a-startup-in-the-new-jersey-suburbs-is-battling-the-giants-of-silicon-valley\", \"text\": \"\"}, {\"link\": \"https://www.coreweave.com/blog/how-nvidia-built-a-competitive-moat-around-a-i-chips\", \"text\": \"How Nvidia Built a Competitive Moat Around A.I. Chips\"}, {\"link\": \"https://www.coreweave.com/blog/how-nvidia-built-a-competitive-moat-around-a-i-chips\", \"text\": \"\"}, {\"link\": \"https://www.coreweave.com/blog/nvidias-best-ai-chips-sold-out-until-2024-says-leading-cloud-gpu-provider\", \"text\": \"Nvidia\u2019s Best AI Chips Sold Out Until 2024, Says Leading Cloud GPU Provider\"}, {\"link\": \"https://www.coreweave.com/blog/nvidias-best-ai-chips-sold-out-until-2024-says-leading-cloud-gpu-provider\", \"text\": \"\"}, {\"link\": \"https://www.coreweave.com/press\", \"text\": \"See More\"}, {\"link\": \"https://www.coreweave.com/contact-us\", \"text\": \"Get in Touch\"}, {\"link\": \"https://www.coreweave.com/why-coreweave\", \"text\": \"Why CoreWeave\"}, {\"link\": \"https://www.coreweave.com/gpu-cloud-pricing\", \"text\": \"Pricing\"}, {\"link\": \"https://www.coreweave.com/contact-us\", \"text\": \"Contact Sales\"}, {\"link\": \"https://www.coreweave.com/about-us\", \"text\": \"About Us\"}, {\"link\": \"https://www.coreweave.com/careers\", \"text\": \"Careers\"}, {\"link\": \"https://www.coreweave.com/blog\", \"text\": \"Blog\"}, {\"link\": \"https://www.coreweave.com/events\", \"text\": \"Events & Webinars\"}, {\"link\": \"https://www.coreweave.com/products/hgx-h100\", \"text\": \"Learn More\"}]",
    "priceAndPlans": "Products\n\nNVIDIA HGX H100sGPU ComputeCPU ComputeKubernetesVirtual\nServersStorageNetworking\n\nSolutions\n\nVFX & RenderingMachine Learning & AIInference ServicePixel Streaming\n\nWhy CoreWeavePricingDocumentation\n\nCompany\n\nAbout UsBlogEvents & WebinarsIn the NewsContact SalesCareersStatusAccelerator\nProgram\n\nContact Us\n\nLogin\n\nX\n\nNVIDIA HGX H100s are here, starting at $2.23/hr. Learn More\n\n\u2715\n\n\u2715\n\n# CoreWeave Cloud Pricing\n\nCoreWeave's pricing is designed for flexibility. Instances are highly\nconfigurable, giving you the freedom to customize GPU, CPU, RAM, and storage\nrequests when scheduling your workloads.  \n  \nOur entire infrastructure is purpose-built for compute-intensive workloads,\nand everything from our servers to our storage and networking solutions are\ndesigned to deliver best-in-class performance that are  \nup to **35x faster and 80% less expensive** than generalized public clouds.  \n\n## CoreWeave GPU Cloud Pricing\n\nCoreWeave Cloud GPU instance pricing is highly flexible, and meant to provide\nyou with ultimate control over configuration and cost. Pricing below is a la\ncarte, where the total instance cost is a combination of a GPU component, the\nnumber of vCPU, and the amount of RAM allocated. To keep things simple, CPU\nand RAM cost are the same per base unit, and the only variable is the GPU\nchosen for your workload or Virtual Server.  \n  \nA valid GPU instance configuration must include at least 1 GPU, at least 1\nvCPU and at least 2GB of RAM. When deploying a Virtual Server, the GPU\ninstance configuration must also include at least 40GB of root disk NVMe tier\nstorage.  \n\nGPU Model\n\nVRAM (GB)\n\nMax vCPUs per GPU  \n( **$0.01** /hr)\n\nMax RAM (GB) per GPU  \n($0.005/hr)\n\nGPU Component  \nCost Per Hour\n\nNVIDIA HGX H100\n\n80\n\n48\n\n256\n\n **$4.76** **  \nReserve Now**\n\nNVIDIA H100 PCIe\n\n80\n\n48\n\n256\n\n **$4.25**\n\nA100 80GB NVLINK\n\n80\n\n48\n\n256\n\n **$2.21**\n\nA100 80GB PCIe\n\n80\n\n48\n\n256\n\n **$2.21**\n\nA100 40GB NVLINK\n\n40\n\n48\n\n256\n\n **$2.06**\n\nA100 40GB PCIe\n\n40\n\n48\n\n256\n\n **$2.06**\n\nA40\n\n48\n\n48\n\n256\n\n **$1.28**\n\nRTX A6000\n\n48\n\n48\n\n256\n\n **$1.28**\n\nRTX A5000\n\n24\n\n36\n\n128\n\n **$0.77**\n\nRTX A4000\n\n16\n\n36\n\n128\n\n **$0.61**\n\nQuadro RTX 5000\n\n16\n\n36\n\n128\n\n **$0.57**\n\nQuadro RTX 4000\n\n8\n\n36\n\n128\n\n **$0.24**\n\nTesla V100 NVLINK\n\n16\n\n36\n\n128\n\n **$0.80**\n\nNVIDIA HGX H100\n\n **Reserve Now**\n\n **80** VRAM\n\n **48** Max CPUs\n\n **256** Max RAM\n\nNVIDIA H100 PCIe\n\n **$4.25** / Hour\n\n **80** VRAM\n\n **48** Max CPUs\n\n **256** Max RAM\n\nA100 80GB NVLINK\n\n **$2.21** / Hour\n\n **80** VRAM\n\n **48** Max CPUs\n\n **256** Max RAM\n\nA100 80GB PCIe\n\n **$2.21** / Hour\n\n **80** VRAM\n\n **48** Max CPUs\n\n **256** Max RAM\n\nA100 40GB NVLINK\n\n **$2.16** / Hour\n\n **40** VRAM\n\n **48** Max CPUs\n\n **256** Max RAM\n\nA100 40GB PCIe\n\n **$2.06** / Hour\n\n **40** VRAM\n\n **48** Max CPUs\n\n **256** Max RAM\n\nA40\n\n **$1.28** / Hour\n\n **48** VRAM\n\n **48** Max CPUs\n\n **256** Max RAM\n\nRTX A6000\n\n **$1.28** / Hour\n\n **48** VRAM\n\n **48** Max CPUs\n\n **256** Max RAM\n\nRTX A5000\n\n **$0.77** / Hour\n\n **24** VRAM\n\n **36** Max CPUs\n\n **128** Max RAM\n\nRTX A4000\n\n **$0.61** / Hour\n\n **16** VRAM\n\n **36** Max CPUs\n\n **128** Max RAM\n\nQuadro RTX 5000\n\n **$0.57** / Hour\n\n **16** VRAM\n\n **36** Max CPUs\n\n **128** Max RAM\n\nQuadro RTX 4000\n\n **$0.24** / Hour\n\n **8** VRAM\n\n **36** Max CPUs\n\n **128** Max RAM\n\nTesla V100 NVLINK\n\n **$0.80** / Hour\n\n **16** VRAM\n\n **36** Max CPUs\n\n **128** Max RAM\n\n*Compute instances on CoreWeave Cloud are configurable. For more info, please refer to our Resource Based Pricing Documentation.\n\n## CoreWeave CPU Cloud Pricing\n\nCPU only instance pricing is simplified and is driven by the cost per vCPU\nrequested. Pricing scales linearly with the vCPU count, and RAM is included in\nthe per vCPU price.\n\nCPU Model\n\nRAM per vCPU\n\nCost Per vCPU\n\nAMD EPYC Milan\n\n4\n\n$0.035\n\nAMD EPYC Rome\n\n4\n\n$0.03\n\nIntel Xeon Scalable\n\n4\n\n$0.03\n\nIntel Xeon v4\n\n4\n\n$0.02\n\nIntel Xeon v3\n\n4\n\n$0.0125\n\nAMD EPYC Milan\n\n **$0.035** / Hour\n\n **4** RAM GB\n\nAMD EPYC Rome\n\n **$0.03** / Hour\n\n **4** RAM GB\n\nIntel Xeon Scalable\n\n **$0.03** / Hour\n\n **4** RAM GB\n\nIntel Xeon v4\n\n **$0.02** / Hour\n\n **4** RAM GB\n\nIntel Xeon v3\n\n **$0.0125** / Hour\n\n **4** RAM GB\n\n*Compute instances on CoreWeave Cloud are configurable. For more info, please refer to our Resource Based Pricing Documentation.\n\n## Storage\n\nCoreWeave Cloud\u2019s high-performance network attached storage is priced to be\nsimple to understand and budget, with no limitations on scale, throughput or\nIOPS. Without extraneous charges for transferring data that drive up costs on\nother providers, our clients find storage pricing to be straightforward and\ntransparent.\n\nOffering\n\nRate\n\n **HDD (Block or Shared Filesystem)**\n\n$0.04 per GB / month\n\n **NVMe (Block or Shared Filesystem)**\n\n$0.07 per GB / month\n\n **Object Storage**\n\n$0.03 per GB / month\n\n **Block Storage (HDD)**\n\n **$0.04** per GB / month\n\n **Block Storage (NVMe)**\n\n **$0.07** per GB / month\n\n## Networking\n\nNetworking on CoreWeave is built to empower scale computational workloads. We\ndon\u2019t charge for ingress and egress, and if your workload requires significant\nthroughput we will work with our partners to structure IP transit or peering\nagreements that work for your business.  \n\u200d\n\nOffering\n\nRate\n\n **Public IP Address**\n\n$4.00 per IP / month\n\n **Burstable IP Address**\n\n$10.00 per IP / month\n\n **VPC**\n\n$20.00 per VPC / month\n\n **Public IP Address**\n\n **$4.00** per IP / month\n\n **Burstable IP Address  \n**\n\n **$10.00** per IP / month\n\n **VPC  \n**\n\n **$20.00** per IP / month\n\n## GPU Cloud Pricing FAQ\n\n### How is CoreWeave up to 80% less expensive?\n\nAt CoreWeave, we focus on delivering the best performance-adjusted cost in the\nmarket. To do this, we offer the lowest on-demand prices of any large-scale\ncloud provider, the industry\u2019s broadest range of NVIDIA GPUs so you can always\nfind the best GPU for your workload, the fastest & most flexible\ninfrastructure for performance and efficiency, and exceptional support to\nensure that your infrastructure is optimized for performance and cost.\n\n###  **Do you offer discounts?**\n\nThe vast majority of our clients enjoy the benefits of CoreWeave\u2019s on-demand\npricing with no contract or commitment. For workloads with relatively\npredictable usage patterns, we offer two types of volumetric discounts:\nReserved Instances (up to 60% for 24/7 committed usage) and Bulk Credits (up\nto 25% for large-scale, burst compute use cases consumed on-demand). Reach out\nto our Sales team to learn more about how these discounts can help your\nbusiness.\n\n### Do you charge for bandwidth/data transfer?\n\nNo! Gross. Other cloud providers \u201ctax\u201d you for each GB you transfer, making\nthe cost to run at scale in the cloud prohibitive. At CoreWeave, we don\u2019t\ncharge for things like region-to-region transfers, workstation data, or egress\nin the vast majority of use cases. If your bandwidth needs are extraordinary,\nwe\u2019ll work to understand your needs before negotiating flat bandwidth rates\nwith our suppliers on your behalf.\n\n### Do you offer \u201cspot\u201d or interruptible instances?\n\nNo, we do not. You can think of our on-demand pricing as \u201cinterruptible\npricing for uninterruptible instances\u201d. Our clients benefit from economics\nthat allow them to scale, with peace of mind knowing that when they spin-up\ninstances on CoreWeave Cloud, it\u2019s theirs until they choose to spin it down\nonce their workloads are complete.\n\n### Does CoreWeave have a free credits program?\n\nWe don\u2019t have a standard free credits program. Other cloud providers hand out\na ton of free credits, and subsidize the cost by charging clients extremely\nhigh prices once those credits run out. By asking all of our clients to pay an\nappropriate cost for compute, we\u2019re able to deliver economics that scale for\nyears to come.\n\n## CoreWeave Cloud  \nPricing FAQs\n\n* * *\n\n### How is CoreWeave up to 80% less expensive?\n\nAt CoreWeave, we focus on delivering the best performance-adjusted cost in the\nmarket. To do this, we offer the lowest on-demand prices of any large-scale\ncloud provider, the industry\u2019s broadest range of NVIDIA GPUs so you can always\nfind the best GPU for your workload, the fastest & most flexible\ninfrastructure for performance and efficiency, and exceptional support to\nensure that your infrastructure is optimized for performance and cost.\n\n### Do you offer discounts?\n\nThe vast majority of our clients enjoy the benefits of CoreWeave\u2019s on-demand\npricing with no contract or commitment. For workloads with relatively\npredictable usage patterns, we offer two types of volumetric discounts:\nReserved Instances (up to 60% for 24/7 committed usage) and Bulk Credits (up\nto 25% for large-scale, burst compute use cases consumed on-demand). Reach out\nto our Sales team to learn more about how these discounts can help your\nbusiness.\n\n### Do you charge for bandwidth/data transfer?\n\nNo! Gross. Other cloud providers \u201ctax\u201d you for each GB you transfer, making\nthe cost to run at scale in the cloud prohibitive. At CoreWeave, we don\u2019t\ncharge for things like region-to-region transfers, workstation data, or egress\nin the vast majority of use cases. If your bandwidth needs are extraordinary,\nwe\u2019ll work to understand your needs before negotiating flat bandwidth rates\nwith our suppliers on your behalf.\n\n### Do you offer \u201cspot\u201d or interruptible instances?\n\nNo, we do not. You can think of our on-demand pricing as \u201cinterruptible\npricing for uninterruptible instances\u201d. Our clients benefit from economics\nthat allow them to scale, with peace of mind knowing that when they spin-up\ninstances on CoreWeave Cloud, it\u2019s theirs until they choose to spin it down\nonce their workloads are complete.\n\n### Does CoreWeave have a free credits program?\n\nWe don\u2019t have a standard free credits program. Other cloud providers hand out\na ton of free credits, and subsidize the cost by charging clients extremely\nhigh prices once those credits run out. By asking all of our clients to pay an\nappropriate cost for compute, we\u2019re able to deliver economics that scale for\nyears to come.\n\nrecent blog posts\n\nDecart & Cerebrium Commit to Empowering Next Million Users With LLM\nApplications\n\nRead more \u2192\n\nKicking Off SC23: CoreWeave to Offer New NVIDIA GH200 Grace Hopper Superchip-\nPowered Instances in Q1 2024\n\nRead more \u2192\n\nIntroducing SUNK: A Slurm on Kubernetes Implementation for HPC and Large Scale\nAI\n\nRead more \u2192\n\n## Ready to get started?  \n\nSign Up Now\n\nH100 PCIe\n\nSIMILAR TO\n\nHGX H100\n\nA100 80GB PCIe\n\nA100 80GB NVLINK\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nH100_PCIE\n\n  * Networking\n\n100Gbps\n\n  * GPU Memory\n\n80GB HBM2e\n\n  * Graphics Bus\n\nPCIe\n\n  * FP32 Performance\n\n51 TFlops\n\n  * NVIDIA Tensor Cores\n\n456\n\n  * Max Power Consumption\n\n350W\n\n  * Cuda Cores\n\n14592\n\nHGX H100\n\nSIMILAR TO\n\nA100 80GB NVLINK\n\nA100 80GB PCIe\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nH100_NVLINK_80GB\n\n  * Networking\n\n100Gbps Ethernet + 1600 Gbps NDR InfiniBand with SHARP\n\n  * GPU Memory\n\n80GB HBM3\n\n  * Graphics Bus\n\nSXM5\n\n  * FP32 Performance\n\n67 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n528 4th Gen\n\n  * Max Power Consumption\n\n700W\n\n  * Cuda Parallel-Processing Cores\n\n16896\n\nRTX A4000\n\nSIMILAR TO\n\nRTX A5000\n\nRTX A6000\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nRTX_A4000\n\n  * Networking\n\n40 Gbps Ethernet\n\n  * RTX-OPS\n\n37.4T\n\n  * VR Ready\n\nYes\n\n  * GPU Memory\n\n16 GB GDDR6\n\n  * Graphics Bus\n\nPCI Express 4.0 x 16\n\n  * NVIDIA RT Cores\n\n48\n\n  * FP32 Performance\n\n19 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n192\n\n  * Max Power Consumption\n\n140W\n\n  * Cuda Parallel-Processing Cores\n\n6\n\nA100 80GB PCIe\n\nSIMILAR TO\n\nA40\n\nRTX A6000\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nA100_PCIE_80GB\n\n  * Networking\n\n40 Gbps Ethernet\n\n  * GPU Memory\n\n80GB HBM2e\n\n  * Graphics Bus\n\nPCIe\n\n  * FP32 Performance\n\n19.5 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n432\n\n  * Max Power Consumption\n\n250W\n\n  * Cuda Parallel-Processing Cores\n\n6912\n\nA100 80GB NVLINK\n\nSIMILAR TO\n\nA40\n\nRTX A6000\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nA100_NVLINK\n\n  * Networking\n\n100 Gbps Ethernet + 800 Gbps HDR InfiniBand with SHARP\n\n  * GPU Memory\n\n80GB HBM2e\n\n  * Graphics Bus\n\nSXM4\n\n  * FP32 Performance\n\n19.5 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n432\n\n  * Max Power Consumption\n\n400W\n\n  * Cuda Parallel-Processing Cores\n\n6912\n\nIntel Xeon v3\n\nSIMILAR TO\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\nIntel Xeon v4\n\nSIMILAR TO\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\nIntel Xeon Scalable\n\nSIMILAR TO\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\nAMD EPYC Rome\n\nSIMILAR TO\n\nAMD EPYC Milan\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\nAMD EPYC Milan\n\nSIMILAR TO\n\nAMD EPYC Rome\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\nQuadro RTX 4000\n\nSIMILAR TO\n\nQuadro RTX 5000\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nQuadro_RTX_4000\n\n  * Ethernet\n\n10 Gbps Ethernet\n\n  * RTX-OPS\n\n43T\n\n  * VR Ready\n\nYes\n\n  * Rays Cast\n\n6 Giga Rays/Sec\n\n  * GPU Memory\n\n8 GB GDDR6\n\n  * Graphics Bus\n\nPCI Express 3.0 x 16\n\n  * NVIDIA RT Cores\n\n36\n\n  * FP32 Performance\n\n7.1 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n288\n\n  * Max Power Consumption\n\n160W\n\n  * Cuda Parallel-Processing Cores\n\n2304\n\nQuadro RTX 5000\n\nSIMILAR TO\n\nQuadro RTX 4000\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nQuadro_RTX_5000\n\n  * Ethernet\n\n10 Gbps Ethernet\n\n  * RTX-OPS\n\n62T\n\n  * VR Ready\n\nYes\n\n  * Rays Cast\n\n8 Giga Rays/Sec\n\n  * GPU Memory\n\n16 GB GDDR6\n\n  * Graphics Bus\n\nPCI Express 3.0 x 16\n\n  * NVIDIA RT Cores\n\n48\n\n  * FP32 Performance\n\n11.2 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n384\n\n  * Max Power Consumption\n\n265W\n\n  * Cuda Parallel-Processing Cores\n\n3072\n\nRTX A6000\n\nSIMILAR TO\n\nA40\n\nRTX A5000\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nRTX_A6000\n\n  * Networking\n\n40 Gbps Ethernet\n\n  * RTX-OPS\n\n43T\n\n  * VR Ready\n\nYes\n\n  * GPU Memory\n\n48 GB GDDR6\n\n  * Graphics Bus\n\nPCI Express 4.0 x 16\n\n  * NVIDIA RT Cores\n\n84\n\n  * FP32 Performance\n\n38TFLOPS\n\n  * NVIDIA Tensor Cores\n\n336\n\n  * Max Power Consumption\n\n300W\n\n  * Cuda Parallel-Processing Cores\n\n10752\n\nTesla V100 NVLINK\n\nSIMILAR TO\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nTesla_V100_NVLINK\n\n  * Networking\n\n10 Gbps Ethernet\n\n  * GPU Memory\n\n16 GB HBM2\n\n  * Graphics Bus\n\nSXM2\n\n  * FP32 Performance\n\n15.7 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n640\n\n  * Max Power Consumption\n\n300W\n\n  * Cuda Parallel-Processing Cores\n\n5120\n\nRTX A5000\n\nSIMILAR TO\n\nRTX A6000\n\nA40\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nRTX_A5000\n\n  * Networking\n\n40 Gbps Ethernet RTX-OPS\n\n  * VR Ready\n\nYes\n\n  * GPU Memory\n\n24 GB GDDR6\n\n  * Graphics Bus\n\nPCI Express 4.0 x 16\n\n  * NVIDIA RT Cores\n\n64\n\n  * FP32 Performance\n\n28 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n256\n\n  * Max Power Consumption\n\n230W\n\n  * Cuda Parallel-Processing Cores\n\n8192\n\nA40\n\nSIMILAR TO\n\nA100 40GB NVLINK\n\nRTX A5000\n\nRTX A6000\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nA40\n\n  * Networking\n\n40 Gbps Ethernet\n\n  * RTX-OPS\n\n43T\n\n  * VR Ready\n\nYes\n\n  * GPU Memory\n\n48 GB GDDR6\n\n  * Graphics Bus\n\nPCI Express 4.0 x 16\n\n  * NVIDIA RT Cores\n\n84\n\n  * FP32 Performance\n\n38TFLOPS\n\n  * NVIDIA Tensor Cores\n\n336\n\n  * Max Power Consumption\n\n300W\n\n  * Cuda Parallel-Processing Cores\n\n10752\n\nA100 40GB PCIe\n\nSIMILAR TO\n\nA40\n\nRTX A6000\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nA100_PCIE_40GB\n\n  * Networking\n\n40 Gbps Ethernet\n\n  * GPU Memory\n\n40GB HBM2e\n\n  * Graphics Bus\n\nSXM4\n\n  * FP32 Performance\n\n19.5 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n432\n\n  * Max Power Consumption\n\n250W\n\n  * Cuda Parallel-Processing Cores\n\n6912\n\nA100 40GB NVLINK\n\nSIMILAR TO\n\nRTX A6000\n\nA40\n\nTECH SPECS\n\n  * GPU Memory\n\nThis is some text inside of a div block.\n\n  * Label\n\nA100_NVLINK\n\n  * Networking\n\n100 Gbps Ethernet + 400 Gbps HDR InfiniBand\n\n  * GPU Memory\n\n40GB HBM2e\n\n  * Graphics Bus\n\nSXM4\n\n  * FP32 Performance\n\n19.5 TFLOPS\n\n  * NVIDIA Tensor Cores\n\n432\n\n  * Max Power Consumption\n\n400W\n\n  * Cuda Parallel-Processing Cores\n\n6912\n\nCoreWeave is a specialized cloud provider, delivering a massive scale of GPUs\non top of the industry\u2019s fastest and most flexible infrastructure.  \n  \n101 Eisenhower Pkwy Ste 106  \nRoseland NJ 07068-1050  \n\n  * Why CoreWeave\n  * Documentation\n  * Pricing\n  * Contact Sales\n  * About Us\n  * Support\n\n  * Careers\n  * Blog\n  * Events & Webinars\n  * Login\n  * Status\n  * Terms of Service\n\n"
}