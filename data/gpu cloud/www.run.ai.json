{
    "summary": "Webinar: **Transforming AI for Financial Services with the Power of NVIDIA and\nRun:ai**\n\nSave Your Spot\n\nPlatform\n\nOverviewGPU OptimizationCluster ManagementAI/ML Workflow ManagementRun:ai x\nNVIDIA\n\nSolutions\n\nBy Team\n\nfor MLOps teamsfor Data Science teamsfor DevOps/Engineering teams\n\nResources\n\nBlog\n\nGuides\n\nAll GuidesAI TechnologyAutoMLCluster ManagementCloud Deep LearningDeep\nLearning for Computer VisionDeep Learning with GPUsDeep Learning with Multiple\nGPUsEdge ComputingHPC ClustersHyperparameter TuningKubernetes\nArchitectureMachine Learning in the CloudMachine Learning EngineeringMachine\nLearning InferenceMachine Learning Operations (MLOps)NVIDIA A100NVIDIA\nCUDAScheduled JobsSlurmTensorFlow\n\nWhite PapersCase Studies\n\nAbout\n\nAbout UsOur PartnersJoin Us\n\nGet a Demo\n\nPlatform\n\nOverviewGPU OptimizationCluster ManagementAI/ML WorkflowsRun:ai x NVIDIA\n\nSolutions\n\nBy Team\n\nfor MLOps teamsfor Data Science teamsfor DevOps/Engineering teams\n\nResources\n\nGuides\n\nAI TechnologyAutoMLCloud Deep LearningDeep Learning for Computer VisionDeep\nLearning with GPUsDeep Learning with Multiple GPUsEdge ComputingHPC\nClustersHyperparameter TuningKubernetes ArchitectureMachine Learning in the\nCloudMachine Learning EngineeringMachine Learning InferenceMachine Learning\nOperations (MLOps)NVIDIA A100NVIDIA CUDAScheduled JobsSlurmTensorFlow\n\nBlogGuidesWhite PapersCase Studies\n\nGuides\n\nAI TechnologyAutoMLCloud Deep LearningDeep Learning for Computer VisionDeep\nLearning with GPUsDeep Learning with Multiple GPUsEdge ComputingHPC\nClustersHyperparameter TuningKubernetes ArchitectureMachine Learning in the\nCloudMachine Learning EngineeringMachine Learning InferenceMachine Learning\nOperations (MLOps)NVIDIA A100NVIDIA CUDAScheduled JobsSlurmTensorFlow\n\nAbout\n\nAbout UsOur PartnersJoin Us\n\nBook a Demo\n\n# Google Cloud GPU\n\n# The Basics and a Quick Tutorial\n\nGuide Categories\n\nAI TechnologyAutoMLCloud Deep LearningCluster ManagementDeep Learning for\nComputer VisionDeep Learning with GPUsDeep Learning with Multiple GPUsEdge\nComputingHPC ClusterHyperparameter TuningKubernetes ArchitectureMachine\nLearning in the CloudMachine Learning EngineeringMachine Learning\nInferenceMachine Learning OperationsNVIDIA A100NVIDIA CUDAScheduled\nJobsSlurmTensorFlow\n\n###### Related Articles\n\nCloud Deep Learning\n\nAWS Deep Learning\n\nGoogle Cloud GPU\n\nGoogle TPU\n\nAI as a Service\n\nTriton Inference Server\n\n## What GPU Options are Offered on Google Cloud?\n\nGoogle Cloud Platform (GCP) is the world\u2019s third largest cloud provider.\nGoogle offers a number of virtual machines (VMs) that provide graphical\nprocessing units (GPUs), including the NVIDIA Tesla K80, P4, T4, P100, and\nV100.\n\nYou can use NVIDIA GPUs on GCP for large scale cloud deep learning projects,\nanalytics, physical object simulation, video transcoding, and molecular\nmodeling. GCP also provides virtual NVIDIA GRID workstations, which can let an\norganization\u2019s employees run graphics-intensive workloads remotely.\n\n **In this article, you will learn:**\n\n  * Google Cloud GPU Options\n  * Google Cloud TPU\n  * Working with GPUs on Google Cloud Compute Engine\n  * Optimizing Google Cloud Platform GPU Performance\n\n## Google Cloud GPU Options\n\nGoogle Cloud provides several GPU options. These GPUs can be selected as part\nof two Google instance types:\n\n  * **Accelerator-Optimized High-GPU** with 7 GB of RAM, 12\u201396 Cascade Lake CPUs, and SSD storage\n  *  **Accelerator-Optimized Mega-GPU** with 14 GB of RAM, 96 Cascade Lake CPUs, and SSD storage\n\nThe available GPUs are as follows:\n\n **GPUs suitable for model training, inference, and high performance\ncomputing:**\n\n  *  **A100** \u201440 GB memory, NVLink operating at 600 GB/s\n  *  **V100** \u201416 GB of memory, NVLink Ring networking operating at 300 GB/s\n  *  **P100** \u201416 GB of memory, no NVLink, supports GPU GRID\n  *  **K80** \u201412 GB of memory, no NVLink\n\n **GPUs suitable for inference, training, remote visualization, and\ntranscoding:**\n\n  *  **T4** \u201416 GB of memory, no NVLink, supports GPU GRID\n  *  **P4** \u20148 GB of memory, no NVLink, supports GPU GRID\n\nRelated content: read our guides to deep learning on other cloud providers:\n\n  * AWS deep learning\n  * Azure deep learning\n\n## Google Cloud TPU\n\nGoogle Cloud provides another hardware acceleration option\u2014the Tensorflow\nProcessing Unit (TPU). While not strictly a GPU, TPUs are a powerful\nalternative for machine learning workloads, especially deep learning.\n\nA TPU is an application-specific integrated circuit (ASIC) developed by Google\nspecifically to accelerate machine learning. Google provides TPU on demand as\na deep learning cloud service called Cloud TPU.\n\nCloud TPU is tightly integrated with Google's open source machine learning\n(ML) framework, TensorFlow, which provides dedicated APIs for TPU hardware.\nCloud TPU lets you create TensorFlow compute unit clusters including TPUs,\nGPUs, and regular CPUs.\n\n **Cloud TPU is mainly suitable for** machine learning models based on matrix\ncalculations, models that require weeks or months to train, models with large\ndatasets or a large number of variables, and those that run a training loop\nmany times (as in neural networks).\n\n **Cloud TPU is not suitable for** models that use linear or elementary\nalgebra, models that do not access memory often, or those that involve high-\nprecision arithmetic operations.\n\n **Related content:** **read our complete guide to google TPU**\n\n## Working with GPUs on Google Cloud Compute Engine\n\nHere is how to create a Google Cloud virtual machine (VM) with an attached\nNVIDIA A100 GPU:\n\n  1. Access the Google Cloud Console and click on **VM Instances**.\n  2. Click **Create Instance** , specify a name, region and zone you want to run your VM in.\n  3. In the **Machine Configuration** section, under **Machine Family** , select GPU.\n  4. Under **Series** , select A2\u2014this is the Google Cloud VM series that comes with NVIDIA A100 GPUs (see details of other machine series on Google Cloud). Select the **Machine type** appropriate to your needs. \n  5. Under **CPU and GPU Platforms** , see the GPU type and number of GPUs provided by the machine type you selected.\n  6. If you want to load your VM using an existing image, select the image in the **Boot disk** section. Review other VM settings to ensure they meet your requirements.\n  7. Click **Create**.\n\nThat\u2019s it! This process spins up a Google Cloud VM with an attached NVIDIA\nGPU.\n\n## Optimizing Google Cloud Platform GPU Performance\n\nHere are two tips that can help you improve GPU performance in a Google Cloud\nVM.\n\n### Disabling Autoboost and Setting Maximum Clock Frequency\n\nAutoboost is a feature in GPUs of the NVIDIA Tesla K80 series. It\nautomatically adjusts clock frequency to determine the best frequency for your\nparticular application. However, constantly adjusting the clock frequency will\nalso reduce GPU performance when running on Google infrastructure.\n\nIf you're running an NVIDIA Tesla K80 GPU on Compute Engine, it is recommended\nto disable auto boost, using the following command (in Linux):\n\nsudo nvidia-smi --auto-boost-default=DISABLED\n\nWhen using Tesla K80, you should also set the GPU clock speed to the highest\nfrequency, using this command:\n\nsudo nvidia-smi --applications-clocks=2505,875\n\n### Using Maximal Network Bandwidth\u2014Up to 100 Gbps\n\nTo make distributed workloads run faster with NVIDIA Tesla T4 or V100, use the\nmaximum network bandwidth of 100 Gbps, as follows:\n\n  1. Make sure you meet the minimal system requirements to use maximum network bandwidth (see documentation).\n  2. Create a VM instance connected to a T4 or V100 GPU. The image used to create the VM instance must have the virtual network interface (gVNIC).\n  3. After creating the virtual machine instance, check the actual network bandwidth consumption, using iperf or a similar tool. You\u2019ll need at least two instances of the VMs with connected GPUs.\n\nSee additional best practices from Google for using the maximum 100 Gbps\nbandwidth.\n\n## Google Cloud GPU with Run:AI\n\nRun:AI automates resource management and workload orchestration for machine\nlearning infrastructure. With Run:AI, you can automatically run as many\ncompute intensive experiments as needed, managing large numbers of GPUs in\nGoogle Cloud and other public clouds.\n\nHere are some of the capabilities you gain when using Run:AI:\n\n  * **Advanced visibility** \u2014create an efficient pipeline of resource sharing by pooling GPU compute resources.\n  *  **No more bottlenecks** \u2014you can set up guaranteed quotas of GPU resources, to avoid bottlenecks and optimize billing.\n  *  **A higher level of control** \u2014Run:AI enables you to dynamically change resource allocation, ensuring each job gets the resources it needs at any given time.\n\nRun:AI simplifies machine learning infrastructure pipelines, helping data\nscientists accelerate their productivity and the quality of their models.\n\nLearn more about the Run:AI GPU virtualization platform.\n\nSolutions\n\nMLOpsData ScientistsAI Infrastructure\n\nPlatform\n\nGPU OptimizationCluster ManagementAI/ML Workflow Management\n\nAbout\n\nAbout UsNewsJoin Us\n\nResources\n\nBlogWhite PapersCase StudiesGuidesDocumentation\n\nSubscribe to our newsletter\n\nutm_source\n\nutm_medium\n\nutm_campaign\n\nutm_term\n\nutm_content\n\nLatest Form Name\n\nEmail Address\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\nNew York, NY, USA\n\nTel Aviv, Israel\n\nPrivacy Policy\n\n\u00a92023 Run:ai\n\ninfo@run.ai\n\nWe use cookies on our site to give you the best experience possible. By\ncontinuing to browse the site, you agree to this use. For more information on\nhow we use cookies, see our Privacy Policy.\n\nAccept\n\n",
    "links": "[{\"link\": \"https://www.run.ai/guides\", \"text\": \"All Guides\"}, {\"link\": \"https://www.run.ai/guides/ai-technology-powering-the-ai-revolution\", \"text\": \"AI Technology\"}, {\"link\": \"https://www.run.ai/guides/automl\", \"text\": \"AutoML\"}, {\"link\": \"https://www.run.ai/guides/cluster-management\", \"text\": \"Cluster Management\"}, {\"link\": \"https://www.run.ai/guides/cloud-deep-learning\", \"text\": \"Cloud Deep Learning\"}, {\"link\": \"https://www.run.ai/guides/deep-learning-for-computer-vision\", \"text\": \"Deep Learning for Computer Vision\"}, {\"link\": \"https://www.run.ai/guides/gpu-deep-learning\", \"text\": \"Deep Learning with GPUs\"}, {\"link\": \"https://www.run.ai/guides/multi-gpu\", \"text\": \"Deep Learning with Multiple GPUs\"}, {\"link\": \"https://www.run.ai/guides/edge-computing\", \"text\": \"Edge Computing\"}, {\"link\": \"https://www.run.ai/guides/hpc-clusters\", \"text\": \"HPC Clusters\"}, {\"link\": \"https://www.run.ai/guides/hyperparameter-tuning\", \"text\": \"Hyperparameter Tuning\"}, {\"link\": \"https://www.run.ai/guides/kubernetes-architecture\", \"text\": \"Kubernetes Architecture\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-in-the-cloud\", \"text\": \"Machine Learning in the Cloud\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-engineering\", \"text\": \"Machine Learning Engineering\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-inference/understanding-machine-learning-inference\", \"text\": \"Machine Learning Inference\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-operations\", \"text\": \"Machine Learning Operations (MLOps)\"}, {\"link\": \"https://www.run.ai/guides/nvidia-a100\", \"text\": \"NVIDIA A100\"}, {\"link\": \"https://www.run.ai/guides/nvidia-cuda-basics-and-best-practices\", \"text\": \"NVIDIA CUDA\"}, {\"link\": \"https://www.run.ai/guides/scheduled-jobs/airflow-scheduling\", \"text\": \"Scheduled Jobs\"}, {\"link\": \"https://www.run.ai/guides/slurm\", \"text\": \"Slurm\"}, {\"link\": \"https://www.run.ai/guides/tensorflow/tensorflow-with-docker\", \"text\": \"TensorFlow\"}, {\"link\": \"https://www.run.ai/guides/ai-technology-powering-the-ai-revolution\", \"text\": \"AI Technology\"}, {\"link\": \"https://www.run.ai/guides/automl\", \"text\": \"AutoML\"}, {\"link\": \"https://www.run.ai/guides/cloud-deep-learning\", \"text\": \"Cloud Deep Learning\"}, {\"link\": \"https://www.run.ai/guides/deep-learning-for-computer-vision\", \"text\": \"Deep Learning for Computer Vision\"}, {\"link\": \"https://www.run.ai/guides/gpu-deep-learning\", \"text\": \"Deep Learning with GPUs\"}, {\"link\": \"https://www.run.ai/guides/multi-gpu\", \"text\": \"Deep Learning with Multiple GPUs\"}, {\"link\": \"https://www.run.ai/guides/edge-computing\", \"text\": \"Edge Computing\"}, {\"link\": \"https://www.run.ai/guides/hpc-clusters\", \"text\": \"HPC Clusters\"}, {\"link\": \"https://www.run.ai/guides/hyperparameter-tuning\", \"text\": \"Hyperparameter Tuning\"}, {\"link\": \"https://www.run.ai/guides/kubernetes-architecture\", \"text\": \"Kubernetes Architecture\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-in-the-cloud\", \"text\": \"Machine Learning in the Cloud\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-engineering\", \"text\": \"Machine Learning Engineering\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-inference/understanding-machine-learning-inference\", \"text\": \"Machine Learning Inference\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-operations\", \"text\": \"Machine Learning Operations (MLOps)\"}, {\"link\": \"https://www.run.ai/guides/nvidia-a100\", \"text\": \"NVIDIA A100\"}, {\"link\": \"https://www.run.ai/guides/nvidia-cuda-basics-and-best-practices\", \"text\": \"NVIDIA CUDA\"}, {\"link\": \"https://www.run.ai/guides/scheduled-jobs/airflow-scheduling\", \"text\": \"Scheduled Jobs\"}, {\"link\": \"https://www.run.ai/guides/slurm\", \"text\": \"Slurm\"}, {\"link\": \"https://www.run.ai/guides/tensorflow/tensorflow-with-docker\", \"text\": \"TensorFlow\"}, {\"link\": \"https://www.run.ai/guides/ai-technology-powering-the-ai-revolution\", \"text\": \"AI Technology\"}, {\"link\": \"https://www.run.ai/guides/automl\", \"text\": \"AutoML\"}, {\"link\": \"https://www.run.ai/guides/cloud-deep-learning\", \"text\": \"Cloud Deep Learning\"}, {\"link\": \"https://www.run.ai/guides/deep-learning-for-computer-vision\", \"text\": \"Deep Learning for Computer Vision\"}, {\"link\": \"https://www.run.ai/guides/gpu-deep-learning\", \"text\": \"Deep Learning with GPUs\"}, {\"link\": \"https://www.run.ai/guides/multi-gpu\", \"text\": \"Deep Learning with Multiple GPUs\"}, {\"link\": \"https://www.run.ai/guides/edge-computing\", \"text\": \"Edge Computing\"}, {\"link\": \"https://www.run.ai/guides/hpc-clusters\", \"text\": \"HPC Clusters\"}, {\"link\": \"https://www.run.ai/guides/hyperparameter-tuning\", \"text\": \"Hyperparameter Tuning\"}, {\"link\": \"https://www.run.ai/guides/kubernetes-architecture\", \"text\": \"Kubernetes Architecture\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-in-the-cloud\", \"text\": \"Machine Learning in the Cloud\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-engineering\", \"text\": \"Machine Learning Engineering\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-inference/understanding-machine-learning-inference\", \"text\": \"Machine Learning Inference\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-operations\", \"text\": \"Machine Learning Operations (MLOps)\"}, {\"link\": \"https://www.run.ai/guides/nvidia-a100\", \"text\": \"NVIDIA A100\"}, {\"link\": \"https://www.run.ai/guides/nvidia-cuda-basics-and-best-practices\", \"text\": \"NVIDIA CUDA\"}, {\"link\": \"https://www.run.ai/guides/scheduled-jobs/airflow-scheduling\", \"text\": \"Scheduled Jobs\"}, {\"link\": \"https://www.run.ai/guides/slurm\", \"text\": \"Slurm\"}, {\"link\": \"https://www.run.ai/guides/tensorflow/tensorflow-with-docker\", \"text\": \"TensorFlow\"}, {\"link\": \"https://www.run.ai/guides/ai-technology-powering-the-ai-revolution\", \"text\": \"AI Technology\"}, {\"link\": \"https://www.run.ai/guides/automl\", \"text\": \"AutoML\"}, {\"link\": \"https://www.run.ai/guides/cloud-deep-learning\", \"text\": \"Cloud Deep Learning\"}, {\"link\": \"https://www.run.ai/guides/cluster-management\", \"text\": \"Cluster Management\"}, {\"link\": \"https://www.run.ai/guides/deep-learning-for-computer-vision\", \"text\": \"Deep Learning for Computer Vision\"}, {\"link\": \"https://www.run.ai/guides/gpu-deep-learning\", \"text\": \"Deep Learning with GPUs\"}, {\"link\": \"https://www.run.ai/guides/multi-gpu\", \"text\": \"Deep Learning with Multiple GPUs\"}, {\"link\": \"https://www.run.ai/guides/edge-computing\", \"text\": \"Edge Computing\"}, {\"link\": \"https://www.run.ai/guides/hpc-clusters\", \"text\": \"HPC\u00a0Cluster\"}, {\"link\": \"https://www.run.ai/guides/hyperparameter-tuning/bayesian-hyperparameter-optimization\", \"text\": \"Hyperparameter Tuning\"}, {\"link\": \"https://www.run.ai/guides/kubernetes-architecture\", \"text\": \"Kubernetes Architecture\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-in-the-cloud\", \"text\": \"Machine Learning in the Cloud\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-engineering\", \"text\": \"Machine Learning Engineering\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-inference/understanding-machine-learning-inference\", \"text\": \"Machine Learning Inference\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-operations\", \"text\": \"Machine Learning Operations\"}, {\"link\": \"https://www.run.ai/guides/nvidia-a100\", \"text\": \"NVIDIA\u00a0A100\"}, {\"link\": \"https://www.run.ai/guides/nvidia-cuda-basics-and-best-practices\", \"text\": \"NVIDIA\u00a0CUDA\"}, {\"link\": \"https://www.run.ai/guides/scheduled-jobs/airflow-scheduling\", \"text\": \"Scheduled Jobs\"}, {\"link\": \"https://www.run.ai/guides/slurm\", \"text\": \"Slurm\"}, {\"link\": \"https://www.run.ai/guides/tensorflow/tensorflow-with-docker\", \"text\": \"TensorFlow\"}, {\"link\": \"https://www.run.ai/guides/cloud-deep-learning\", \"text\": \"Cloud Deep Learning\"}, {\"link\": \"https://www.run.ai/guides/cloud-deep-learning/aws-deep-learning\", \"text\": \"AWS Deep Learning\"}, {\"link\": \"https://www.run.ai/guides/cloud-deep-learning/google-cloud-gpu\", \"text\": \"Google Cloud GPU\"}, {\"link\": \"https://www.run.ai/guides/cloud-deep-learning/google-tpu\", \"text\": \"Google TPU\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-in-the-cloud/ai-as-a-service\", \"text\": \"AI as a Service\"}, {\"link\": \"https://www.run.ai/guides/machine-learning-engineering/triton-inference-server\", \"text\": \"Triton Inference Server\"}]",
    "priceAndPlans": "Error: Timeout 30000ms exceeded. =========================== logs\n=========================== navigating to\n\"http://web.archive.org/web/20231117194501/https://www.run.ai/\", waiting until\n\"load\" ============================================================\n\n"
}