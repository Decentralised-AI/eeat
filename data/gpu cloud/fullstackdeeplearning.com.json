{
    "summary": "Skip to content\n\nSign up for our latest in-person course!\n\nThe Full Stack\n\nCloud GPUs\n\nInitializing search\n\nThe Full Stack Website\n\n  * 923\n  * 182\n\n  * Home \n  * LLM Bootcamp \n  * Deep Learning Course \n  * Blog \n  * Cloud GPUs \n\nThe Full Stack\n\nThe Full Stack Website\n\n  * 923\n  * 182\n\n  * Home \n  * LLM Bootcamp \n\nLLM Bootcamp\n\n    * Spring 2023 \n\nSpring 2023\n\n      * Launch an LLM App in One Hour \n      * LLM Foundations \n      * Learn to Spell: Prompt Engineering \n      * Augmented Language Models \n      * Project Walkthrough: askFSDL \n      * UX for Language User Interfaces \n      * LLMOps \n      * What's Next? \n      * Reza Shabani: How to train your own LLM \n      * Harrison Chase: Agents \n      * Fireside Chat with Peter Welinder \n  * Deep Learning Course \n\nDeep Learning Course\n\n    * FSDL 2022 \n\nFSDL 2022\n\n      * Lecture 1: Course Vision and When to Use ML \n      * Lab Overview \n      * Lecture 2: Development Infrastructure & Tooling \n      * Lab 4: Experiment Management \n      * Lecture 3: Troubleshooting & Testing \n      * Lab 5: Troubleshooting & Testing \n      * Lecture 4: Data Management \n      * Lab 6: Data Annotation \n      * Lecture 5: Deployment \n      * Lab 7: Web Deployment \n      * Lecture 6: Continual Learning \n      * Lab 8: Model Monitoring \n      * Lecture 7: Foundation Models \n      * Lecture 8: ML Teams and Project Management \n      * Lecture 9: Ethics \n      * Project Showcase \n      * Course Announcement \n    * Older  Older \n      * FSDL 2021 \n\nFSDL 2021\n\n        * Synchronous Online Course \n        * Course Projects Showcase \n        * Lecture 1: DL Fundamentals \n        * Lab 1: Setup and Introduction \n        * Notebook: Coding a neural net \n        * Lecture 2A: CNNs \n        * Lecture 2B: Computer Vision \n        * Lab 2: CNNs and Synthetic Data \n        * Lecture 3: RNNs \n        * Lab 3: RNNs \n        * Lecture 4: Transformers \n        * Lab 4: Transformers \n        * Lecture 5: ML Projects \n        * Lecture 6: MLOps Infrastructure & Tooling \n        * Lab 5: Experiment Management \n        * Lecture 7: Troubleshooting Deep Neural Networks \n        * Lecture 8: Data Management \n        * Lab 6: Data Labeling \n        * Lecture 9: AI Ethics \n        * Lab 7: Paragraph Recognition \n        * Lecture 10: Testing & Explainability \n        * Lab 8: Testing & CI \n        * Lecture 11: Deployment & Monitoring \n        * Lab 9: Web Deployment \n        * Lecture 12: Research Directions \n        * Lecture 13: ML Teams and Startups \n        * Panel Discussion: Do I need a PhD to work in ML? \n      * FSDL 2021 (Berkeley) \n      * FSDL 2020 (UW) \n      * FSDL 2019 (Online) \n      * FSDL 2019 (Bootcamp) \n      * FSDL 2018 (Bootcamp) \n  * Blog \n  * Cloud GPUs  Cloud GPUs  Table of contents \n    * GPU Cloud Server Comparison \n      * Notes \n    * Serverless GPUs \n      * Notes \n    * How do I choose a GPU? \n    * GPU Raw Performance Numbers and Datasheets \n    * GPU Performance Benchmarks \n\nTable of contents\n\n  * GPU Cloud Server Comparison \n    * Notes \n  * Serverless GPUs \n    * Notes \n  * How do I choose a GPU? \n  * GPU Raw Performance Numbers and Datasheets \n  * GPU Performance Benchmarks \n\n# Cloud GPUs\n\nBy Sergey Karayev and Charles Frye. Updated October 30, 2023.\n\nDiscussion of this page on Hacker News, May 21, 2023.\n\nTraining and running neural networks often requires hardware acceleration, and\nthe most popular hardware accelerator is the venerable _graphics processing\nunit_ , or GPU.\n\nWe have assembled cloud GPU vendor pricing all into tables, sortable and\nfilterable to your liking!\n\nWe have split the vendor offerings into two classes:\n\n  * **GPU Cloud Servers** , which are long-running (but possibly pre-emptible) machines, and\n  * **Severless GPUs** , which are machines that scale-to-zero in the absence of traffic (like an AWS Lambda or Google Cloud Function)\n\n**We welcome your help in adding more cloud GPU providers and keeping the\npricing info current.**\n\nPlease file an issue or make a pull request to this repo, editing this file to\nupdate the text on this page or one of the CSV files to update the data:\n`cloud-gpus.csv` for servers and `serverless-gpus.csv` for serverless options.\n\n## GPU Cloud Server Comparison\n\n### Notes\n\nThe table below does not include all possible configurations for all\nproviders, as providers differ in their configuration strategy.\n\n  * Most providers, including AWS, Azure, and Lambda, provide instances with pre-set configurations.\n  * On GCP, any suitable machine can be connected to a configuration of GPUs.\n  * On other providers, like Oblivus Cloud, Cudo Compute, and RunPod, users have precise control over the resources they request. Note that RunPod's Community Cloud, Oblivus, and Cudo are all \"open clouds\", meaning compute is provided by third parties.\n  * For providers without pre-set instance configurations, we have selected configurations that are roughly equivalent to AWS's options. Generally, these configurations are good for workloads that require heavy inter-GPU communication.\n  * Where possible, regions were set to be the west or central parts of the United States. GPU availability depends on the region.\n  * Raw data can be found in a csv on GitHub.\n  * Costs can be substantially reduced via preemption recovery and failover across clouds. If you don't want to roll your own, consider a tool like SkyPilot. See discussion of their launch on Hacker News, December 13, 2022.\n\n_All prices are in $/hr._\n\nCloud\n\n|\n\nGPU Type\n\n|\n\nGPU Arch\n\n|\n\nGPUs\n\n|\n\nGPU RAM\n\n|\n\nvCPUs\n\n|\n\nRAM\n\n|\n\nOn-demand\n\n|\n\nPer-GPU\n\n|\n\nSpot\n\n|\n\nName  \n  \n---|---|---|---|---|---|---|---|---|---|---  \nAWS| A100 (80 GB)| Ampere| 8| 640| 96| 1152| 40.97| 5.12| | p4de.24xlarge  \nAWS| A100 (40 GB)| Ampere| 8| 320| 96| 1152| 32.77| 4.1| 9.83| p4d.24xlarge  \nAWS| V100 (16 GB)| Volta| 1| 16| 8| 61| 3.06| 3.06| 0.92| p3.2xlarge  \nAWS| V100 (16 GB)| Volta| 4| 64| 32| 244| 12.24| 3.06| 3.67| p3.8xlarge  \nAWS| V100 (16 GB)| Volta| 8| 128| 64| 488| 24.48| 3.06| 7.34| p3.16xlarge  \nAWS| V100 (32 GB)| Volta| 8| 256| 96| 768| 31.21| 3.9| 9.36| p3dn.24xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 16| 64| 1.62| 1.62| 0.49| g5.4xlarge  \nAWS| A10G (24 GB)| Ampere| 4| 96| 48| 192| 5.67| 1.42| 1.74| g5.12xlarge  \nAWS| A10G (24 GB)| Ampere| 8| 192| 192| 768| 16.29| 2.04| 4.89| g5.48xlarge  \nAWS| A10G (24 GB)| Ampere| 4| 96| 96| 384| 8.14| 2.04| 2.44| g5.24xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 4| 16| 1.01| 1.01| 0.3| g5.xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 8| 32| 1.21| 1.21| 0.39| g5.2xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 32| 128| 2.45| 2.45| 0.76| g5.8xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 64| 256| 4.1| 4.1| 1.23| g5.16xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 16| 64| 1.2| 1.2| 0.36| g4dn.4xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 4| 16| 0.53| 0.53| 0.16| g4dn.xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 8| 32| 0.75| 0.75| 0.23| g4dn.2xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 16| 64| 1.2| 1.2| 0.36| g4dn.4xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 32| 128| 2.18| 2.18| 0.65| g4dn.8xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 64| 256| 4.35| 4.35| 1.21| g4dn.16xlarge  \nAWS| T4 (16 GB)| Turing| 4| 64| 48| 192| 3.91| 0.98| 1.31| g4dn.12xlarge  \nAWS| T4 (16 GB)| Turing| 8| 128| 96| 384| 7.82| 0.98| 2.35| g4dn.metal  \nAWS| T4 (16 GB)| Turing| 1| 16| 16| 64| 1.2| 1.2| 0.36| g4dn.4xlarge  \nAWS| K80 (12 GB)| Kepler| 1| 12| 4| 61| 0.9| 0.9| 0.27| p2.xlarge  \nAWS| K80 (12 GB)| Kepler| 8| 96| 32| 488| 7.2| 0.9| 2.16| p2.8xlarge  \nAWS| K80 (12 GB)| Kepler| 16| 192| 64| 732| 14.4| 0.9| 4.32| p2.16xlarge  \nAzure| A100 (80 GB)| Ampere| 1| 80| 24| 220| 3.67| 3.67| 1.47| NC24ads A100 v4  \nAzure| A100 (80 GB)| Ampere| 2| 160| 48| 440| 7.35| 3.67| 2.94| NC48ads A100\nv4  \nAzure| A100 (80 GB)| Ampere| 4| 320| 96| 880| 14.69| 3.67| 5.88| NC96ads A100\nv4  \nAzure| A100 (80 GB)| Ampere| 8| 640| 96| 1900| 37.18| 4.64| | ND96amsr A100 v4  \nAzure| V100 (16 GB)| Volta| 1| 16| 6| 112| 3.06| 3.06| 1.35| NC6s v3  \nAzure| V100 (16 GB)| Volta| 2| 32| 12| 224| 6.12| 3.06| 2.7| NC12s v3  \nAzure| V100 (16 GB)| Volta| 4| 64| 24| 448| 12.24| 3.06| 5.4| NC24s v3  \nAzure| P100 (16 GB)| Pascal| 1| 16| 6| 112| 2.07| 2.07| 0.22| NC6s v2  \nAzure| P100 (16 GB)| Pascal| 2| 32| 12| 224| 4.14| 2.07| 0.43| NC12s v2  \nAzure| P100 (16 GB)| Pascal| 4| 64| 24| 448| 8.28| 2.07| 0.87| NC24s v2  \nAzure| T4 (16 GB)| Turing| 1| 16| 4| 28| 0.53| 0.53| 0.21| NC4as T4 v3  \nAzure| T4 (16 GB)| Turing| 1| 16| 8| 56| 0.75| 0.75| 0.3| NC8as T4 v3  \nAzure| T4 (16 GB)| Turing| 1| 16| 16| 110| 1.2| 1.2| 0.48| NC16as T4 v3  \nAzure| T4 (16 GB)| Turing| 4| 64| 64| 440| 4.35| 1.09| 1.73| NC64as T4 v3  \nAzure| K80 (12 GB)| Kepler| 1| 12| 6| 56| 0.9| 0.9| 0.09| NC6  \nAzure| K80 (12 GB)| Kepler| 2| 24| 12| 112| 1.8| 0.9| 0.18| NC12  \nAzure| K80 (12 GB)| Kepler| 4| 48| 24| 224| 3.6| 0.9| 0.36| NC24  \nCudo Compute| A4000 (16 GB)| Ampere| 1| 16| 2| 4| 0.3| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 1| 16| 2| 4| 0.32| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 1| 24| 2| 4| 0.53| 0.52| |  \nCudo Compute| A5000 (24 GB)| Ampere| 1| 24| 2| 4| 0.57| 0.55| |  \nCudo Compute| A6000 (48 GB)| Ampere| 1| 48| 2| 4| 0.79| 0.78| |  \nCudo Compute| A4000 (16 GB)| Ampere| 1| 16| 4| 8| 0.3| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 1| 16| 4| 8| 0.32| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 1| 24| 4| 8| 0.53| 0.55| |  \nCudo Compute| A5000 (24 GB)| Ampere| 1| 24| 4| 8| 0.57| 0.58| |  \nCudo Compute| A6000 (48 GB)| Ampere| 1| 48| 4| 8| 0.79| 0.81| |  \nCudo Compute| A4000 (16 GB)| Ampere| 2| 32| 8| 16| 0.65| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 2| 32| 8| 16| 0.67| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 2| 48| 8| 16| 1.11| 0.55| |  \nCudo Compute| A5000 (24 GB)| Ampere| 2| 48| 8| 16| 1.17| 0.58| |  \nCudo Compute| A6000 (48 GB)| Ampere| 2| 96| 8| 16| 1.63| 0.81| |  \nCudo Compute| A4000 (16 GB)| Ampere| 2| 32| 16| 32| 0.74| 0.3| |  \nCudo Compute| A4000 (16 GB)| Ampere| 2| 32| 16| 32| 0.72| 0.29| |  \nCudo Compute| A5000 (24 GB)| Ampere| 2| 48| 16| 32| 1.18| 0.55| |  \nCudo Compute| A5000 (24 GB)| Ampere| 2| 48| 16| 32| 1.24| 0.58| |  \nCudo Compute| A6000 (48 GB)| Ampere| 2| 96| 16| 32| 1.7| 0.81| |  \nCudo Compute| A4000 (16 GB)| Ampere| 4| 64| 32| 64| 1.44| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 4| 64| 32| 64| 1.49| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 4| 96| 32| 64| 2.36| 0.55| |  \nCudo Compute| A5000 (24 GB)| Ampere| 4| 96| 32| 64| 2.49| 0.58| |  \nCudo Compute| A6000 (48 GB)| Ampere| 4| 192| 32| 64| 3.4| 0.81| |  \nCudo Compute| A4000 (16 GB)| Ampere| 8| 64| 64| 128| 2.89| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 8| 64| 64| 128| 2.99| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 8| 192| 64| 128| 4.73| 0.52| |  \nCudo Compute| A6000 (48 GB)| Ampere| 8| 384| 64| 128| 6.81| 0.78| |  \nDatacrunch| A100 (80 GB)| Ampere| 1| 80| 22| 120| 1.85| 1.85| | 1A100.22V  \nDatacrunch| A100 (80 GB)| Ampere| 2| 160| 44| 240| 3.7| 1.85| | 2A100.44V  \nDatacrunch| A100 (80 GB)| Ampere| 4| 320| 88| 480| 7.4| 1.85| | 4A100.88V  \nDatacrunch| A100 (80 GB)| Ampere| 8| 640| 176| 960| 14.8| 1.85| | 8A100.176V  \nDatacrunch| A6000 (48 GB)| Ampere| 1| 48| 10| 60| 1.25| 1.25| | 1A6000.10V  \nDatacrunch| A6000 (48 GB)| Ampere| 8| 384| 80| 480| 10| 1.25| | 8A6000.80V  \nDatacrunch| V100 (16 GB)| Volta| 1| 16| 6| 23| 1| 1| | 1V100.6V  \nDatacrunch| V100 (16 GB)| Volta| 2| 32| 10| 45| 2| 1| | 2V100.10V  \nDatacrunch| V100 (16 GB)| Volta| 4| 64| 20| 90| 4| 1| | 4V100.20V  \nDatacrunch| V100 (16 GB)| Volta| 8| 128| 48| 180| 8| 1| | 8V100.48V  \nExoscale| P100 (16 GB)| Pascal| 1| 16| 12| 56| 1.17| 1.17| | GPU SMALL  \nExoscale| P100 (16 GB)| Pascal| 2| 32| 24| 90| 1.71| 0.86| | GPU MEDIUM  \nExoscale| P100 (16 GB)| Pascal| 4| 64| 48| 120| 2.54| 0.64| | GPU LARGE  \nExoscale| P100 (16 GB)| Pascal| 8| 128| 96| 225| 2.82| 0.35| | GPU HUGE  \nExoscale| V100 (16 GB)| Tesla| 1| 16| 12| 56| 1.38| 1.38| | GPU2 SMALL  \nExoscale| V100 (16 GB)| Tesla| 2| 32| 16| 90| 2.01| 1.01| | GPU2 MEDIUM  \nExoscale| V100 (16 GB)| Tesla| 4| 64| 48| 225| 3.32| 0.83| | GPU2 HUGE  \nExoscale| A40 (48 GB)| Ampere| 1| 48| 12| 56| 2.13| 2.13| | GPU3 SMALL  \nExoscale| A40 (48 GB)| Ampere| 2| 96| 24| 120| 4.27| 2.14| | GPU3 MEDIUM  \nExoscale| A40 (48 GB)| Ampere| 4| 192| 48| 224| 8.53| 2.13| | GPU3 LARGE  \nExoscale| A40 (48 GB)| Ampere| 8| 384| 96| 448| 17.07| 2.13| | GPU3 HUGE  \nFluidStack| A100 (80 GB)| Ampere| 1| 80| 48| 256| 2.21| 2.21| |  \nFluidStack| A100 (80 GB)| Ampere| 2| 80| 96| 512| 4.42| 2.21| |  \nFluidStack| A100 (80 GB)| Ampere| 4| 80| 192| 768| 8.84| 2.21| |  \nFluidStack| A100 (80 GB)| Ampere| 8| 80| 384| 1024| 17.68| 2.21| |  \nFluidStack| H100 (80 GB)| Hopper| 1| 80| 48| 256| 4.76| 4.76| |  \nFluidStack| H100 (80 GB)| Hopper| 2| 80| 96| 512| 9.52| 4.76| |  \nFluidStack| H100 (80 GB)| Hopper| 4| 80| 192| 768| 19.04| 4.76| |  \nFluidStack| H100 (80 GB)| Hopper| 8| 80| 384| 1024| 38.08| 4.76| |  \nGCP| A100 (40 GB)| Ampere| 1| 40| 12| 85| 3.67| 3.67| 1.1| a2-highgpu-1g  \nGCP| A100 (40 GB)| Ampere| 2| 80| 24| 170| 7.34| 3.67| 2.2| a2-highgpu-2g  \nGCP| A100 (40 GB)| Ampere| 4| 160| 48| 340| 14.68| 3.67| 4.41| a2-highgpu-4g  \nGCP| A100 (40 GB)| Ampere| 8| 320| 96| 680| 29.36| 3.67| 8.81| a2-highgpu-8g  \nGCP| A100 (40 GB)| Ampere| 16| 640| 96| 1360| 55.68| 3.48| 16.72|\na2-megagpu-16g  \nGCP| V100 (16 GB)| Volta| 1| 16| 8| 52| 2.95| 2.95| 0.84| n1-highmem-8  \nGCP| V100 (16 GB)| Volta| 2| 32| 16| 104| 5.91| 2.95| 1.68| n1-highmem-16  \nGCP| V100 (16 GB)| Volta| 4| 64| 32| 208| 11.81| 2.95| 3.36| n1-highmem-32  \nGCP| V100 (16 GB)| Volta| 8| 128| 64| 416| 23.63| 2.95| 6.72| n1-highmem-64  \nGCP| P100 (16 GB)| Pascal| 1| 16| 8| 52| 1.93| 1.93| 0.53| n1-highmem-8  \nGCP| P100 (16 GB)| Pascal| 2| 32| 16| 104| 3.87| 1.93| 1.06| n1-highmem-16  \nGCP| P100 (16 GB)| Pascal| 4| 64| 32| 208| 7.73| 1.93| 2.12| n1-highmem-32  \nGCP| K80 (12 GB)| Kepler| 1| 12| 8| 52| 0.92| 0.92| 0.14| n1-highmem-8  \nGCP| K80 (12 GB)| Kepler| 2| 24| 16| 104| 1.85| 0.92| 0.28| n1-highmem-16  \nGCP| K80 (12 GB)| Kepler| 4| 48| 32| 208| 3.69| 0.92| 0.56| n1-highmem-32  \nGCP| K80 (12 GB)| Kepler| 8| 96| 64| 416| 7.39| 0.92| 1.12| n1-highmem-64  \nGCP| T4 (16 GB)| Turing| 1| 16| 8| 52| 0.82| 0.82| 0.21| n1-highmem-8  \nGCP| T4 (16 GB)| Turing| 2| 32| 16| 104| 1.65| 0.82| 0.42| n1-highmem-16  \nGCP| T4 (16 GB)| Turing| 4| 64| 32| 208| 3.29| 0.82| 0.84| n1-highmem-32  \nGCP| P4 (8 GB)| Pascal| 1| 8| 4| 26| 0.84| 0.84| 0.27| n1-highmem-4  \nGCP| P4 (8 GB)| Pascal| 2| 16| 8| 52| 1.67| 0.84| 0.54| n1-highmem-8  \nGCP| P4 (8 GB)| Pascal| 4| 32| 16| 104| 3.35| 0.84| 1.08| n1-highmem-16  \nJarvislabs| A100 (40 GB)| Ampere| 1| 40| 7| 32| 1.1| 1.1| 0.69|  \nJarvislabs| A100 (40 GB)| Ampere| 2| 80| 14| 64| 2.2| 1.1| 1.38|  \nJarvislabs| A100 (40 GB)| Ampere| 4| 160| 28| 128| 4.4| 1.1| 2.76|  \nJarvislabs| A100 (40 GB)| Ampere| 8| 320| 56| 256| 8.8| 1.1| 5.52|  \nJarvislabs| A6000 (48 GB)| Ampere| 1| 48| 7| 32| 0.99| 0.99| 0.59|  \nJarvislabs| A6000 (48 GB)| Ampere| 2| 96| 14| 64| 1.98| 0.99| 1.18|  \nJarvislabs| A6000 (48 GB)| Ampere| 4| 192| 28| 128| 3.96| 0.99| 2.36|  \nJarvislabs| A6000 (48 GB)| Ampere| 8| 384| 56| 256| 7.92| 0.99| 4.72|  \nJarvislabs| A5000 (24 GB)| Ampere| 1| 24| 7| 32| 0.59| 0.59| 0.39|  \nJarvislabs| A5000 (24 GB)| Ampere| 2| 48| 14| 64| 1.18| 0.59| 0.78|  \nJarvislabs| A5000 (24 GB)| Ampere| 4| 96| 28| 128| 2.36| 0.59| 1.56|  \nJarvislabs| A5000 (24 GB)| Ampere| 8| 192| 56| 256| 4.72| 0.59| 3.12|  \nJarvislabs| RTX5000 (16 GB)| Turing| 1| 16| 7| 32| 0.49| 0.49| 0.19|  \nJarvislabs| RTX5000 (16 GB)| Turing| 2| 32| 14| 64| 0.98| 0.49| 0.38|  \nJarvislabs| RTX5000 (16 GB)| Turing| 4| 64| 28| 128| 1.96| 0.49| 0.76|  \nJarvislabs| RTX5000 (16 GB)| Turing| 8| 128| 56| 256| 3.92| 0.49| 1.52|  \nLambda| A10G (24 GB)| Ampere| 1| 24| 30| 200| 0.6| 0.6| |  \nLambda| A100 (40 GB)| Ampere| 1| 40| 30| 200| 1.1| 1.1| |  \nLambda| A100 (40 GB)| Ampere| 2| 80| 60| 400| 2.2| 1.1| |  \nLambda| A100 (40 GB)| Ampere| 4| 160| 120| 800| 4.4| 1.1| |  \nLambda| A100 (40 GB)| Ampere| 8| 320| 124| 1800| 8.8| 1.1| |  \nLambda| A100 (80 GB)| Ampere| 8| 640| 240| 1800| 12| 1.5| |  \nLambda| A6000 (48 GB)| Ampere| 1| 48| 14| 100| 0.8| 0.8| |  \nLambda| A6000 (48 GB)| Ampere| 2| 96| 28| 200| 1.6| 0.8| |  \nLambda| A6000 (48 GB)| Ampere| 4| 192| 56| 400| 3.2| 0.8| |  \nLambda| Quadro RTX 6000 (24 GB)| Turing| 1| 24| 92| 448| 4.4| 4.4| |  \nLambda| H100 (80 GB)| Hopper| 1| 80| 26| 200| 1.99| 1.99| |  \nLambda| V100 (16 GB)| Volta| 8| 128| 92| 448| 4.4| 0.55| |  \nLatitude.sh| H100 (80 GB)| Hopper| 1| 80| 128| 192| 4.4| 4.4| | g3.small.x86  \nLatitude.sh| H100 (80 GB)| Hopper| 4| 320| 128| 768| 17.6| 4.4| |\ng3.medium.x86  \nLatitude.sh| A100 (80 GB)| Ampere| 8| 640| 128| 1536| 23.2| 2.9| |\ng3.large.x86  \nLatitude.sh| H100 (80 GB)| Hopper| 8| 640| 128| 1536| 32.2| 4.03| |\ng3.xlarge.x86  \nOblivus Cloud| A100 (80 GB)| Ampere| 1| 80| 4| 16| 2.55| 2.55| |  \nOblivus Cloud| A100 (80 GB)| Ampere| 2| 160| 8| 32| 5.1| 2.55| |  \nOblivus Cloud| A100 (80 GB)| Ampere| 4| 320| 16| 64| 10.2| 2.55| |  \nOblivus Cloud| A100 (80 GB)| Ampere| 8| 640| 32| 128| 20.4| 2.55| |  \nOblivus Cloud| A100 (40 GB)| Ampere| 1| 40| 4| 16| 2.39| 2.39| |  \nOblivus Cloud| A100 (40 GB)| Ampere| 2| 80| 8| 32| 4.78| 2.39| |  \nOblivus Cloud| A100 (40 GB)| Ampere| 4| 160| 16| 64| 9.56| 2.39| |  \nOblivus Cloud| A100 (40 GB)| Ampere| 8| 320| 32| 128| 19.12| 2.39| |  \nOblivus Cloud| A40 (48 GB)| Ampere| 1| 48| 4| 16| 1.54| 1.54| |  \nOblivus Cloud| A40 (48 GB)| Ampere| 2| 96| 8| 32| 3.08| 1.54| |  \nOblivus Cloud| A40 (48 GB)| Ampere| 4| 192| 16| 64| 6.16| 1.54| |  \nOblivus Cloud| A40 (48 GB)| Ampere| 8| 384| 32| 128| 12.32| 1.54| |  \nOblivus Cloud| V100 (16 GB)| Volta| 1| 16| 4| 16| 0.65| 0.65| |  \nOblivus Cloud| V100 (16 GB)| Volta| 2| 32| 8| 32| 1.3| 0.65| |  \nOblivus Cloud| V100 (16 GB)| Volta| 4| 64| 16| 64| 2.6| 0.65| |  \nOblivus Cloud| A6000 (48 GB)| Ampere| 1| 48| 4| 16| 1.54| 1.54| |  \nOblivus Cloud| A6000 (48 GB)| Ampere| 2| 96| 8| 32| 3.08| 1.54| |  \nOblivus Cloud| A6000 (48 GB)| Ampere| 4| 192| 16| 64| 6.16| 1.54| |  \nOblivus Cloud| A6000 (48 GB)| Ampere| 8| 384| 32| 128| 12.32| 1.54| |  \nOblivus Cloud| A5000 (24 GB)| Ampere| 1| 24| 4| 16| 0.98| 0.98| |  \nOblivus Cloud| A5000 (24 GB)| Ampere| 2| 48| 8| 32| 1.96| 0.98| |  \nOblivus Cloud| A5000 (24 GB)| Ampere| 4| 96| 16| 64| 3.92| 0.98| |  \nOblivus Cloud| A5000 (24 GB)| Ampere| 8| 192| 32| 128| 7.84| 0.98| |  \nOblivus Cloud| A4000 (16 GB)| Ampere| 1| 16| 4| 16| 0.81| 0.81| |  \nOblivus Cloud| A4000 (16 GB)| Ampere| 2| 32| 8| 32| 1.62| 0.81| |  \nOblivus Cloud| A4000 (16 GB)| Ampere| 4| 64| 16| 64| 3.24| 0.81| |  \nOblivus Cloud| RTX5000 (16 GB)| Turing| 1| 16| 4| 16| 0.76| 0.76| |  \nOblivus Cloud| RTX5000 (16 GB)| Turing| 2| 32| 8| 32| 1.52| 0.76| |  \nOblivus Cloud| RTX5000 (16 GB)| Turing| 4| 64| 16| 64| 3.04| 0.76| |  \nOblivus Cloud| RTX4000 (8 GB)| Turing| 1| 8| 4| 16| 0.41| 0.27| |  \nOblivus Cloud| RTX4000 (8 GB)| Turing| 2| 16| 8| 32| 0.82| 0.27| |  \nOblivus Cloud| RTX4000 (8 GB)| Turing| 4| 32| 16| 64| 1.64| 0.27| |  \nOVHcloud| V100 (16 GB)| Volta| 1| 16| 8| 45| 1.97| 1.97| | t1-45  \nOVHcloud| V100 (16 GB)| Volta| 2| 32| 18| 90| 3.94| 1.97| | t1-90  \nOVHcloud| V100 (16 GB)| Volta| 4| 64| 36| 180| 7.89| 1.97| | t1-180  \nOVHcloud| V100 (32 GB)| Volta| 1| 32| 14| 45| 2.19| 2.19| | t2-45  \nOVHcloud| V100 (32 GB)| Volta| 2| 64| 28| 90| 4.38| 2.19| | t2-90  \nOVHcloud| V100 (32 GB)| Volta| 4| 128| 56| 180| 8.76| 2.19| | t2-180  \nOracle Cloud| A10| Ampere| 1| 24| 15| 240| 2| 2| | VM.GPU.A10.1  \nOracle Cloud| A10| Ampere| 2| 48| 30| 480| 4| 2| | VM.GPU.A10.2  \nOracle Cloud| A100 (40 GB)| Ampere| 8| 320| 64| 2048| 24.4| 3.05| |\nBM.GPU.A4.8  \nOracle Cloud| A100 (80 GB)| Ampere| 8| 640| 128| 2048| 32| 4| |\nBM.GPU.A100-v2.8  \nOracle Cloud| H100 (80 GB)| Hopper| 8| 640| | | 80| 10| | BM.GPU.H100.8  \nPaperspace| V100 (16 GB)| Volta| 1| 16| 8| 30| 2.3| 2.3| |  \nPaperspace| V100 (32 GB)| Volta| 1| 32| 8| 30| 2.3| 2.3| |  \nPaperspace| V100 (32 GB)| Volta| 2| 64| 16| 60| 4.6| 2.3| |  \nPaperspace| V100 (32 GB)| Volta| 4| 128| 32| 120| 9.2| 2.3| |  \nPaperspace| A6000 (48 GB)| Ampere| 1| 48| 8| 45| 1.89| 1.89| |  \nPaperspace| A6000 (48 GB)| Ampere| 2| 96| 16| 90| 3.78| 1.89| |  \nPaperspace| A6000 (48 GB)| Ampere| 4| 192| 32| 180| 7.56| 1.89| |  \nPaperspace| A100 (40 GB)| Ampere| 1| 40| 12| 90| 3.09| 3.09| |  \nPaperspace| A100 (40 GB)| Ampere| 2| 80| 24| 180| 6.18| 3.09| |  \nPaperspace| A100 (40 GB)| Ampere| 4| 160| 48| 360| 12.36| 3.09| |  \nPaperspace| A100 (40 GB)| Ampere| 8| 320| 96| 720| 24.72| 3.09| |  \nPaperspace| A100 (80 GB)| Ampere| 1| 80| 12| 90| 3.18| 3.18| |  \nPaperspace| A100 (80 GB)| Ampere| 2| 160| 24| 180| 6.36| 3.18| |  \nPaperspace| A100 (80 GB)| Ampere| 4| 320| 48| 360| 12.72| 3.18| |  \nPaperspace| A100 (80 GB)| Ampere| 8| 640| 96| 720| 25.44| 3.18| |  \nRunPod| A100 (80 GB)| Ampere| 8| 640| 112| 1006| 15.12| 1.89| |  \nRunPod| A40 (48 GB)| Ampere| 8| 384| 40| 502| 6.32| 0.79| |  \nRunPod| A4000 (16 GB)| Ampere| 8| 128| 64| 234| 2.72| 0.34| |  \nRunPod| A5000 (24 GB)| Ampere| 8| 192| 64| 313| 3.52| 0.44| |  \nRunPod| A6000 (48 GB)| Ampere| 8| 384| 40| 502| 6.32| 0.79| |  \n  \nCloud\n\n|\n\nGPU Type\n\n|\n\nGPU Arch\n\n|\n\nGPUs\n\n|\n\nGPU RAM\n\n|\n\nvCPUs\n\n|\n\nRAM\n\n|\n\nOn-demand\n\n|\n\nPer-GPU\n\n|\n\nSpot\n\n|\n\nName  \n  \n---|---|---|---|---|---|---|---|---|---|---  \n  \n## Serverless GPUs\n\n### Notes\n\nWe use the classic definition of \"serverless\", courtesy of the original AWS\nannouncement on serverless computing: no server management, flexible scaling,\nhigh availability, and no idle capacity. We only include services that fit\nthis criterion in our options below.\n\nFurthermore, we only include services that provide serverless GPUs, which can\nbe used to run custom workloads, not just inference in particular models as a\nservice.\n\n  * Direct price comparisons are trickier for serverless offerings: cold boot time and autoscaling logic can substantially impact cost-of-traffic.\n  * Some of the providers allow configuration of CPU and RAM resources. We have selected reasonable defaults, generally comparable to the fixed offerings of other providers.\n  * You can find pricing pages for the providers here: Banana, Baseten, Beam, Modal, Replicate, RunPod\n  * Serverless GPUs are a newer technology, so the details change quickly and you can expect bugs/growing pains. Stay frosty!\n  * Raw data can be found in a csv on GitHub.\n\n_All prices are in $/hr._\n\nVendor\n\n|\n\nGPU Type\n\n|\n\nGPU Arch\n\n|\n\nGPUs\n\n|\n\nGPU RAM\n\n|\n\nvCPUs\n\n|\n\nRAM\n\n|\n\nPer-Hour\n\n|\n\nPer-GPU  \n  \n---|---|---|---|---|---|---|---|---  \nBanana| A5000 (24 GB)| Ampere| 1| 24| 4| 64| 2.32| 2.32  \nBaseten| T4 (16 GB)| Turing| 1| 16| 4| 16| 0.63| 0.64  \nBaseten| V100 (16 GB)| Volta| 1| 16| 16| 61| 3.67| 3.67  \nBaseten| A10G (24 GB)| Ampere| 1| 24| 8| 32| 1.45| 1.45  \nBaseten| A10G (24 GB)| Ampere| 2| 48| 24| 96| 3.4| 1.7  \nBaseten| A10G (24 GB)| Ampere| 4| 96| 48| 192| 6.81| 1.7  \nBaseten| A10G (24 GB)| Ampere| 8| 192| 192| 768| 19.55| 2.44  \nBaseten| A100 (80 GB)| Ampere| 1| 80| 12| 144| 6.12| 6.12  \nBaseten| A100 (80 GB)| Ampere| 2| 160| 24| 288| 12.29| 6.15  \nBaseten| A100 (80 GB)| Ampere| 4| 320| 48| 576| 24.58| 6.15  \nBaseten| A100 (80 GB)| Ampere| 8| 640| 96| 1152| 49.15| 6.14  \nBeam| A10G (24 GB)| Ampere| 1| 24| 4| 16| 2.11| 2.11  \nBeam| T4 (16 GB)| Turing| 1| 16| 4| 16| 1.55| 1.55  \nModal| T4 (16 GB)| Turing| 1| 16| 2| 16| 1.17| 1.17  \nModal| A10G (24 GB)| Ampere| 1| 24| 4| 16| 1.87| 1.87  \nModal| A100 (20 GB)| Ampere| 1| 20| 4| 16| 3.07| 3.07  \nModal| A100 (40 GB)| Ampere| 1| 40| 8| 40| 5.45| 5.45  \nModal| A100 (40 GB)| Ampere| 2| 40| 16| 80| 10.92| 5.45  \nModal| A100 (40 GB)| Ampere| 4| 40| 32| 160| 21.83| 5.45  \nReplicate| T4 (16 GB)| Turing| 1| 16| 4| 8| 0.81| 0.81  \nReplicate| A40| Ampere| 1| 48| 4| 16| 2.07| 2.07  \nReplicate| A40| Ampere| 1| 48| 10| 72| 2.61| 2.61  \nReplicate| A100 (40 GB)| Ampere| 1| 40| 10| 72| 4.14| 4.14  \nReplicate| A100 (80 GB)| Ampere| 1| 80| 10| 144| 5.04| 5.04  \nReplicate| A40| Ampere| 8| 384| 48| 680| 20.88| 2.61  \nRunPod| A4000 (16 GB)| Ampere| 1| 16| 6| 23| 0.72| 0.72  \nRunPod| A5000 (24 GB)| Ampere| 1| 24| 8| 29| 0.9| 0.9  \nRunPod| A6000 (48 GB)| Ampere| 1| 48| 16| 58| 1.44| 1.44  \nRunPod| A100 (80 GB)| Ampere| 1| 80| 14| 125| 3.6| 3.6  \nInferless| A100 (80 GB)| Ampere| 1| 80| 20| 200| 5.36| 5.36  \nInferless| A10 (24 GB)| Ampere| 1| 24| 7| 30| 1.22| 1.22  \nInferless| T4 (16 GB)| Turing| 1| 16| 3| 20| 0.66| 0.66  \n  \nVendor\n\n|\n\nGPU Type\n\n|\n\nGPU Arch\n\n|\n\nGPUs\n\n|\n\nGPU RAM\n\n|\n\nvCPUs\n\n|\n\nRAM\n\n|\n\nPer-Hour\n\n|\n\nPer-GPU  \n  \n---|---|---|---|---|---|---|---|---  \n  \n## How do I choose a GPU?\n\nThis page is intended to track and make explorable the current state of\npricing and hardware for cloud GPUs.\n\nIf you want advice on which machines and cards are best for your use case, we\nrecommend Tim Dettmer's blog post on GPUs for deep learning.\n\nThe whole post is a tutorial and FAQ on GPUS for DNNs, but if you just want\nthe resulting heuristics for decision-making, see the \"GPU Recommendations\"\nsection, which is the source of the chart below.\n\nFlowchart for quickly selecting an appropriate GPU for your needs, by Tim\nDettmers\n\n## GPU Raw Performance Numbers and Datasheets\n\nBelow are the raw TFLOPs of the different GPUs available from cloud providers.\n\nModel | Arch | FP32 | Mixed-precision | FP16 | Source  \n---|---|---|---|---|---  \nA100 | Ampere | 19.5 | 156 | 312 | Datasheet  \nA10G | Ampere | 35 | 35 | 70 | Datasheet  \nA6000 | Ampere | 38 | ? | ? | Datasheet  \nV100 | Volta | 14 | 112 | 28 | Datasheet  \nT4 | Turing | 8.1 | 65 | ? | Datasheet  \nP4 | Pascal | 5.5 | N/A | N/A | Datasheet  \nP100 | Pascal | 9.3 | N/A | 18.7 | Datasheet  \nK80 | Kepler | 8.73 | N/A | N/A | Datasheet  \nA40 | Ampere | 37 | 150 | 150 | Datasheet  \n  \n## GPU Performance Benchmarks\n\nBelow are some basic benchmarks for GPUs on common deep learning tasks.\n\nBenchmark of different GPUs on a single ImageNet epoch, by AIME\n\nBenchmark of different GPUs on a mix of tasks, by Lambda Labs\n\n## We are excited to share this course with you for **free**.\n\nWe have more upcoming great content. Subscribe to stay up to date as we\nrelease it.\n\nEnter\n\nWe take your privacy and attention very seriously and will never spam you.  I\nam already a subscriber\n\nThe Full Stack, 2023\n\nMade with  Material for MkDocs\n\n",
    "links": "[{\"link\": \"https://fullstackdeeplearning.com/march2019.html\", \"text\": \"\"}, {\"link\": \"https://fullstackdeeplearning.com/august2018.html\", \"text\": \"\"}]",
    "priceAndPlans": "Skip to content\n\nSign up for our latest in-person course!\n\nThe Full Stack\n\nCloud GPUs\n\nInitializing search\n\nThe Full Stack Website\n\n  * 923\n  * 182\n\n  * Home \n  * LLM Bootcamp \n  * Deep Learning Course \n  * Blog \n  * Cloud GPUs \n\nThe Full Stack\n\nThe Full Stack Website\n\n  * 923\n  * 182\n\n  * Home \n  * LLM Bootcamp \n\nLLM Bootcamp\n\n    * Spring 2023 \n\nSpring 2023\n\n      * Launch an LLM App in One Hour \n      * LLM Foundations \n      * Learn to Spell: Prompt Engineering \n      * Augmented Language Models \n      * Project Walkthrough: askFSDL \n      * UX for Language User Interfaces \n      * LLMOps \n      * What's Next? \n      * Reza Shabani: How to train your own LLM \n      * Harrison Chase: Agents \n      * Fireside Chat with Peter Welinder \n  * Deep Learning Course \n\nDeep Learning Course\n\n    * FSDL 2022 \n\nFSDL 2022\n\n      * Lecture 1: Course Vision and When to Use ML \n      * Lab Overview \n      * Lecture 2: Development Infrastructure & Tooling \n      * Lab 4: Experiment Management \n      * Lecture 3: Troubleshooting & Testing \n      * Lab 5: Troubleshooting & Testing \n      * Lecture 4: Data Management \n      * Lab 6: Data Annotation \n      * Lecture 5: Deployment \n      * Lab 7: Web Deployment \n      * Lecture 6: Continual Learning \n      * Lab 8: Model Monitoring \n      * Lecture 7: Foundation Models \n      * Lecture 8: ML Teams and Project Management \n      * Lecture 9: Ethics \n      * Project Showcase \n      * Course Announcement \n    * Older  Older \n      * FSDL 2021 \n\nFSDL 2021\n\n        * Synchronous Online Course \n        * Course Projects Showcase \n        * Lecture 1: DL Fundamentals \n        * Lab 1: Setup and Introduction \n        * Notebook: Coding a neural net \n        * Lecture 2A: CNNs \n        * Lecture 2B: Computer Vision \n        * Lab 2: CNNs and Synthetic Data \n        * Lecture 3: RNNs \n        * Lab 3: RNNs \n        * Lecture 4: Transformers \n        * Lab 4: Transformers \n        * Lecture 5: ML Projects \n        * Lecture 6: MLOps Infrastructure & Tooling \n        * Lab 5: Experiment Management \n        * Lecture 7: Troubleshooting Deep Neural Networks \n        * Lecture 8: Data Management \n        * Lab 6: Data Labeling \n        * Lecture 9: AI Ethics \n        * Lab 7: Paragraph Recognition \n        * Lecture 10: Testing & Explainability \n        * Lab 8: Testing & CI \n        * Lecture 11: Deployment & Monitoring \n        * Lab 9: Web Deployment \n        * Lecture 12: Research Directions \n        * Lecture 13: ML Teams and Startups \n        * Panel Discussion: Do I need a PhD to work in ML? \n      * FSDL 2021 (Berkeley) \n      * FSDL 2020 (UW) \n      * FSDL 2019 (Online) \n      * FSDL 2019 (Bootcamp) \n      * FSDL 2018 (Bootcamp) \n  * Blog \n  * Cloud GPUs  Cloud GPUs  Table of contents \n    * GPU Cloud Server Comparison \n      * Notes \n    * Serverless GPUs \n      * Notes \n    * How do I choose a GPU? \n    * GPU Raw Performance Numbers and Datasheets \n    * GPU Performance Benchmarks \n\nTable of contents\n\n  * GPU Cloud Server Comparison \n    * Notes \n  * Serverless GPUs \n    * Notes \n  * How do I choose a GPU? \n  * GPU Raw Performance Numbers and Datasheets \n  * GPU Performance Benchmarks \n\n# Cloud GPUs\n\nBy Sergey Karayev and Charles Frye. Updated October 30, 2023.\n\nDiscussion of this page on Hacker News, May 21, 2023.\n\nTraining and running neural networks often requires hardware acceleration, and\nthe most popular hardware accelerator is the venerable _graphics processing\nunit_ , or GPU.\n\nWe have assembled cloud GPU vendor pricing all into tables, sortable and\nfilterable to your liking!\n\nWe have split the vendor offerings into two classes:\n\n  * **GPU Cloud Servers** , which are long-running (but possibly pre-emptible) machines, and\n  * **Severless GPUs** , which are machines that scale-to-zero in the absence of traffic (like an AWS Lambda or Google Cloud Function)\n\n**We welcome your help in adding more cloud GPU providers and keeping the\npricing info current.**\n\nPlease file an issue or make a pull request to this repo, editing this file to\nupdate the text on this page or one of the CSV files to update the data:\n`cloud-gpus.csv` for servers and `serverless-gpus.csv` for serverless options.\n\n## GPU Cloud Server Comparison\n\n### Notes\n\nThe table below does not include all possible configurations for all\nproviders, as providers differ in their configuration strategy.\n\n  * Most providers, including AWS, Azure, and Lambda, provide instances with pre-set configurations.\n  * On GCP, any suitable machine can be connected to a configuration of GPUs.\n  * On other providers, like Oblivus Cloud, Cudo Compute, and RunPod, users have precise control over the resources they request. Note that RunPod's Community Cloud, Oblivus, and Cudo are all \"open clouds\", meaning compute is provided by third parties.\n  * For providers without pre-set instance configurations, we have selected configurations that are roughly equivalent to AWS's options. Generally, these configurations are good for workloads that require heavy inter-GPU communication.\n  * Where possible, regions were set to be the west or central parts of the United States. GPU availability depends on the region.\n  * Raw data can be found in a csv on GitHub.\n  * Costs can be substantially reduced via preemption recovery and failover across clouds. If you don't want to roll your own, consider a tool like SkyPilot. See discussion of their launch on Hacker News, December 13, 2022.\n\n_All prices are in $/hr._\n\nCloud\n\n|\n\nGPU Type\n\n|\n\nGPU Arch\n\n|\n\nGPUs\n\n|\n\nGPU RAM\n\n|\n\nvCPUs\n\n|\n\nRAM\n\n|\n\nOn-demand\n\n|\n\nPer-GPU\n\n|\n\nSpot\n\n|\n\nName  \n  \n---|---|---|---|---|---|---|---|---|---|---  \nAWS| A100 (80 GB)| Ampere| 8| 640| 96| 1152| 40.97| 5.12| | p4de.24xlarge  \nAWS| A100 (40 GB)| Ampere| 8| 320| 96| 1152| 32.77| 4.1| 9.83| p4d.24xlarge  \nAWS| V100 (16 GB)| Volta| 1| 16| 8| 61| 3.06| 3.06| 0.92| p3.2xlarge  \nAWS| V100 (16 GB)| Volta| 4| 64| 32| 244| 12.24| 3.06| 3.67| p3.8xlarge  \nAWS| V100 (16 GB)| Volta| 8| 128| 64| 488| 24.48| 3.06| 7.34| p3.16xlarge  \nAWS| V100 (32 GB)| Volta| 8| 256| 96| 768| 31.21| 3.9| 9.36| p3dn.24xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 16| 64| 1.62| 1.62| 0.49| g5.4xlarge  \nAWS| A10G (24 GB)| Ampere| 4| 96| 48| 192| 5.67| 1.42| 1.74| g5.12xlarge  \nAWS| A10G (24 GB)| Ampere| 8| 192| 192| 768| 16.29| 2.04| 4.89| g5.48xlarge  \nAWS| A10G (24 GB)| Ampere| 4| 96| 96| 384| 8.14| 2.04| 2.44| g5.24xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 4| 16| 1.01| 1.01| 0.3| g5.xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 8| 32| 1.21| 1.21| 0.39| g5.2xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 32| 128| 2.45| 2.45| 0.76| g5.8xlarge  \nAWS| A10G (24 GB)| Ampere| 1| 24| 64| 256| 4.1| 4.1| 1.23| g5.16xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 16| 64| 1.2| 1.2| 0.36| g4dn.4xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 4| 16| 0.53| 0.53| 0.16| g4dn.xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 8| 32| 0.75| 0.75| 0.23| g4dn.2xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 16| 64| 1.2| 1.2| 0.36| g4dn.4xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 32| 128| 2.18| 2.18| 0.65| g4dn.8xlarge  \nAWS| T4 (16 GB)| Turing| 1| 16| 64| 256| 4.35| 4.35| 1.21| g4dn.16xlarge  \nAWS| T4 (16 GB)| Turing| 4| 64| 48| 192| 3.91| 0.98| 1.31| g4dn.12xlarge  \nAWS| T4 (16 GB)| Turing| 8| 128| 96| 384| 7.82| 0.98| 2.35| g4dn.metal  \nAWS| T4 (16 GB)| Turing| 1| 16| 16| 64| 1.2| 1.2| 0.36| g4dn.4xlarge  \nAWS| K80 (12 GB)| Kepler| 1| 12| 4| 61| 0.9| 0.9| 0.27| p2.xlarge  \nAWS| K80 (12 GB)| Kepler| 8| 96| 32| 488| 7.2| 0.9| 2.16| p2.8xlarge  \nAWS| K80 (12 GB)| Kepler| 16| 192| 64| 732| 14.4| 0.9| 4.32| p2.16xlarge  \nAzure| A100 (80 GB)| Ampere| 1| 80| 24| 220| 3.67| 3.67| 1.47| NC24ads A100 v4  \nAzure| A100 (80 GB)| Ampere| 2| 160| 48| 440| 7.35| 3.67| 2.94| NC48ads A100\nv4  \nAzure| A100 (80 GB)| Ampere| 4| 320| 96| 880| 14.69| 3.67| 5.88| NC96ads A100\nv4  \nAzure| A100 (80 GB)| Ampere| 8| 640| 96| 1900| 37.18| 4.64| | ND96amsr A100 v4  \nAzure| V100 (16 GB)| Volta| 1| 16| 6| 112| 3.06| 3.06| 1.35| NC6s v3  \nAzure| V100 (16 GB)| Volta| 2| 32| 12| 224| 6.12| 3.06| 2.7| NC12s v3  \nAzure| V100 (16 GB)| Volta| 4| 64| 24| 448| 12.24| 3.06| 5.4| NC24s v3  \nAzure| P100 (16 GB)| Pascal| 1| 16| 6| 112| 2.07| 2.07| 0.22| NC6s v2  \nAzure| P100 (16 GB)| Pascal| 2| 32| 12| 224| 4.14| 2.07| 0.43| NC12s v2  \nAzure| P100 (16 GB)| Pascal| 4| 64| 24| 448| 8.28| 2.07| 0.87| NC24s v2  \nAzure| T4 (16 GB)| Turing| 1| 16| 4| 28| 0.53| 0.53| 0.21| NC4as T4 v3  \nAzure| T4 (16 GB)| Turing| 1| 16| 8| 56| 0.75| 0.75| 0.3| NC8as T4 v3  \nAzure| T4 (16 GB)| Turing| 1| 16| 16| 110| 1.2| 1.2| 0.48| NC16as T4 v3  \nAzure| T4 (16 GB)| Turing| 4| 64| 64| 440| 4.35| 1.09| 1.73| NC64as T4 v3  \nAzure| K80 (12 GB)| Kepler| 1| 12| 6| 56| 0.9| 0.9| 0.09| NC6  \nAzure| K80 (12 GB)| Kepler| 2| 24| 12| 112| 1.8| 0.9| 0.18| NC12  \nAzure| K80 (12 GB)| Kepler| 4| 48| 24| 224| 3.6| 0.9| 0.36| NC24  \nCudo Compute| A4000 (16 GB)| Ampere| 1| 16| 2| 4| 0.3| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 1| 16| 2| 4| 0.32| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 1| 24| 2| 4| 0.53| 0.52| |  \nCudo Compute| A5000 (24 GB)| Ampere| 1| 24| 2| 4| 0.57| 0.55| |  \nCudo Compute| A6000 (48 GB)| Ampere| 1| 48| 2| 4| 0.79| 0.78| |  \nCudo Compute| A4000 (16 GB)| Ampere| 1| 16| 4| 8| 0.3| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 1| 16| 4| 8| 0.32| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 1| 24| 4| 8| 0.53| 0.55| |  \nCudo Compute| A5000 (24 GB)| Ampere| 1| 24| 4| 8| 0.57| 0.58| |  \nCudo Compute| A6000 (48 GB)| Ampere| 1| 48| 4| 8| 0.79| 0.81| |  \nCudo Compute| A4000 (16 GB)| Ampere| 2| 32| 8| 16| 0.65| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 2| 32| 8| 16| 0.67| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 2| 48| 8| 16| 1.11| 0.55| |  \nCudo Compute| A5000 (24 GB)| Ampere| 2| 48| 8| 16| 1.17| 0.58| |  \nCudo Compute| A6000 (48 GB)| Ampere| 2| 96| 8| 16| 1.63| 0.81| |  \nCudo Compute| A4000 (16 GB)| Ampere| 2| 32| 16| 32| 0.74| 0.3| |  \nCudo Compute| A4000 (16 GB)| Ampere| 2| 32| 16| 32| 0.72| 0.29| |  \nCudo Compute| A5000 (24 GB)| Ampere| 2| 48| 16| 32| 1.18| 0.55| |  \nCudo Compute| A5000 (24 GB)| Ampere| 2| 48| 16| 32| 1.24| 0.58| |  \nCudo Compute| A6000 (48 GB)| Ampere| 2| 96| 16| 32| 1.7| 0.81| |  \nCudo Compute| A4000 (16 GB)| Ampere| 4| 64| 32| 64| 1.44| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 4| 64| 32| 64| 1.49| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 4| 96| 32| 64| 2.36| 0.55| |  \nCudo Compute| A5000 (24 GB)| Ampere| 4| 96| 32| 64| 2.49| 0.58| |  \nCudo Compute| A6000 (48 GB)| Ampere| 4| 192| 32| 64| 3.4| 0.81| |  \nCudo Compute| A4000 (16 GB)| Ampere| 8| 64| 64| 128| 2.89| 0.29| |  \nCudo Compute| A4000 (16 GB)| Ampere| 8| 64| 64| 128| 2.99| 0.3| |  \nCudo Compute| A5000 (24 GB)| Ampere| 8| 192| 64| 128| 4.73| 0.52| |  \nCudo Compute| A6000 (48 GB)| Ampere| 8| 384| 64| 128| 6.81| 0.78| |  \nDatacrunch| A100 (80 GB)| Ampere| 1| 80| 22| 120| 1.85| 1.85| | 1A100.22V  \nDatacrunch| A100 (80 GB)| Ampere| 2| 160| 44| 240| 3.7| 1.85| | 2A100.44V  \nDatacrunch| A100 (80 GB)| Ampere| 4| 320| 88| 480| 7.4| 1.85| | 4A100.88V  \nDatacrunch| A100 (80 GB)| Ampere| 8| 640| 176| 960| 14.8| 1.85| | 8A100.176V  \nDatacrunch| A6000 (48 GB)| Ampere| 1| 48| 10| 60| 1.25| 1.25| | 1A6000.10V  \nDatacrunch| A6000 (48 GB)| Ampere| 8| 384| 80| 480| 10| 1.25| | 8A6000.80V  \nDatacrunch| V100 (16 GB)| Volta| 1| 16| 6| 23| 1| 1| | 1V100.6V  \nDatacrunch| V100 (16 GB)| Volta| 2| 32| 10| 45| 2| 1| | 2V100.10V  \nDatacrunch| V100 (16 GB)| Volta| 4| 64| 20| 90| 4| 1| | 4V100.20V  \nDatacrunch| V100 (16 GB)| Volta| 8| 128| 48| 180| 8| 1| | 8V100.48V  \nExoscale| P100 (16 GB)| Pascal| 1| 16| 12| 56| 1.17| 1.17| | GPU SMALL  \nExoscale| P100 (16 GB)| Pascal| 2| 32| 24| 90| 1.71| 0.86| | GPU MEDIUM  \nExoscale| P100 (16 GB)| Pascal| 4| 64| 48| 120| 2.54| 0.64| | GPU LARGE  \nExoscale| P100 (16 GB)| Pascal| 8| 128| 96| 225| 2.82| 0.35| | GPU HUGE  \nExoscale| V100 (16 GB)| Tesla| 1| 16| 12| 56| 1.38| 1.38| | GPU2 SMALL  \nExoscale| V100 (16 GB)| Tesla| 2| 32| 16| 90| 2.01| 1.01| | GPU2 MEDIUM  \nExoscale| V100 (16 GB)| Tesla| 4| 64| 48| 225| 3.32| 0.83| | GPU2 HUGE  \nExoscale| A40 (48 GB)| Ampere| 1| 48| 12| 56| 2.13| 2.13| | GPU3 SMALL  \nExoscale| A40 (48 GB)| Ampere| 2| 96| 24| 120| 4.27| 2.14| | GPU3 MEDIUM  \nExoscale| A40 (48 GB)| Ampere| 4| 192| 48| 224| 8.53| 2.13| | GPU3 LARGE  \nExoscale| A40 (48 GB)| Ampere| 8| 384| 96| 448| 17.07| 2.13| | GPU3 HUGE  \nFluidStack| A100 (80 GB)| Ampere| 1| 80| 48| 256| 2.21| 2.21| |  \nFluidStack| A100 (80 GB)| Ampere| 2| 80| 96| 512| 4.42| 2.21| |  \nFluidStack| A100 (80 GB)| Ampere| 4| 80| 192| 768| 8.84| 2.21| |  \nFluidStack| A100 (80 GB)| Ampere| 8| 80| 384| 1024| 17.68| 2.21| |  \nFluidStack| H100 (80 GB)| Hopper| 1| 80| 48| 256| 4.76| 4.76| |  \nFluidStack| H100 (80 GB)| Hopper| 2| 80| 96| 512| 9.52| 4.76| |  \nFluidStack| H100 (80 GB)| Hopper| 4| 80| 192| 768| 19.04| 4.76| |  \nFluidStack| H100 (80 GB)| Hopper| 8| 80| 384| 1024| 38.08| 4.76| |  \nGCP| A100 (40 GB)| Ampere| 1| 40| 12| 85| 3.67| 3.67| 1.1| a2-highgpu-1g  \nGCP| A100 (40 GB)| Ampere| 2| 80| 24| 170| 7.34| 3.67| 2.2| a2-highgpu-2g  \nGCP| A100 (40 GB)| Ampere| 4| 160| 48| 340| 14.68| 3.67| 4.41| a2-highgpu-4g  \nGCP| A100 (40 GB)| Ampere| 8| 320| 96| 680| 29.36| 3.67| 8.81| a2-highgpu-8g  \nGCP| A100 (40 GB)| Ampere| 16| 640| 96| 1360| 55.68| 3.48| 16.72|\na2-megagpu-16g  \nGCP| V100 (16 GB)| Volta| 1| 16| 8| 52| 2.95| 2.95| 0.84| n1-highmem-8  \nGCP| V100 (16 GB)| Volta| 2| 32| 16| 104| 5.91| 2.95| 1.68| n1-highmem-16  \nGCP| V100 (16 GB)| Volta| 4| 64| 32| 208| 11.81| 2.95| 3.36| n1-highmem-32  \nGCP| V100 (16 GB)| Volta| 8| 128| 64| 416| 23.63| 2.95| 6.72| n1-highmem-64  \nGCP| P100 (16 GB)| Pascal| 1| 16| 8| 52| 1.93| 1.93| 0.53| n1-highmem-8  \nGCP| P100 (16 GB)| Pascal| 2| 32| 16| 104| 3.87| 1.93| 1.06| n1-highmem-16  \nGCP| P100 (16 GB)| Pascal| 4| 64| 32| 208| 7.73| 1.93| 2.12| n1-highmem-32  \nGCP| K80 (12 GB)| Kepler| 1| 12| 8| 52| 0.92| 0.92| 0.14| n1-highmem-8  \nGCP| K80 (12 GB)| Kepler| 2| 24| 16| 104| 1.85| 0.92| 0.28| n1-highmem-16  \nGCP| K80 (12 GB)| Kepler| 4| 48| 32| 208| 3.69| 0.92| 0.56| n1-highmem-32  \nGCP| K80 (12 GB)| Kepler| 8| 96| 64| 416| 7.39| 0.92| 1.12| n1-highmem-64  \nGCP| T4 (16 GB)| Turing| 1| 16| 8| 52| 0.82| 0.82| 0.21| n1-highmem-8  \nGCP| T4 (16 GB)| Turing| 2| 32| 16| 104| 1.65| 0.82| 0.42| n1-highmem-16  \nGCP| T4 (16 GB)| Turing| 4| 64| 32| 208| 3.29| 0.82| 0.84| n1-highmem-32  \nGCP| P4 (8 GB)| Pascal| 1| 8| 4| 26| 0.84| 0.84| 0.27| n1-highmem-4  \nGCP| P4 (8 GB)| Pascal| 2| 16| 8| 52| 1.67| 0.84| 0.54| n1-highmem-8  \nGCP| P4 (8 GB)| Pascal| 4| 32| 16| 104| 3.35| 0.84| 1.08| n1-highmem-16  \nJarvislabs| A100 (40 GB)| Ampere| 1| 40| 7| 32| 1.1| 1.1| 0.69|  \nJarvislabs| A100 (40 GB)| Ampere| 2| 80| 14| 64| 2.2| 1.1| 1.38|  \nJarvislabs| A100 (40 GB)| Ampere| 4| 160| 28| 128| 4.4| 1.1| 2.76|  \nJarvislabs| A100 (40 GB)| Ampere| 8| 320| 56| 256| 8.8| 1.1| 5.52|  \nJarvislabs| A6000 (48 GB)| Ampere| 1| 48| 7| 32| 0.99| 0.99| 0.59|  \nJarvislabs| A6000 (48 GB)| Ampere| 2| 96| 14| 64| 1.98| 0.99| 1.18|  \nJarvislabs| A6000 (48 GB)| Ampere| 4| 192| 28| 128| 3.96| 0.99| 2.36|  \nJarvislabs| A6000 (48 GB)| Ampere| 8| 384| 56| 256| 7.92| 0.99| 4.72|  \nJarvislabs| A5000 (24 GB)| Ampere| 1| 24| 7| 32| 0.59| 0.59| 0.39|  \nJarvislabs| A5000 (24 GB)| Ampere| 2| 48| 14| 64| 1.18| 0.59| 0.78|  \nJarvislabs| A5000 (24 GB)| Ampere| 4| 96| 28| 128| 2.36| 0.59| 1.56|  \nJarvislabs| A5000 (24 GB)| Ampere| 8| 192| 56| 256| 4.72| 0.59| 3.12|  \nJarvislabs| RTX5000 (16 GB)| Turing| 1| 16| 7| 32| 0.49| 0.49| 0.19|  \nJarvislabs| RTX5000 (16 GB)| Turing| 2| 32| 14| 64| 0.98| 0.49| 0.38|  \nJarvislabs| RTX5000 (16 GB)| Turing| 4| 64| 28| 128| 1.96| 0.49| 0.76|  \nJarvislabs| RTX5000 (16 GB)| Turing| 8| 128| 56| 256| 3.92| 0.49| 1.52|  \nLambda| A10G (24 GB)| Ampere| 1| 24| 30| 200| 0.6| 0.6| |  \nLambda| A100 (40 GB)| Ampere| 1| 40| 30| 200| 1.1| 1.1| |  \nLambda| A100 (40 GB)| Ampere| 2| 80| 60| 400| 2.2| 1.1| |  \nLambda| A100 (40 GB)| Ampere| 4| 160| 120| 800| 4.4| 1.1| |  \nLambda| A100 (40 GB)| Ampere| 8| 320| 124| 1800| 8.8| 1.1| |  \nLambda| A100 (80 GB)| Ampere| 8| 640| 240| 1800| 12| 1.5| |  \nLambda| A6000 (48 GB)| Ampere| 1| 48| 14| 100| 0.8| 0.8| |  \nLambda| A6000 (48 GB)| Ampere| 2| 96| 28| 200| 1.6| 0.8| |  \nLambda| A6000 (48 GB)| Ampere| 4| 192| 56| 400| 3.2| 0.8| |  \nLambda| Quadro RTX 6000 (24 GB)| Turing| 1| 24| 92| 448| 4.4| 4.4| |  \nLambda| H100 (80 GB)| Hopper| 1| 80| 26| 200| 1.99| 1.99| |  \nLambda| V100 (16 GB)| Volta| 8| 128| 92| 448| 4.4| 0.55| |  \nLatitude.sh| H100 (80 GB)| Hopper| 1| 80| 128| 192| 4.4| 4.4| | g3.small.x86  \nLatitude.sh| H100 (80 GB)| Hopper| 4| 320| 128| 768| 17.6| 4.4| |\ng3.medium.x86  \nLatitude.sh| A100 (80 GB)| Ampere| 8| 640| 128| 1536| 23.2| 2.9| |\ng3.large.x86  \nLatitude.sh| H100 (80 GB)| Hopper| 8| 640| 128| 1536| 32.2| 4.03| |\ng3.xlarge.x86  \nOblivus Cloud| A100 (80 GB)| Ampere| 1| 80| 4| 16| 2.55| 2.55| |  \nOblivus Cloud| A100 (80 GB)| Ampere| 2| 160| 8| 32| 5.1| 2.55| |  \nOblivus Cloud| A100 (80 GB)| Ampere| 4| 320| 16| 64| 10.2| 2.55| |  \nOblivus Cloud| A100 (80 GB)| Ampere| 8| 640| 32| 128| 20.4| 2.55| |  \nOblivus Cloud| A100 (40 GB)| Ampere| 1| 40| 4| 16| 2.39| 2.39| |  \nOblivus Cloud| A100 (40 GB)| Ampere| 2| 80| 8| 32| 4.78| 2.39| |  \nOblivus Cloud| A100 (40 GB)| Ampere| 4| 160| 16| 64| 9.56| 2.39| |  \nOblivus Cloud| A100 (40 GB)| Ampere| 8| 320| 32| 128| 19.12| 2.39| |  \nOblivus Cloud| A40 (48 GB)| Ampere| 1| 48| 4| 16| 1.54| 1.54| |  \nOblivus Cloud| A40 (48 GB)| Ampere| 2| 96| 8| 32| 3.08| 1.54| |  \nOblivus Cloud| A40 (48 GB)| Ampere| 4| 192| 16| 64| 6.16| 1.54| |  \nOblivus Cloud| A40 (48 GB)| Ampere| 8| 384| 32| 128| 12.32| 1.54| |  \nOblivus Cloud| V100 (16 GB)| Volta| 1| 16| 4| 16| 0.65| 0.65| |  \nOblivus Cloud| V100 (16 GB)| Volta| 2| 32| 8| 32| 1.3| 0.65| |  \nOblivus Cloud| V100 (16 GB)| Volta| 4| 64| 16| 64| 2.6| 0.65| |  \nOblivus Cloud| A6000 (48 GB)| Ampere| 1| 48| 4| 16| 1.54| 1.54| |  \nOblivus Cloud| A6000 (48 GB)| Ampere| 2| 96| 8| 32| 3.08| 1.54| |  \nOblivus Cloud| A6000 (48 GB)| Ampere| 4| 192| 16| 64| 6.16| 1.54| |  \nOblivus Cloud| A6000 (48 GB)| Ampere| 8| 384| 32| 128| 12.32| 1.54| |  \nOblivus Cloud| A5000 (24 GB)| Ampere| 1| 24| 4| 16| 0.98| 0.98| |  \nOblivus Cloud| A5000 (24 GB)| Ampere| 2| 48| 8| 32| 1.96| 0.98| |  \nOblivus Cloud| A5000 (24 GB)| Ampere| 4| 96| 16| 64| 3.92| 0.98| |  \nOblivus Cloud| A5000 (24 GB)| Ampere| 8| 192| 32| 128| 7.84| 0.98| |  \nOblivus Cloud| A4000 (16 GB)| Ampere| 1| 16| 4| 16| 0.81| 0.81| |  \nOblivus Cloud| A4000 (16 GB)| Ampere| 2| 32| 8| 32| 1.62| 0.81| |  \nOblivus Cloud| A4000 (16 GB)| Ampere| 4| 64| 16| 64| 3.24| 0.81| |  \nOblivus Cloud| RTX5000 (16 GB)| Turing| 1| 16| 4| 16| 0.76| 0.76| |  \nOblivus Cloud| RTX5000 (16 GB)| Turing| 2| 32| 8| 32| 1.52| 0.76| |  \nOblivus Cloud| RTX5000 (16 GB)| Turing| 4| 64| 16| 64| 3.04| 0.76| |  \nOblivus Cloud| RTX4000 (8 GB)| Turing| 1| 8| 4| 16| 0.41| 0.27| |  \nOblivus Cloud| RTX4000 (8 GB)| Turing| 2| 16| 8| 32| 0.82| 0.27| |  \nOblivus Cloud| RTX4000 (8 GB)| Turing| 4| 32| 16| 64| 1.64| 0.27| |  \nOVHcloud| V100 (16 GB)| Volta| 1| 16| 8| 45| 1.97| 1.97| | t1-45  \nOVHcloud| V100 (16 GB)| Volta| 2| 32| 18| 90| 3.94| 1.97| | t1-90  \nOVHcloud| V100 (16 GB)| Volta| 4| 64| 36| 180| 7.89| 1.97| | t1-180  \nOVHcloud| V100 (32 GB)| Volta| 1| 32| 14| 45| 2.19| 2.19| | t2-45  \nOVHcloud| V100 (32 GB)| Volta| 2| 64| 28| 90| 4.38| 2.19| | t2-90  \nOVHcloud| V100 (32 GB)| Volta| 4| 128| 56| 180| 8.76| 2.19| | t2-180  \nOracle Cloud| A10| Ampere| 1| 24| 15| 240| 2| 2| | VM.GPU.A10.1  \nOracle Cloud| A10| Ampere| 2| 48| 30| 480| 4| 2| | VM.GPU.A10.2  \nOracle Cloud| A100 (40 GB)| Ampere| 8| 320| 64| 2048| 24.4| 3.05| |\nBM.GPU.A4.8  \nOracle Cloud| A100 (80 GB)| Ampere| 8| 640| 128| 2048| 32| 4| |\nBM.GPU.A100-v2.8  \nOracle Cloud| H100 (80 GB)| Hopper| 8| 640| | | 80| 10| | BM.GPU.H100.8  \nPaperspace| V100 (16 GB)| Volta| 1| 16| 8| 30| 2.3| 2.3| |  \nPaperspace| V100 (32 GB)| Volta| 1| 32| 8| 30| 2.3| 2.3| |  \nPaperspace| V100 (32 GB)| Volta| 2| 64| 16| 60| 4.6| 2.3| |  \nPaperspace| V100 (32 GB)| Volta| 4| 128| 32| 120| 9.2| 2.3| |  \nPaperspace| A6000 (48 GB)| Ampere| 1| 48| 8| 45| 1.89| 1.89| |  \nPaperspace| A6000 (48 GB)| Ampere| 2| 96| 16| 90| 3.78| 1.89| |  \nPaperspace| A6000 (48 GB)| Ampere| 4| 192| 32| 180| 7.56| 1.89| |  \nPaperspace| A100 (40 GB)| Ampere| 1| 40| 12| 90| 3.09| 3.09| |  \nPaperspace| A100 (40 GB)| Ampere| 2| 80| 24| 180| 6.18| 3.09| |  \nPaperspace| A100 (40 GB)| Ampere| 4| 160| 48| 360| 12.36| 3.09| |  \nPaperspace| A100 (40 GB)| Ampere| 8| 320| 96| 720| 24.72| 3.09| |  \nPaperspace| A100 (80 GB)| Ampere| 1| 80| 12| 90| 3.18| 3.18| |  \nPaperspace| A100 (80 GB)| Ampere| 2| 160| 24| 180| 6.36| 3.18| |  \nPaperspace| A100 (80 GB)| Ampere| 4| 320| 48| 360| 12.72| 3.18| |  \nPaperspace| A100 (80 GB)| Ampere| 8| 640| 96| 720| 25.44| 3.18| |  \nRunPod| A100 (80 GB)| Ampere| 8| 640| 112| 1006| 15.12| 1.89| |  \nRunPod| A40 (48 GB)| Ampere| 8| 384| 40| 502| 6.32| 0.79| |  \nRunPod| A4000 (16 GB)| Ampere| 8| 128| 64| 234| 2.72| 0.34| |  \nRunPod| A5000 (24 GB)| Ampere| 8| 192| 64| 313| 3.52| 0.44| |  \nRunPod| A6000 (48 GB)| Ampere| 8| 384| 40| 502| 6.32| 0.79| |  \n  \nCloud\n\n|\n\nGPU Type\n\n|\n\nGPU Arch\n\n|\n\nGPUs\n\n|\n\nGPU RAM\n\n|\n\nvCPUs\n\n|\n\nRAM\n\n|\n\nOn-demand\n\n|\n\nPer-GPU\n\n|\n\nSpot\n\n|\n\nName  \n  \n---|---|---|---|---|---|---|---|---|---|---  \n  \n## Serverless GPUs\n\n### Notes\n\nWe use the classic definition of \"serverless\", courtesy of the original AWS\nannouncement on serverless computing: no server management, flexible scaling,\nhigh availability, and no idle capacity. We only include services that fit\nthis criterion in our options below.\n\nFurthermore, we only include services that provide serverless GPUs, which can\nbe used to run custom workloads, not just inference in particular models as a\nservice.\n\n  * Direct price comparisons are trickier for serverless offerings: cold boot time and autoscaling logic can substantially impact cost-of-traffic.\n  * Some of the providers allow configuration of CPU and RAM resources. We have selected reasonable defaults, generally comparable to the fixed offerings of other providers.\n  * You can find pricing pages for the providers here: Banana, Baseten, Beam, Modal, Replicate, RunPod\n  * Serverless GPUs are a newer technology, so the details change quickly and you can expect bugs/growing pains. Stay frosty!\n  * Raw data can be found in a csv on GitHub.\n\n_All prices are in $/hr._\n\nVendor\n\n|\n\nGPU Type\n\n|\n\nGPU Arch\n\n|\n\nGPUs\n\n|\n\nGPU RAM\n\n|\n\nvCPUs\n\n|\n\nRAM\n\n|\n\nPer-Hour\n\n|\n\nPer-GPU  \n  \n---|---|---|---|---|---|---|---|---  \nBanana| A5000 (24 GB)| Ampere| 1| 24| 4| 64| 2.32| 2.32  \nBaseten| T4 (16 GB)| Turing| 1| 16| 4| 16| 0.63| 0.64  \nBaseten| V100 (16 GB)| Volta| 1| 16| 16| 61| 3.67| 3.67  \nBaseten| A10G (24 GB)| Ampere| 1| 24| 8| 32| 1.45| 1.45  \nBaseten| A10G (24 GB)| Ampere| 2| 48| 24| 96| 3.4| 1.7  \nBaseten| A10G (24 GB)| Ampere| 4| 96| 48| 192| 6.81| 1.7  \nBaseten| A10G (24 GB)| Ampere| 8| 192| 192| 768| 19.55| 2.44  \nBaseten| A100 (80 GB)| Ampere| 1| 80| 12| 144| 6.12| 6.12  \nBaseten| A100 (80 GB)| Ampere| 2| 160| 24| 288| 12.29| 6.15  \nBaseten| A100 (80 GB)| Ampere| 4| 320| 48| 576| 24.58| 6.15  \nBaseten| A100 (80 GB)| Ampere| 8| 640| 96| 1152| 49.15| 6.14  \nBeam| A10G (24 GB)| Ampere| 1| 24| 4| 16| 2.11| 2.11  \nBeam| T4 (16 GB)| Turing| 1| 16| 4| 16| 1.55| 1.55  \nModal| T4 (16 GB)| Turing| 1| 16| 2| 16| 1.17| 1.17  \nModal| A10G (24 GB)| Ampere| 1| 24| 4| 16| 1.87| 1.87  \nModal| A100 (20 GB)| Ampere| 1| 20| 4| 16| 3.07| 3.07  \nModal| A100 (40 GB)| Ampere| 1| 40| 8| 40| 5.45| 5.45  \nModal| A100 (40 GB)| Ampere| 2| 40| 16| 80| 10.92| 5.45  \nModal| A100 (40 GB)| Ampere| 4| 40| 32| 160| 21.83| 5.45  \nReplicate| T4 (16 GB)| Turing| 1| 16| 4| 8| 0.81| 0.81  \nReplicate| A40| Ampere| 1| 48| 4| 16| 2.07| 2.07  \nReplicate| A40| Ampere| 1| 48| 10| 72| 2.61| 2.61  \nReplicate| A100 (40 GB)| Ampere| 1| 40| 10| 72| 4.14| 4.14  \nReplicate| A100 (80 GB)| Ampere| 1| 80| 10| 144| 5.04| 5.04  \nReplicate| A40| Ampere| 8| 384| 48| 680| 20.88| 2.61  \nRunPod| A4000 (16 GB)| Ampere| 1| 16| 6| 23| 0.72| 0.72  \nRunPod| A5000 (24 GB)| Ampere| 1| 24| 8| 29| 0.9| 0.9  \nRunPod| A6000 (48 GB)| Ampere| 1| 48| 16| 58| 1.44| 1.44  \nRunPod| A100 (80 GB)| Ampere| 1| 80| 14| 125| 3.6| 3.6  \nInferless| A100 (80 GB)| Ampere| 1| 80| 20| 200| 5.36| 5.36  \nInferless| A10 (24 GB)| Ampere| 1| 24| 7| 30| 1.22| 1.22  \nInferless| T4 (16 GB)| Turing| 1| 16| 3| 20| 0.66| 0.66  \n  \nVendor\n\n|\n\nGPU Type\n\n|\n\nGPU Arch\n\n|\n\nGPUs\n\n|\n\nGPU RAM\n\n|\n\nvCPUs\n\n|\n\nRAM\n\n|\n\nPer-Hour\n\n|\n\nPer-GPU  \n  \n---|---|---|---|---|---|---|---|---  \n  \n## How do I choose a GPU?\n\nThis page is intended to track and make explorable the current state of\npricing and hardware for cloud GPUs.\n\nIf you want advice on which machines and cards are best for your use case, we\nrecommend Tim Dettmer's blog post on GPUs for deep learning.\n\nThe whole post is a tutorial and FAQ on GPUS for DNNs, but if you just want\nthe resulting heuristics for decision-making, see the \"GPU Recommendations\"\nsection, which is the source of the chart below.\n\nFlowchart for quickly selecting an appropriate GPU for your needs, by Tim\nDettmers\n\n## GPU Raw Performance Numbers and Datasheets\n\nBelow are the raw TFLOPs of the different GPUs available from cloud providers.\n\nModel | Arch | FP32 | Mixed-precision | FP16 | Source  \n---|---|---|---|---|---  \nA100 | Ampere | 19.5 | 156 | 312 | Datasheet  \nA10G | Ampere | 35 | 35 | 70 | Datasheet  \nA6000 | Ampere | 38 | ? | ? | Datasheet  \nV100 | Volta | 14 | 112 | 28 | Datasheet  \nT4 | Turing | 8.1 | 65 | ? | Datasheet  \nP4 | Pascal | 5.5 | N/A | N/A | Datasheet  \nP100 | Pascal | 9.3 | N/A | 18.7 | Datasheet  \nK80 | Kepler | 8.73 | N/A | N/A | Datasheet  \nA40 | Ampere | 37 | 150 | 150 | Datasheet  \n  \n## GPU Performance Benchmarks\n\nBelow are some basic benchmarks for GPUs on common deep learning tasks.\n\nBenchmark of different GPUs on a single ImageNet epoch, by AIME\n\nBenchmark of different GPUs on a mix of tasks, by Lambda Labs\n\n## We are excited to share this course with you for **free**.\n\nWe have more upcoming great content. Subscribe to stay up to date as we\nrelease it.\n\nEnter\n\nWe take your privacy and attention very seriously and will never spam you.  I\nam already a subscriber\n\nThe Full Stack, 2023\n\nMade with  Material for MkDocs\n\n"
}