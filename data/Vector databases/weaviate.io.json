{
    "summary": "Skip to main content\n\nWe've updated the Python Client - introduced typing, faster imports, intuitive\ncode, and more. Read Shape the Future - Try Our New Python Client API to learn\nmore.\n\n ****\n\nProducts\n\nDevelopers\n\n  * Weaviate Docs\n  * Weaviate Cloud Services Docs\n  * Academy\n  * Workshops\n  * Partners\n  * Contributor Guide\n  * GitHub\n  * Slack\n  * Forum\n\nCompany\n\n  * About us\n  * Playbook\n  * Careers\n  * Investors\n  * Contact us\n\nBlogTry Now\n\n __Search\n\n\u2318K\n\n# A Gentle Introduction to Vector Databases\n\nAugust 1, 2023 \u00b7 12 min read\n\nLeonie Monigatti\n\nDeveloper Advocate\n\nIf you have just recently heard the term \u201cvector database\u201d for the first time,\nyou are not alone. Although vector databases have been around for a few years,\nthey\u2019ve just recently drawn the wider developer community\u2019s attention.\n\nThe excitement around vector databases is closely related to the release of\nChatGPT. Since late 2022, the public has started to understand the\ncapabilities of state-of-the-art large language models (LLMs), while\ndevelopers have realized that vector databases can enhance these models\nfurther.\n\nThis article will walk through what vector databases are and explain some of\ntheir core concepts, such as vector embeddings and vector search. Then we will\ndive into the technical details of how distance metrics are used in vector\nsearch and how vector indexes enable efficient retrieval. Once we have a\nfundamental understanding of vector databases, we will discuss their use cases\nand the current tool landscape.\n\n## What is a Vector Database?\u200b\n\nA vector database indexes, stores, and provides access to structured or\nunstructured data (e.g., text or images) alongside its vector embeddings,\nwhich are the data's numerical representation. It allows users to find and\nretrieve similar objects quickly at scale in production.\n\nThis section introduces the core concepts of vector databases. We will discuss\nwhat vector embeddings are, how they enable similarity search, and how they\naccelerate it.\n\n### Vector Embeddings\u200b\n\nWhen you think of data, you probably think of neatly organized numbers in\nspreadsheets. This type of data is called structured data because it can\neasily be stored in tabular format. But roughly 80% of today\u2019s data is said to\nbe unstructured. Examples of unstructured data are images, text (e.g.,\ndocuments, social media posts, or emails), or time series data (e.g., audio\nfiles, sensor data, or video). The problem with unstructured data is that it\nis difficult to store it in an organized way so you can easily find what you\nare looking for.\n\nBut innovations in Artificial Intelligence (AI) and Machine Learning (ML) have\nenabled us to numerically represent unstructured data without losing its\nsemantic meaning in so-called **vector embeddings**. A vector embedding is\njust a long list of numbers, each describing a feature of the data object.\n\nAn example is how we numerically represent colors in the RGB system, where\neach number in the vector describes how red, green, or blue a color is. E.g.,\nthe following green color can be represented as `[6, 205, 0]` in the RGB\nsystem.\n\nBut fitting more complex data, such as words, sentences, or text, into a\nmeaningful series of numbers isn\u2019t trivial. This is where ML models come in:\nML models enable us to represent the meaning of, e.g., a word as a vector\nbecause they have learned to represent the relationship between different\nwords in a **vector space**. These types of ML models are also called\n**embeddings model** or **vectorizer**.\n\nBelow you can see an example of the three-dimensional vector space of the RGB\nsystem with a few sample data points (colors).\n\nDepending on the ML model used, the data can be represented in different\nvector spaces. Vector spaces can differ in the number of dimensions ranging\nfrom tens to thousands, where the precision of the representation increases\nwith increasing dimensions. In the color example, there are also different\nways to represent a color numerically. E.g., you can represent the previous\ngreen color in the RGB system as `[6, 205, 0]` or in the CMYK system as `[97,\n0, 100, 20]`.\n\nThis is why it is important to use the same vectorizer for all your data to\nensure it is in the respective vector space.\n\nVector embeddings numerically capture the semantic meaning of the objects in\nrelation to other objects. Thus, similar objects are grouped together in the\nvector space, which means the closer two objects, the more similar they are.\n\nFor now, let\u2019s consider a simpler example with numerical representations of\nwords - also called word vectors. In the following image, you can see the\nwords \u201cWolf\u201d and \u201cDog\u201d close to each other because dogs are direct descendants\nof wolfs. Close to the dog, you can see the word \u201cCat,\u201d which is similar to\nthe word \u201cDog\u201d because both are animals that are also common pets. But further\naway, on the right-hand side of the illustrated vector space, you can see\nwords that represent fruit, such as \u201cApple\u201d or \u201cBanana\u201d, which are close to\neach other but further away from the animal terms.\n\nFor an in-depth explanation of vector embeddings, see Vector Embeddings\nExplained\n\n### Vector Search\u200b\n\nVector embeddings allow us to find and retrieve similar objects from the\nvector database by searching for objects that are close to each other in the\nvector space, which is called vector search, similarity search, or semantic\nsearch.\n\nSimilarly to how we can find similar vectors for a dog, we can find similar\nvectors to a search query. E.g. to find words similar to the word \u201cKitten\u201d, we\ncan generate a vector embedding for the word \u201cKitten\u201d and retrieve all items\nclose to the query vector, such as the word \u201cCat\u201d, as illustrated below.\n\nThe numerical representation of a data object allows us to apply mathematical\noperations to them, such as calculating the distance between two vector\nembeddings to determine their similarity. To calculate the distance between\ntwo vectors, you can use several similarity measures. E.g., Weaviate supports\nthe following distance metrics:\n\n  *  **Squared** **Euclidean or L2-squared distance** calculates the straight-line distance between two vectors. Its range is [0, \u221e], where 0 represents identical vectors, and larger values represent increasingly dissimilar vectors.\n  *  **Manhattan or L1 distance** calculates the sum of the lengths of the projections of the line segment between the points onto the coordinate axes. Its range is [0, \u221e], where 0 represents identical vectors, and larger values represent increasingly dissimilar vectors.\n  *  **Cosine similarity** calculates the cosine of the angle between two vectors. In Weaviate, the cosine distance is used for the complement of cosine similarity. Its range is [0, 2], where 0 represents identical vectors, and 2 represents vectors that point in opposite directions.\n  *  **Dot product**  calculates the product of the magnitudes of two vectors and the cosine of the angle between them. Its range is [-\u221e, \u221e], where 0 represents orthogonal vectors, and larger values represent increasingly similar vectors. In Weaviate, the negative dot product is used to keep the intuition that larger values represent increasingly dissimilar vectors.\n  *  **Hamming distance** calculates the number of differences between vectors at each dimension.\n\nAs you can see, there are many different similarity measures. As a rule of\nthumb, select the same metric as the one used to train the ML model. If you\nare unsure, which one was used, you can usually find this information on the\nhosting service\u2019s site, e.g., OpenAI\u2019s `ada-002` uses cosine similarity or\nMiniLM hosted on Hugging Face supports cosine similarity, dot product, and\nEuclidean distance.\n\nTo learn more about the different distance metrics, you can continue reading\nour blog post on What are Distance Metrics in Vector Search?\n\n### Vector Indexing for Approximate Nearest Neighbor Approach\u200b\n\nVector indexing is the process of organizing vector embeddings in a way that\ndata can be retrieved efficiently.\n\nWhen you want to find the closest items to your query vector, the brute force\napproach would be to use the k-Nearest Neighbors (kNN) algorithm. But\ncalculating the similarity between your query vector and every entry in the\nvector database can become computationally expensive if you have millions or\neven billions of data points because the required calculations increase\nlinearly (O(n)) with the dimensionality and the number of data points.\n\nA more efficient solution to find similar objects is to use an **approximate\nnearest neighbor (ANN) approach**. The underlying idea is to pre-calculate the\ndistances between the vector embeddings and organize and store similar vectors\nclose to each other (e.g., in clusters or a graph), so that you can later find\nsimilar objects faster. This process is called vector indexing. Note, that the\nspeed gains are traded in for some accuracy because the ANN approach returns\nonly the approximate results.\n\nIn the previous example, one option would be to pre-calculate some clusters,\ne.g., animals, fruits, and so on. So, when you query the vector database for\n\"Kitten\", you could start your search by only looking at the closest animals\nand not waste time calculating the distances between all fruits and other non-\nanimal objects. More specifically, the ANN algorithm can help you to start the\nsearch in a region near, e.g., four-legged animals. Then the algorithm would\nalso prevent you from venturing further from relevant results, as the objects\nare pre-organized by similarity.\n\nThe above example roughly describes the concept of the Hierarchical Navigable\nSmall World (HNSW) algorithm, which is the default ANN algorithm in Weaviate.\nHowever, there are several ANN algorithms to index the vectors, which can be\ncategorized into the following groups:\n\n  *  **Clustering** -based index (e.g., FAISS)\n  *  **Proximity graph** -based index (e.g., HNSW)\n  *  **Tree** -based index (e.g., ANNOY)\n  *  **Hash** -based index (e.g., LSH)\n  *  **Compression** -based index (e.g., PQ or SCANN)\n\nNote that indexing enables fast retrieval at query time, but it can take a lot\nof time to build the index initially.\n\nFurther reading Why Is Vector Search So Fast?\n\n## Use Cases of Vector Databases\u200b\n\nVector databases\u2019 search capabilities can be used in various applications\nranging from classical ML use cases, such as recommender systems, to providing\nlong-term memory to large language models in modern applications.\n\nThe **most popular use case of vector databases is for search**. Because a\nvector database can help find similar objects, it is predestined for\napplications where you might want to find similar products, movies, books,\nsongs, etc. That\u2019s why vector databases are also **used in recommendation\nsystems in classical ML applications** as a restated task of search.\n\nWith the rise of LLMs, **vector databases have already been used to enhance\nmodern LLM applications**. You might have already come across the term that\nvector databases provide LLMs with long-term memory. LLMs are stateless, which\nmeans that they immediately forget what you have just discussed if you don\u2019t\nstore this information in, e.g., a vector database and thus provide them with\na state. This enables LLMs to hold an actual conversation. Also, you can store\nthe domain-specific context in a vector database to minimize the LLM from\nhallucinating. Other popular examples of vector databases improving the\ncapabilities of LLMs are question-answering and retrieval-augmented\ngeneration.\n\nHere is an example demo called HealthSearch of a Weaviate vector database in\naction together with an LLM showcasing the potential of leveraging user-\nwritten reviews and queries to retrieve supplement products based on specific\nhealth effects.\n\n## Tool Landscape around Vector Databases\u200b\n\nAlthough vector databases are AI-native and specifically designed to handle\nvector embeddings and enable efficient vector search, alternatives like vector\nlibraries and vector-capable databases exist as well.\n\n### Vector Database vs. Traditional (Relational) Database\u200b\n\nThe main difference between a traditional (relational) database and a modern\nvector database comes from the type of data they were optimized for. While a\nrelational database is designed to store structured data in columns, a vector\ndatabase is also optimized to store unstructured data (e.g., text, images, or\naudio) and their vector embeddings.\n\nBecause vector databases and relational databases are optimized for different\ntypes of data, they also differ in the way data is stored and retrieved. In a\nrelational database, data is stored in columns and retrieved by keyword\nmatches in traditional search. In contrast, vector databases also store the\nvector embeddings of the original data, which enables efficient semantic\nsearch.\n\nFor example, imagine you have a database that stores Jeopardy questions, and\nyou want to retrieve all questions that contain an animal. Because traditional\nsearch relies on keyword matches, you would have to create a big query that\nqueries all animals (e.g., contains \"dog\", or contains \"cat\", or contains\n\"wolf\", etc.). With semantic search, you could simply query for the concept of\n\"animals\".\n\nBecause most vector databases do not only store vector embeddings but store\nthem together with the original source data, they are not only capable of\nvector search but also enable traditional keyword search. Some vector\ndatabases like Weaviate even support hybrid search capabilities which combine\nvector search with keyword search.\n\n### Vector Database vs. Vector-Capable Database (SQL and NoSQL)\u200b\n\nToday many existing databases have already enabled vector support and vector\nsearch. However, they usually don\u2019t index the vector embeddings, which makes\nthe vector search slow. Thus, an advantage of AI-native vector databases over\nvector-capable databases is their efficiency in vector search due to vector\nindexing.\n\n### Vector Database vs. Vector Indexing Library\u200b\n\nSimilarly to vector databases, vector libraries also enable fast vector\nsearch. However, vector libraries only store vector embeddings of data\nobjects, and they store them in in-memory indexes. This results in two key\ndifferences:\n\n  1. Updatability: The index data is immutable, and thus no real-time updates are possible.\n  2. Scalability: Most vector libraries cannot be queried while importing your data, which can be a scalability concern for applications that require importing millions or even billions of objects.\n\nThus, vector libraries are a great solution for applications with a limited\nstatic snapshot of data. However, if your application requires real-time\nscalable semantic search at the production level, you should consider using a\nvector database.\n\n## Summary\u200b\n\nThis article explained that vector databases are a type of database that\nindexes, stores, and provides access to structured or unstructured data\nalongside its vector embeddings. Vector databases like Weaviate allow for\nefficient similarity search and retrieval of data based on their vector\ndistance or similarity at scale.\n\nWe covered core concepts around vector databases, such as unstructured data\nand vector embeddings, and discussed that vector databases enable efficient\nvector search by leveraging ANN algorithms. Additionally, we explored the tool\nlandscape around vector databases and discussed the advantages of vector\ndatabases over traditional and vector-capable databases and vector libraries.\n\n## What's next\u200b\n\nCheck out Getting Started with Weaviate, and begin building amazing apps with\nWeaviate.\n\nYou can reach out to us on Slack or Twitter, or join the community forum.\n\nWeaviate is open source, and you can follow the project on GitHub. Don\u2019t\nforget to give us a \u2b50\ufe0f while you are there!\n\n **Tags:**\n\n  * concepts\n\nEdit this page\n\nNewer Post\n\nDistance Metrics in Vector Search\n\nOlder Post\n\nDiscover Healthsearch Unlocking Health with Semantic Search \u2728\n\n  * What is a Vector Database?\n    * Vector Embeddings\n    * Vector Search\n    * Vector Indexing for Approximate Nearest Neighbor Approach\n  * Use Cases of Vector Databases\n  * Tool Landscape around Vector Databases\n    * Vector Database vs. Traditional (Relational) Database\n    * Vector Database vs. Vector-Capable Database (SQL and NoSQL)\n    * Vector Database vs. Vector Indexing Library\n  * Summary\n\nWeaviate Cloud Services\n\n  * Products\n  * Console\n  * Partners\n  * Terms & Policies\n\nCommunity\n\n  * Slack\n  * Instagram\n  * Twitter\n  * GitHub\n  * Forum\n\nMeetups\n\n  * Amsterdam\n  * Boston\n  * New York\n  * San Francisco\n  * Toronto\n\nMore\n\n  * Blog\n  * Podcast\n  * Playbook\n\nCopyright \u00a9 2023 Weaviate, B.V. Built with Docusaurus.\n\n"
}