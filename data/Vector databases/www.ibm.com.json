{
    "summary": "My IBM Log in\n\n#  What is a vector database?\n\nLearn more about watsonx\n\nA vector database is designed to store, manage and index massive quantities of\nhigh-dimensional vector data efficiently. These databases are rapidly growing\nin interest to create additional value for generative artificial intelligence\n(AI) use cases and applications. According to Gartner, by 2026, more than 30\npercent of enterprises will have adopted vector databases to ground their\nfoundation models with relevant business data.1\n\nUnlike traditional relational databases with rows and columns, data points in\na vector database are represented by vectors with a fixed number of\ndimensions, clustered based on similarity. This design enables low latency\nqueries, making them ideal for AI-driven applications.\n\n\n\nNow available: watsonx\n\nAn AI and data platform with a set of AI assistants designed to help you scale\nand accelerate the impact of AI with trusted data across your business.\n\nRelated content\n\nSubscribe to the IBM newsletter\n\n##  Vector databases vs. traditional databases  \n\nThe nature of data has undergone a profound transformation. It's no longer\nconfined to structured information easily stored in traditional databases.\nUnstructured data is growing 30 to 60 percent year over year, comprising\nsocial media posts, images, videos, audio clips and more.2 Typically, if you\nwanted to load unstructured data sources into a traditional relational\ndatabase to store, manage and prepare for AI, the process is labor-intensive\nand far from efficient, especially when it comes to new generative use cases\nsuch as similarity search. Relational databases are great for managing\nstructured and semi-structured datasets in specific formats, while vector\ndatabases are best suited for unstructured datasets through high-dimensional\nvector embeddings.\n\n##  What are vectors?\n\nEnter vectors. Vectors are arrays of numbers that can represent complex\nobjects like words, images, videos and audio, generated by a machine learning\n(ML) model. High-dimensional vector data is essential to machine learning,\nnatural language processing (NLP) and other AI tasks. Some examples of vector\ndata include:\n\n  * Text: Think about the last time you interacted with a chatbot. How do they understand natural language? They rely on vectors which can represent words, paragraphs and entire documents, that are converted via machine learning algorithms. \n\n  * Images: Image pixels can be described by numerical data and combined to make up a high-dimensional vector for that image. \n\n  * Speech/Audio: Like images, sound waves can also be broken down into numerical data and represented as vectors, enabling AI applications such as voice recognition. \n\n##  What are vector embeddings?\n\nThe volume of unstructured datasets your organization needs for AI will only\ncontinue to grow, so how do you handle millions of vectors? This is where\nvector embeddings and vector databases come into play. These vectors are\nrepresented in a continuous, multi-dimensional space known as an embedding,\nwhich are generated by embedding models, specialized to convert your vector\ndata into an embedding. Vector databases serve to store and index the output\nof an embedding model. Vector embeddings are a numerical representation of\ndata, grouping sets of data based on semantic meaning or similar features\nacross virtually any data type.  \n\nFor example, take the words \u201ccar\u201d and \u201cvehicle.\u201d They both have similar\nmeanings even though they are spelled differently. For an AI application to\nenable effective semantic search, vector representations of \u201ccar\u201d and\n\u201cvehicle\u201d must capture their semantic similarity. When it comes to machine\nlearning, embeddings represent high-dimensional vectors that encode this\nsemantic information. These vector embeddings are the backbone of\nrecommendations, chatbots and generative apps like ChatGPT.\n\n##  Vector database vs graph database  \n\nKnowledge graphs represent a network of entities such as objects or events and\ndepicts the relationship between them. A graph database is a fit-for-purpose\ndatabase for storing knowledge graph information and visualizing it as a graph\nstructure. Graph databases are built on nodes and edges that represent the\nknown entities and complex relationships between them, while vector databases\nare built on high-dimensional vectors. As a result, graph databases are\npreferred for processing complex relationships between data points while\nvector databases are better for handling different forms of data such as\nimages or videos.\n\n##  How vector embeddings and vector databases work\n\nEnterprise vector data can be fed into an embedding model such as IBM\u2019s\nwatsonx.ai models or Hugging Face (link resides outside ibm.com), which are\nspecialized to convert your data into an embedding by transforming complex,\nhigh-dimensional vector data into numerical forms that computers can\nunderstand. These embeddings represent the attributes of your data used in AI\ntasks such as classification and anomaly detection.\n\nVector storage\n\nVector databases store the output of an embedding model algorithm, the vector\nembeddings. They also store each vector\u2019s metadata, which can be queried using\nmetadata filters. By ingesting and storing these embeddings, the database can\nthen facilitate fast retrieval of a similarity search, matching the user\u2019s\nprompt with a similar vector embedding.\n\nVector indexing\n\nStoring data as embeddings isn't enough. The vectors need to be indexed to\naccelerate the search process. Vector databases create indexes on vector\nembeddings for search functionality. The vector database indexes vectors using\na machine learning algorithm. Indexing maps vectors to new data structures\nthat enable faster similarity or distance searches, such as nearest neighbor\nsearch between vectors.\n\nSimilarity search based on querying or prompting\n\nQuerying vectors can be done via calculations measuring the distance between\nvectors using algorithms, such as nearest neighbor search. This measuring can\nbe based on various similarity metrics such as cosine similarity, used by that\nindex to measure how close or distant those vectors are. When a user queries\nor prompts an AI model, an embedding is computed using the same embedding\nmodel algorithm. The database calculates distances and performs similarity\ncalculations between query vectors and vectors stored in the index. They\nreturn the most similar vectors or nearest neighbors according to the\nsimilarity ranking. These calculations support various machine learning tasks\nsuch as recommendation systems, semantic search, image recognition and other\nnatural language processing tasks.\n\n##  Vector databases and retrieval augmented generation (RAG)\n\nEnterprises are increasingly favoring retrieval augmented generation (RAG)\napproach in generative AI workflows for its faster time-to-market, efficient\ninference and reliable output, particularly in key use cases such as customer\ncare and HR/Talent. RAG ensures that the model is linked to the most current,\nreliable facts and that users have access to the model\u2019s sources, so that its\nclaims can be checked for accuracy. RAG is core to our ability to anchor large\nlanguage models in trusted data to reduce model hallucinations. This approach\nrelies on leveraging high-dimensional vector data to enrich prompts with\nsemantically relevant information for in-context learning by foundation\nmodels. It requires effective storage and retrieval during the inference\nstage, which handles the highest volume of data. Vector databases excel at\nefficiently indexing, storing and retrieving these high-dimensional vectors,\nproviding the speed, precision and scale needed for applications like\nrecommendation engines and chatbots.\n\n##  Advantages of vector databases\n\nWhile it\u2019s clear that vector database functionality is rapidly growing in\ninterest and adoption to enhance enterprise AI-based applications, the\nfollowing benefits have also demonstrated business value for adopters:\n\n**Speed and performance** : Vector databases use various indexing techniques\nto enable faster searching. Vector indexing along with distance-calculating\nalgorithms such as nearest neighbor search, are particularly helpful with\nsearching for relevant results across millions if not billions of data points,\nwith optimized performance.\n\n**Scalability** : Vector databases can store and manage massive amounts of\nunstructured data by scaling horizontally, maintaining performance as query\ndemands and data volumes increase.\n\n**Cost of ownership** : Vector databases are a valuable alternative to\ntraining foundation models from scratch or fine-tuning them. This reduces the\ncost and speed of inferencing of foundation models.\n\n**Flexibility** : Whether you have images, videos or other multi-dimensional\ndata, vector databases are built to handle complexity. Given the multiple use\ncases ranging from semantic search to conversational AI applications, the use\nof vector databases can be customized to meet your business and AI\nrequirements.\n\n**Long term memory of LLMs** : Organizations can start with a general-purpose\nmodels like IBM watsonx.ai\u2019s Granite series models, Meta's Llama-2 or Google's\nFlan models, and then provide their own data in a vector database to enhance\nthe output of the models and AI applications critical to retrieval augmented\ngeneration.\n\n**Data management components** : Vector databases also typically provide\nbuilt-in features to easily update and insert new unstructured data.\n\n##  Considerations for vector databases and your data strategy\n\nThere is a breadth of options when it comes to choosing a vector database\ncapability to meet your organization\u2019s data and AI needs.\n\nTypes of vector databases\n\n\n\nThere are a few alternatives to choose from.\n\n  * Standalone, proprietary vector databases such as Pinecone\n  * Open-source solutions such as weaviate or milvus, which provide built-in RESTful APIs and support for Python and Java programming languages\n  * Platforms with vector database capabilities integrated, coming soon to IBM watsonx.data\n\n  * Vector database/search extensions such as PostgreSQL\u2019s open source pgvector extension, providing vector similarity search capabilities\n\nIntegration with your data ecosystem\n\nVector databases should not be considered as standalone capabilities, but\nrather a part of your broader data and AI ecosystem. Many offer APIs, native\nextensions or can be integrated with your databases. Since they are built to\nleverage your own enterprise data to enhance your models, you must also have\nproper data governance and security in place to ensure the data with which you\nare grounding these LLMs can be trusted.\n\nThis is where a trusted data foundation plays an important role in AI, and\nthat starts with your data and how it\u2019s stored, managed and governed before\nbeing used for AI. Central to this is a data lakehouse, one that is open,\nhybrid and governed, such IBM watsonx.data, part of the watsonx AI data\nplatform that fits seamlessly into a data fabric architecture. For example,\nIBM watsonx.data, is built to access, catalog, govern and transform all of\nyour structured, semi-structured and unstructured data and metadata. You can\nthen leverage this governed data and watsonx.data\u2019s integrated vector database\ncapabilities (tech preview Q4, 2023) for machine learning and generative AI\nuse cases.\n\nWhen vector indexing is not optimal\n\nUsing a vector store and index is well suited for applications that are based\non facts or fact-based querying. For example, asking about a company\u2019s legal\nterms last year or extracting specific information from complex documents. The\nset of retrieval context you would get would be those that are most\nsemantically similar to your query through embedding distance. However, if you\nwant to get a summary of topics, this doesn\u2019t lend itself well to a vector\nindex. In this case you would want the LLM to go through all of the different\npossible contexts on that topic within your data. Instead, you may use a\ndifferent kind of index, such as a list index rather than a vector index,\nsince a vector index would only fetch the most relevant data.  \n\n##  Use Cases of Vector Databases\n\nThe applications of vector databases are vast and growing. Some key use cases\ninclude:\n\n**Semantic search** : Perform searches based on the meaning or context of a\nquery, enabling more precise and relevant results. As not only words but\nphrases can be represented as vectors, semantic vector search functionality\nunderstands user intent better than general keywords.\n\n**Similarity search and applications** : Find similar images, text, audio or\nvideo data with ease, for content retrieval including advanced image and\nspeech recognition, natural language processing and more.\n\n**Recommendation engines** : E-commerce sites, for instance, can use vector\ndatabases and vectors to represent customer preferences and product\nattributes. This enables them to suggest items similar to past purchases based\non vector similarity, enhancing user experience and increasing retention.\n\n**Conversational AI** : Improving virtual agent interactions by enhancing the\nability to parse through relevant knowledge bases efficiently and accurately\nto provide real-time contextual answers to user queries, along with the source\ndocuments and page numbers for reference.\n\n##  Vector database capabilities\n\nwatsonx.ai\n\nA next generation enterprise studio for AI builders to build, train, validate,\ntune and deploy both traditional machine learning and new generative AI\ncapabilities powered by foundation models. Build a Q&A resource from a broad\ninternal or external knowledge base with the help of AI tasks in watsonx.ai,\nsuch as retrieval augmented generation.\n\nLearn more\n\n* * *\n\nwatsonx.data\n\nA fit-for-purpose data store built on an open data lakehouse architecture to\nscale AI workloads, for all your data, anywhere. Store, query and search\nvector embeddings in watsonx.data with integrated vector capabilities (planned\ntech preview Q4 2023).\n\nLearn more\n\n* * *\n\nIBM Cloud\u00ae Databases for PostgreSQL-\n\nOur PostgreSQL database-as-a-service offering lets teams spend more time\nbuilding with high availability, backup orchestration, point-in-time-recovery\n(PITR) and read replica with ease. PostgreSQL offers pgvector, an open-source\nvector extension that will be able to be configured with IBM Cloud PostgreSQL\nextensions (coming soon), providing vector similarity search capabilities.\n\nLearn more\n\nIBM Cloud Databases for Elasticsearch\n\nOur Elasticsearch database-as-a-service comes with a full-text search engine,\nwhich makes it the perfect home for your unstructured text data. Elasticsearch\nalso support various forms of semantic (link resides outside ibm.com)\nsimilarity search. It supports dense vectors (link resides outside ibm.com)\nfor exact nearest neighbor search, but it also provides built-in AI models to\ncompute sparse vectors and conduct advanced similarity search (link resides\noutside ibm.com).\n\nLearn more\n\n##  Vector database resources\n\nBlog  Foundation models and data stores unlock the potential of generative AI\n\nOrganizations that utilize generative AI models correctly can see a myriad of\nbenefits\u2014from increased operational efficiency and improved decision-making to\nthe rapid creation of marketing content.\n\nNews  Enterprise-ready, IBM-developed watsonx Granite models now available\n\nIBM announces the general availability of the first models in the watsonx\nGranite model series \u2014 a collection of generative AI models to advance the\ninfusion of generative AI into business applications and workflows.\n\nResearch  What is retrieval-augmented generation?\n\nRAG is an AI framework for retrieving facts from an external knowledge base to\nground LLMs on the most accurate, up-to-date information and to give users\ninsight into LLMs\u2019 generative process.\n\nTake the next step\n\nWatsonx is an AI and data platform with a set of AI assistants designed to\nhelp you scale and accelerate the impact of AI with trusted data across your\nbusiness. See how you can put AI to work for your business.\n\nLearn more\n\n#####  Footnotes\n\n1 Gartner Innovation Insight: Vector Databases (link resides outside ibm.com),\nrequires Gartner account), Gartner\n\n2 Gartner 2022 Strategic Roadmap for Storage (link resides outside ibm.com),\nrequires Gartner account), Gartner\n\nTop products & platforms Industries Artificial intelligence Blockchain\nBusiness operations Cloud computing Data & Analytics Hybrid cloud IT\ninfrastructure Security Supply chain Financing What is Hybrid Cloud? What is\nArtificial intelligence? What is Cloud Computing? What is Kubernetes? What are\nContainers? What is DevOps? What is Machine Learning? IBM Consulting\nCommunities Developer education Support - Download fixes, updates & drivers\nIBM Research Partner with us - Partner Plus Training - Courses Upcoming events\n& webinars Annual report Career opportunities Corporate social responsibility\nDiversity & inclusion Industry analyst reports Investor relations News &\nannouncements Thought leadership Security, privacy & trust About IBM LinkedIn\nTwitter Instagram Subscription Center United States \u2014 English Contact IBM\nPrivacy Terms of use Accessibility\n\n"
}