{
    "summary": "Open in app\n\nSign up\n\nSign in\n\nWrite\n\nSign up\n\nSign in\n\n# A guide to Data Transformation\n\n## Learn when and how to transform your variables for better insights.\n\nTim M. Schendzielorz\n\n\u00b7\n\nFollow\n\nPublished in\n\nAnalytics Vidhya\n\n\u00b7\n\n9 min read\n\n\u00b7\n\nJan 15, 2020\n\n\\--\n\n2\n\nListen\n\nShare\n\nPhoto by Arseny Togulev on Unsplash\n\n _This article was also published on_ _https://www.r-bloggers.com/_ _._\n\n _I have two series online about more data infrastructure related topics, the\nfirst one is about_ ** _building and robustly deploying a Shiny Flexdashboard\nwith Docker_** _(_ _Link to Part I_ _). The second one is about_ ** _building\nand robustly deploying an API for extracting live data from Google Big\nQuery_** _(_ _Link to Part I_ _)._\n\n# What is Data Transformation?\n\nData Transformation in a statistics context means the application of a\nmathematical expression to each point in the data. In contrast, in a Data\nEngineering context Transformation can also mean transforming data from one\nformat to another in the Extract Transform Load (ETL) process.\n\n## Why should I transform my data?\n\n  *  **Improve interpretability.** Some variables are not in the format we need for a certain question, e.g. car manufactures supply miles/gallon values for fuel consumption, however for comparing car models we are more interested in the reciprocal gallons/mile.\n  *  **De-clutter graphs.** If you visualize two or more variables that are not evenly distributed across the parameters, you end up with data points close by. For a better visualization it might be a good idea to transform the data so it is more evenly distributed across the graph. Another approach could be to use a different scale on your graph axis.\n  *  **To get insight about the relationship between variables.** The relationship between variables is often not linear but of a different type. Common example is taking the log of income to compare it to another variable as the utility of more income diminishes with higher income. (See this excellent discussion about the highly utilized log- transform on Cross Validated.) Another example is the polynomial growth of money on an bank account with interest rate compared to time. To calculate a simple correlation coefficient between variables, the variables need to show a linear relationship. To meet this criteria, you might be able to transform one or both variables.\n  *  **To meet assumptions for statistical inference.** When constructing simple confidence intervals, the assumption is that the data is normally distributed and not skewed left or right. For linear regression analysis an important assumption is homoscedasticity, meaning that the error variance of your dependent outcome variable is independent from your predictor variables. An assumption for many statistical test as the T-test is that the errors of a model ( the values of a measurement sampled from a population) are normally distributed.\n\n## Important distinctions\n\nIt is important to know what we are talking about when we use the term\ntransformation. Transformation, normalization and standardization are often\nused interchangeably and wrongly so.\n\n  *  **Normalization** is the process of scaling in respect to the entire data range so that the data has a range from 0 to 1.\n  *  **Standardization** is the process of transforming in respect to the entire data range so that the data has a mean of 0 and a standard deviation of 1. It\u2019s distribution is now a Standard Normal Distribution.\n  *  **Transformation** is the application of the same calculation to every point of the data separately.\n\nStandardization transforms the data to follow a Standard Normal Distribution\n(left graph). Normalization and Standardization can be seen as special cases\nof Transformation. To demonstrate the difference between a standard normal\ndistribution and a standard distribution we simulate data and graph it:\n\nHistogram of normal data generated with different parameters. Zoom in to the\nsee density distribution more clearly.\n\nR Code for the Plotly graphs above. Interactive Plotly graphs are embedded via\nplot.ly hosting, but could also be embedded from Github Pages via an iframe,\nsee this article for instructions.\n\n# How to transform data?\n\nTo get insights, data is most often transformed to follow close to a normal\ndistribution either to meet statistical assumptions or to detect linear\nrelationships between other variables. One of the first steps for those\ntechniques is to check how close the variables already follow a normal\ndistribution.\n\n##  **How to check if your data follows a normal distribution?**\n\nIt is common to inspect your data visually and/or check the assumption of\nnormality with a statistical test.\n\nVariable distribution histogram and corresponding QQ-plot with reference line\nof a perfect normal distribution. From UCD\n\nTo **visually explore** the distribution of your data, we will look at the\ndensity plot as well as a simple **QQ-plot**. The QQ-plot is an excellent tool\nfor inspecting various properties of your data distribution and asses if and\nhow you need to transform your data. Here the quantiles of a perfect normal\ndistribution are plotted against the quantiles of your data. Quantiles measure\nat which data point a certain percentage of the data is included. For example,\nthe data point of the 0.2 quantile is the point where 20% of the data is below\nand 80% is above. A reference line is drawn which indicates how the plot would\nlook if your variable would follow a perfect normal distribution. The closer\nyour points in the QQ-plot are to this line, the more likely it is that your\ndata follows a normal distribution and does not need additional\ntransformation.\n\nFor a **statistical analysis** of normality of your data, commonly used tests\nare the Shapiro-Wilk-Test or the Kolmogorov-Smirnov-Test. The SW Test has\ngenerally a higher detection power, the non-parametric KS Test should be used\nwith a high number of observations. Generally speaking, those tests calculate\nhow likely it is that your data distribution is similar to a normal\ndistribution (Technically, how likely it is that you do not err with H0- the\nhypothesis that the data is normally distributed). These tests however have\nthe well known problems of Frequentist Null Hypothesis Testing, which is not\nin the scope of this article to discuss, i.e. the problem of being too\nsensitive with a huge amount of observations. The KS test is generally too\nsensitive to points in the middle of the data distribution in comparison to\nthe more important tails. Additionally, those tests can not tell you how\nproblematic a non-normality would be for getting insights from your data.\nBecause of this, **I would advise to use an exploratory, visual approach to\ncheck your data distribution and forego any statistical testing** if you do\nnot need this for an automated script.\n\n## Data distributions and their corresponding QQ-plots\n\nThe following diagrams show simulated data with the density distribution and\nthe corresponding QQ-plot. Four strong and typical deviations from a normal\ndistribution are shown. Only for the normally distributed data an additional\nstatistical test for normality is shown in the code snippet for completeness.\n\nNormally distributed data and it`s QQ-plot with sample quantiles vs\ntheoretical quantiles.\n\nR Code for data generation, interactive visualization and statistical test for\nnormality\n\nRight skewed data and corresponding QQ-plot\n\nLeft skewed data and corresponding QQ-plot\n\nHeavy tailed (leptokurtic) data and corresponding QQ-plot\n\nLight tailed (platykurtic) data and corresponding QQ-plot\n\nR code for data generation for the additional plots with the libraries fGarch\nand LambertW\n\nFor playing around with distributions and their corresponding QQ-plots I can\nrecommend this nice little R **shiny app** from Cross Validated user\nZhanxiong.\n\n##  **Which transformation to pick?**\n\nIf you decide that your data should follow a normal distribution and needs\ntransformation, there are simple and highly utilized power transformations we\nwill have a look at. They transform your data to follow a normal distribution\nmore closely. It is however important to note, that **when transforming data\nyou will lose information about the data generation process and you will lose\ninterpretability of the values, too.** You might consider to back-transform\nthe variable at a certain step in your analysis. **Generally speaking, the\nexpression for transformation which matches data generation is suited best.**\nLogarithm should be used if data generation effects were multiplicative and\nthe data follows order of magnitudes. Roots should be used if the data\ngeneration involved squared effects.\n\n## Simple Transformations\n\nFor transformation multiply every data point with one of the following\nexpression. The **** expressions are sorted from weakest effect to strongest.\nIf your transformation of choice is too strong, you will end up with data\nskewed in the other direction.\n\n **Right (positive) skewed data:**\n\n  *  **Root _\u207f\u221ax._** Weakest transformation, stronger with higher order root. For negative numbers special care needs to be taken with the sign while transforming negative numbers:\n\nR code for cube root transformation\n\n  *  **Logarithm _log(x)._** Commonly used transformation, the strength of this transformation can be somewhat altered by the root of the logarithm. It can not be used on negative numbers or 0, here you need to shift the entire data by adding at least _|min(x)|+1._\n  *  **Reciprocal** **_1/x._** Strongest transformation, the transformation is stronger with higher exponents, e.g. _1/x\u00b3_. This transformation should not be done with negative numbers and numbers close to zero, hence the data should be shifted similar as the log transform.\n\n **Left (negative) skewed data**\n\n  *  **Reflect Data and use the appropriate transformation for right skew.** Reflect every data point by subtracting it from the maximum value. Add 1 to every data point to avoid having one or multiple 0 in your data.\n  *  **Square _x\u00b2._** Stronger with higher power. Can not be used with negative values.\n  *  **Exponential _e\u02e3._** Strongest transformation and can be used with negative values. Stronger with higher base.\n\n **Light & heavy tailed data**\n\n  *  **Subtract the data points from the median and transform.** Deviations of the tail from normality are usually less critical than skewness and might not need transformation after all. The subtraction from the median sets your data to a median of 0. After that use an appropriate transformation for skewed data on the absolute deviations from 0 on either side. For **heavy-tailed** data use transformations for right skew to pull in on the median and for **light-tailed** data use transformations for left skew to push data away from the median.\n\n## Automatic Transformations\n\nThere are various implementations of automatic transformations in R that\nchoose the optimal transformation expression for you. They determine a\n_lambda_ value which is the power coefficient used to transform your data\nclosest to a normal distribution.\n\n  *  **Use Lambert W x Gaussian transform.** The R package LambertW has an implementation for automatically transforming heavy or light tailed data with `Gaussianize()`.\n  *  **Tukey\u2019s Ladder of Powers.** For skewed data **,** the implementation `transformTukey()`from the R package rcompanion uses Shapiro-Wilk tests iteratively to find at which _lambda_ value the data is closest to normality and transforms it. Left skewed data should be reflected to right skew and there should be no negative values.\n\nTukey\u2019s Ladder of Powers _lamda values and corresponding power transforms.\nLambda values can be decimal._ _Source_\n\n  *  **Box-Cox Transformation.** The implementation `BoxCox.lambda()`from the R package forecast finds iteratively a _lambda_ value which maximizes the log-likelihood of a linear model. However it can be used on a single variable with model formula _x~1_. The transformation with the resulting _lambda_ value can be done via the forecast function `BoxCox()`. There is also an implementation in the R package MASS. Standard Box-Cox can not be used with negative values, two-parameter Box-Cox however can.\n\n _Source_ _GIPHY_\n\n  *  **Yeo-Johnson Transformation.** This can be seen as an useful extension to the Box-Cox. It is the same as Box-Cox for non-negative values and handles negative and 0 values as well. There are various implementations in R via packages car, VGAM and recipes in the meta machine-learning framework tidymodels.\n\n# Concluding remarks\n\nThis guide provides an overview over an important data preprocessing\ntechnique, data transformation. It demonstrates why you want to transform your\ndata during analysis. It explains how you can detect if your data needs\ntransformation to meet the most common requirement to data distribution of\nnormality and transform it accordingly. It shows which mathematical expression\nto use for transformation for stereotypical cases of non-normality and how to\nautomate this. There are a few advanced cases for transformation, e.g. for\nmultimodal distributions which is not covered here.\n\nA word of caution must be given, however. There are no definitive rules when\nand how to transform your data. It depends on how the data was generated (and\nhow much you do know about this), what insights you want to generate from it,\nhow important interpretability is and how much the data distribution deviates\nfrom your desired distribution, which will be in the majority of cases a\nnormal distribution. Hence, some **closing advice for data transformation** :\n\n  * Decide if the insights you will get from transforming are worth the downsides. E.g. decide if being able to do statistical modelling, applying a geometric technique such as k-means clustering, being able to better compare ratios or just de-clutter your graphs is worth losing direct interpretability.\n  * Decide if an alternative approach instead satisfies your analysis. **Generally speaking, finding an appropriate model to use with the raw data, e.g. quantile, spline or weighted-least-squares regression or nonparametric models for non linear relationships should be preferred instead of transformation.** Alternatively you could remove outliers, however you should remember that you need a quite good reason to delete measurements.\n  * Before and after transformation, check your distribution with a QQ-plot, even with an automatic transformation approach.\n  * Make it clear when and why you use a transformation for reproducibility.\n\n## Sign up to discover human stories that deepen your understanding of the\nworld.\n\n## Free\n\nDistraction-free reading. No ads.\n\nOrganize your knowledge with lists and highlights.\n\nTell your story. Find your audience.\n\nSign up for free\n\n## Membership\n\nAccess the best member-only stories.\n\nSupport independent authors.\n\nListen to audio narrations.\n\nRead offline.\n\nJoin the Partner Program and earn for your writing.\n\nTry for $5/month\n\nData Science\n\nData Wrangling\n\nStatistics\n\nData Analysis\n\nR\n\n\\--\n\n\\--\n\n2\n\nFollow\n\n## Written by Tim M. Schendzielorz\n\n91 Followers\n\n\u00b7Writer for\n\nAnalytics Vidhya\n\nFollow\n\nHelp\n\nStatus\n\nAbout\n\nCareers\n\nBlog\n\nPrivacy\n\nTerms\n\nText to speech\n\nTeams\n\n",
    "links": "[{\"link\": \"https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&source=post_page---two_column_layout_nav-----------------------global_nav-----------\", \"text\": \"Sign in\"}, {\"link\": \"https://medium.com/?source=---two_column_layout_nav----------------------------------\", \"text\": \"\"}, {\"link\": \"https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------\", \"text\": \"\"}, {\"link\": \"https://medium.com/search?source=---two_column_layout_nav----------------------------------\", \"text\": \"\"}, {\"link\": \"https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&source=post_page---two_column_layout_nav-----------------------global_nav-----------\", \"text\": \"Sign in\"}, {\"link\": \"https://medium.com/@tschendzie_99009?source=post_page-----9e5fa9ae1ca3--------------------------------\", \"text\": \"\"}, {\"link\": \"https://medium.com/@tschendzie_99009?source=post_page-----9e5fa9ae1ca3--------------------------------\", \"text\": \"Tim M. Schendzielorz\"}, {\"link\": \"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7056b9dfb238&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&user=Tim+M.+Schendzielorz&userId=7056b9dfb238&source=post_page-7056b9dfb238----9e5fa9ae1ca3---------------------post_header-----------\", \"text\": \"Follow\"}, {\"link\": \"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fanalytics-vidhya%2F9e5fa9ae1ca3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&user=Tim+M.+Schendzielorz&userId=7056b9dfb238&source=-----9e5fa9ae1ca3---------------------clap_footer-----------\", \"text\": \"\"}, {\"link\": \"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9e5fa9ae1ca3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&source=-----9e5fa9ae1ca3---------------------bookmark_footer-----------\", \"text\": \"\"}, {\"link\": \"https://medium.com/analytics-vidhya/shiny-dashboards-with-flexdashboard-e66aaafac1f2\", \"text\": \"Link to Part I\"}, {\"link\": \"https://medium.com/analytics-vidhya/google-big-query-with-r-875facef7844\", \"text\": \"Link to Part I\"}, {\"link\": \"https://medium.com/tag/data-science?source=post_page-----9e5fa9ae1ca3---------------data_science-----------------\", \"text\": \"Data Science\"}, {\"link\": \"https://medium.com/tag/data-wrangling?source=post_page-----9e5fa9ae1ca3---------------data_wrangling-----------------\", \"text\": \"Data Wrangling\"}, {\"link\": \"https://medium.com/tag/statistics?source=post_page-----9e5fa9ae1ca3---------------statistics-----------------\", \"text\": \"Statistics\"}, {\"link\": \"https://medium.com/tag/data-analysis?source=post_page-----9e5fa9ae1ca3---------------data_analysis-----------------\", \"text\": \"Data Analysis\"}, {\"link\": \"https://medium.com/tag/r?source=post_page-----9e5fa9ae1ca3---------------r-----------------\", \"text\": \"R\"}, {\"link\": \"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fanalytics-vidhya%2F9e5fa9ae1ca3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&user=Tim+M.+Schendzielorz&userId=7056b9dfb238&source=-----9e5fa9ae1ca3---------------------clap_footer-----------\", \"text\": \"\"}, {\"link\": \"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fanalytics-vidhya%2F9e5fa9ae1ca3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&user=Tim+M.+Schendzielorz&userId=7056b9dfb238&source=-----9e5fa9ae1ca3---------------------clap_footer-----------\", \"text\": \"\"}, {\"link\": \"https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9e5fa9ae1ca3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&source=--------------------------bookmark_footer-----------\", \"text\": \"\"}, {\"link\": \"https://medium.com/@tschendzie_99009?source=post_page-----9e5fa9ae1ca3--------------------------------\", \"text\": \"\"}, {\"link\": \"https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdab7cb7dde4a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&newsletterV3=7056b9dfb238&newsletterV3Id=dab7cb7dde4a&user=Tim+M.+Schendzielorz&userId=7056b9dfb238&source=-----9e5fa9ae1ca3---------------------subscribe_user-----------\", \"text\": \"\"}, {\"link\": \"https://medium.com/@tschendzie_99009?source=post_page-----9e5fa9ae1ca3--------------------------------\", \"text\": \"Written by Tim M. Schendzielorz\"}, {\"link\": \"https://medium.com/@tschendzie_99009/followers?source=post_page-----9e5fa9ae1ca3--------------------------------\", \"text\": \"91 Followers\"}, {\"link\": \"https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdab7cb7dde4a&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fa-guide-to-data-transformation-9e5fa9ae1ca3&newsletterV3=7056b9dfb238&newsletterV3Id=dab7cb7dde4a&user=Tim+M.+Schendzielorz&userId=7056b9dfb238&source=-----9e5fa9ae1ca3---------------------subscribe_user-----------\", \"text\": \"\"}, {\"link\": \"https://medium.com/about?autoplay=1&source=post_page-----9e5fa9ae1ca3--------------------------------\", \"text\": \"About\"}, {\"link\": \"https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----9e5fa9ae1ca3--------------------------------\", \"text\": \"Careers\"}, {\"link\": \"https://medium.com/business?source=post_page-----9e5fa9ae1ca3--------------------------------\", \"text\": \"Teams\"}]",
    "priceAndPlans": "Error: Timeout 30000ms exceeded. =========================== logs\n=========================== navigating to\n\"http://web.archive.org/web/20240113194507/https://medium.com/\", waiting until\n\"load\" ============================================================\n\n"
}