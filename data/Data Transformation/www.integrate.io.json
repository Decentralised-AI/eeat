{
    "summary": "  *   * Platform \n\nETL & Reverse ETL\n\nDatabase Replication (CDC)\n\nAPI Generation\n\nData Observability\n\n  * Our Superpowers \n  * Connectors\n  * Pricing \n  * More \n\nBlog\n\nCompare Us\n\nSupport & Resources\n\nSecurity\n\nCase Studies\n\nWhite Papers\n\nWebinars\n\nDocumentation\n\nAbout\n\n  * Sign In\n  * GET A DEMO\n\n  * (888) 884 6405\n  * Sign In\n  * GET A DEMO\n\n# Data Transformation:  \nExplained\n\nBy Integrate.io\n\nOct 19, 2023\n\nRaw data is a precious resource for modern businesses. However, before you can\nbenefit from raw data, the process of data transformation is necessary.\n\nData transformation is the process of extracting data, sifting through it,\nunderstanding it, and then transforming it into something you can analyze.\nThat\u2019s where ETL (extract, transform, load) pipelines come into play. As a\nvital stage of the ETL process, data transformation is necessary to change the\ninformation into a format or schema that a business intelligence platform can\ninteract with to derive actionable insights.\n\nGood data management can effectively transform a struggling business into a\nsuccessful one. In our marketplace, good data powers create dynamic business\nanalysis, which in turn promotes business agility. Essentially, good data\nboosts supply chain efficiency and customer satisfaction.\n\nThe opposite, bad data, keeps CTOs, BI professionals, and developers up at\nnight. Bad data is data that is suspect, missing, or flawed. It's also very\ndangerous. In fact, bad data can lead to misguided decision-making and lost\nprofits. According to Gartner, poor data quality costs companies millions in\nrevenues annually - dollars and cents proof of the importance of data\ntransformation.\n\nHowever, the insights value chain is only as strong as its weakest data link.\nAccording to KPMG, 71% of CEOs say they have disregarded insights from\nunreliable data. Unsurprisingly, BI professionals spend 80% of their time\npreparing (cleaning and transforming) data before engaging in analysis.\n\nHowever, as essential as the data transformation process is, only data\nengineers and data scientists tend to understand it. Read on to learn more\nabout data transformation, different types, and common challenges that come\nalong with data transformation.\n\n**[Key Takeaways] Here are the key things you need to know about Data\nTransformation:**\n\nData transformation is the linchpin of effective data analytics, converting\nraw data into usable formatting.  \nVarious types of transformations cater to different data needs and scenarios.  \nChallenges in data transformation range from maintaining data integrity to\nensuring scalability.  \nMany tools exist in the market, each offering unique features to streamline\nthe transformation process.  \nIntegrate.io stands out with its robust, cloud-native solutions tailored for\nmodern data transformation needs.\n\n### Table of Contents\n\n  * What is Data Transformation?\n  * Data Transformation Types\n  * The Top 4 Data Transformation Challenges\n  * The Best Tools to Help With Data Transformation Needs\n  * How Integrate.io Can Help\n\n### The Unified Stack for Modern Data Teams\n\n##### Get a personalized platform demo & 30-minute Q&A session with a Solution\nEngineer\n\nGET A LIVE DEMO\n\n## What is Data Transformation?\n\nRaw or source data is often:\n\n  * **Inconsistent:** It uses both relevant and irrelevant data.\n\n  * **Imprecise** : It contains incorrectly entered information or missing values.\n\n  * **Repetitive** : It has duplicate data.\n\nData transformation is the process of extracting good, reliable data from\nthese sources. This involves converting data from one structure (or no\nstructure) to another so you can integrate it with a data warehouse or with\ndifferent applications. It allows you to expose the information to advanced\nbusiness intelligence tools to create valuable performance reports and\nforecast future trends.\n\nData transformation includes three primary stages: understanding, mapping, and\ntransforming the data.\n\n**Related Reading:** ******Data Engineering: What is a Data Engineer and How\nDo I Become One?**\n\n### Stage 1: Understanding and Mapping the Data\n\nYou\u2019ll identify and study your data sources during the first data\ntransformation stage. Examples of different sources include:\n\n  * Databases attached to different business systems\n\n  * Accounting software\n\n  * CRM platforms\n\n  * Streaming sources\n\n  * Customer log files\n\n  * Web application data\n\n  * Information related to mobile app usage statistics\n\nOnce you identify your data sources, you\u2019ll determine what kind of data\nstructures each has and the types of transformations required to connect them.\nSome questions you might want to ask are:\n\n  * What do the columns and rows look like?\n\n  * How are they labeled?\n\n  * What kind of information do they contain?\n\n  * How does the information in one data source relate to another source? \n\nNext, you'll perform data mapping to define how the fields in different data\nsources connect and what types of data transformations they require.\n\n### Stage 2: Transforming the Data\n\nDuring the second data transformation stage, you will carry out the different\ndata transformations you mapped in the first stage. There are several\nstrategies for doing this:\n\n  * **Hand-Coding ETL Solutions** : Traditionally, you would set up your ETL process through scripting by hand-writing code in SQL or Python. This was a task you'd give to offsite developers, and it was a time-consuming process. Also, because offsite developers had to interpret your requirements, the process often resulted in misunderstandings and unintentional errors.\n\n  * **Onsite Server-Based ETL Solutions** : Onsite ETL solutions work through your onsite servers to extract, transform, and load information into an onsite data warehouse. Although most companies are now moving to cloud-based data warehousing and ETL solutions, onsite ETL still has its place. Compared to offsite scripting solutions, onsite ETL offers the benefit of more oversight by the end-user. However, you may need to hire expert staff to manage it. \n\n  * **Cloud-Based ETL Solutions** : Cloud-based ETL solutions are the future of data transformation. They work through the cloud rather than on an onsite server. These solutions are beneficial when linking cloud-based software as a service (SaaS) platforms like Salesforce to a cloud-based data warehouse like Amazon Redshift. They can also help you integrate an onsite business system into a cloud-based data warehouse. Many feature drag-and-drop graphical interfaces that make it easy for end-users (with no expertise in data science) to manipulate and control their data transformations. In addition, they offer automatic integrations that perform the transformations your data requires.\n\n##\n\n## Data Transformations Types\n\nBelow, we've listed the types of transformations that you, your ETL platform,\nor your data team may need to perform during the ETL process. Although most of\nthese tasks can happen automatically with a data transformation platform,\nsometimes you may need to set up and code ETL processes yourself.\n\n  * Bucketing/Binning\n\n  * Data Aggregation\n\n  * Data Cleansing\n\n  * Data Deduplication\n\n  * Data Derivation\n\n  * Data Filtering\n\n  * Data Integration\n\n  * Data Joining\n\n  * Data Splitting\n\n  * Data Summarization\n\n  * Data Validation\n\n  * Format Revision\n\n  * Key Restructuring\n\n  * Z-Score Normalization and Max-Min Scaling\n\n**Related Reading:** ******ETL & Data Warehousing Explained: ETL Tool Basics**\n\n**Bucketing/Binning**\n\nBucketing or binning gets used to change a numeric series into fixed,\ncategorical ranges, say, from {2,5,8\u2026} to {2-5, 6-9, 10-13\u2026}. Take, for\nexample, the seasonal fluctuations in consumer prices. Bucketing/binning lets\nus isolate noisy data and look at long-term averages. Focusing away from\nshort-term volatility provides a more accurate picture of price trends.\n\n**Data Aggregation**\n\nData aggregation is a process that searches, gathers, summarizes, and presents\ndata in different reports. For example, you have a list of male and female\nemployees and their salaries, and you want to know the total male wages\ncompared to the total female salaries. You can aggregate the list by male and\nfemale, then sum up the total wages for each group.\n\nYou can use a business intelligence platform to perform data aggregations\nbased on the insights decision-makers need, or you can perform manual\naggregations by coding in SQL.\n\n**Data Cleansing**\n\nData cleansing involves deleting out-of-date, inaccurate, or incomplete\ninformation to increase data accuracy. Also referred to as data scrubbing and\ndata cleaning, data cleansing relies on carefully analyzing datasets and data\nstorage protocols to support the most accurate data possible.\n\nThe process might include parsing data to remove syntax errors, deleting\nrecord fragments, and correcting typos. It could also involve fixing\nduplication problems that result from merging multiple datasets. The ultimate\ngoal of data cleansing is to ensure that any data you work with is as accurate\nas possible and meets the highest standard of quality.\n\n**Data Deduplication**\n\nData deduplication is a data compression process where you identify and remove\nduplicate or repeated copies of information. Also referred to as single-\ninstance storage, intelligent compression, commonality factoring, or data\nreduction, deduplication allows you to store one unique copy of data in your\ndata warehouse or database.\n\nThe deduplication process analyzes incoming data and compares it to data\nalready stored in the system. If the data is already there, deduplication\nalgorithms delete the duplicate information while creating a reference to it.\nIf you upload a changed version of a previous file, the system will back up\nthe said file while adding the changes to the data segment. Deduplication\nalgorithms also keep track of outgoing data to delete duplicates, which speeds\nup the information transfer process.\n\n**Data Derivation**\n\nData derivation involves the creation of special rules to \u201cderive\u201d the\nspecific information you want from the data source. For example, you might\nhave a database that includes total revenue data from sales, but you\u2019re only\ninterested in loading the profit figures _after_ subtracting costs and tax\nliabilities. Data derivation allows you to create transformation rules that\ndeduct expenses and taxes from the total revenue information.\n\n**Data Filtering  **\n\nData filtering includes techniques used to refine datasets. Data filtering\naims to distill a data source to only what the user needs by eliminating\nrepeated, irrelevant, or overly sensitive data. Data filters can be used to\namend query results and reports.\n\nIn its most practical form, data filtering simply involves the selection of\nspecific rows, columns, or fields to display from the dataset. For example, if\nthe end-user doesn\u2019t need to see each client's addresses or Social Security\nnumbers in the report, data filtering will scrub them from the report.\n\n**Data Integration**\n\nData integration is the process of taking different data types (like different\ndatabases and datasets relating to sales, marketing, and operations) and\nmerging them into the same structure or schema. As a primary goal of ETL for\ndata warehousing purposes, data integration supports the analysis of massive\ndata sets by merging multiple data sources into an easy-to-analyze whole.\n\nAt its core, data integration reconciles differing values and names that refer\nto the same data elements within the data warehouse. By giving each element a\nstandard name and definition, data integration makes it possible to analyze\nthe information with a business intelligence platform.\n\n**Data Joining**\n\nJoining data is one of the most important functions of data transformation. A\n\u201cjoin\u201d is an operation in the SQL database language allows you to connect two\nor more database tables by their matching columns. This enables you to\nestablish a relationship between multiple tables, which merges table data so\nyou can query correlating data on the tables.\n\n**Data Splitting**\n\nData splitting refers to dividing a single column into multiple columns. This\nis critical for analyzing the available data; splitting the single column into\nmultiple columns can be useful to develop \"training\" and \"testing\" sets, for\nexample. The \"training\" gets used for experimental analysis and making models,\nwhile the \"testing\" set is the untouched \"control\" element. Data splitting can\nbe helpful with a large amount of data gathered over a significant amount of\ntime.\n\n**Data Summarization**\n\nData summarization is similar to data aggregation. It refers to the creation\nof different business metrics through the calculation of value totals. You\ncould sum up the total revenue of all the sales made by the individual\nsalespeople on your staff, then create sales metrics that reveal total sales\nfor particular periods.\n\n**Data Validation**\n\nData validation is creating automated rules or algorithms that engage when the\nsystem encounters different data issues. Data validation helps ensure the\naccuracy and quality of the data you transform. For example, a rule could go\ninto effect when the system finds that the first three fields in a row are\nempty (or NULL value). The rule might flag the row for the end-user to\ninvestigate later or stop the system from processing the row altogether.\n\n**Format Revision**\n\nFormat revisions fix problems that stem from fields having different data\ntypes. Some fields might be numeric, and others might be text. One data system\ncould treat text versus numeric information differently, so you might have to\nstandardize the formats to integrate source data with the target data schema.\nThis could involve the conversion of male/female, date/time, measurements, and\nother information into a consistent format.\n\nField lengths can also be an issue\u2014especially if the target schema has smaller\ncharacter limits. In these cases, it may be necessary to standardize the\nlength of fields by breaking up long serial numbers into smaller parts and\nputting them into separate columns.\n\nAdditionally, format revision could involve splitting up a comma-separated\nlist of words or numbers into multiple columns.\n\n**Key Restructuring**\n\nWhen the tables in a data warehouse have keys with built-in meanings, serious\nproblems can develop. For example, if a client's phone number serves as a\nprimary key, changing the phone number in the original data source means the\nnumber would have to change everywhere it appears in the data system. That\nwould cause a cascade of updates that over-burden or slow down the system.\n\nThrough key restructuring, you can transform any keys with built-in meanings\nto generic keys\u2014i.e., random numbers that reference back to the source\ndatabase with the actual information. By drawing key connections from one\ntable to another, key restructuring optimizes the data warehouse for speed and\nefficiency.\n\n**Z-Score Normalization and Max-Min Scaling**\n\nScaling changes our data ranges, but in z-score normalization, individual data\nfeatures have zero-min and unit variance. So, all values will be between 0 and\n1. Scaling is especially important because datasets often contain elements in\nvarying units and ranges. This is incompatible with many machine learning\nalgorithms that use Euclidean metric measurements.\n\n**Related Reading:** ******What is a Data Warehouse and Why Are They\nImportant?**\n\n****\n\n## The Top 4 Data Transformation Challenges\n\nAccording to a recent survey, companies are falling behind in their data-\ndriven goals: 72% of survey participants have yet to forge an internal data\nculture. In comparison, 52% say they have not leveraged data and analytics to\nremain competitive.\n\nWhy are companies failing to meet their goals? There are a few possibilities:\n\n  1. **The talent gap may be insurmountable.** Depending on your infrastructure, transforming your data may require a team of experts and substantial investment in on-premise infrastructure. New tools have evolved to optimize the process of data transformation. However, wielding big data technologies successfully requires both knowledge and talent.\n\n  2. **The process of preparing and migrating data is complex and time-consuming.** Data scientists and BI professionals maintain that data preparation (prior to transformation) takes up more than two-thirds of their time. According to a Crowdflower report, data scientists spend 51% of their time compiling, cleaning, and organizing data. They also spend 30% of their time collecting datasets and mining data to identify patterns.\n\n  3. **Without the proper tools, data transformation is daunting for the uninitiated.** Ideally, data discovery and mapping must occur before transformations can commence. Without a good roadmap, the already daunting data transformation task is made more challenging. However, roadmap and workflow creation may be impossible without the proper tools and expertise.\n\n  4. **Developing a sustainable, fault-tolerant data pipeline often requires consensus building.** Building an efficient data pipeline for many organizations involves extensive buy-in from critical stakeholders. Consensus on the data collection and transformation process must often precede the building of a pipeline. This is easier said than done.\n\nAdditionally, the pipeline must easily accommodate changes to support\nscalability and functionality. The path to ETL hell is broad and wide,\nespecially if there isn't an efficient mechanism to support schema evolution.\n\n**Related Reading:** ******Top Challenges of Data Migration**\n\n****\n\n## The Best Tools to Help With Data Transformation Needs\n\nWith data being such a valuable resource to today's businesses, you must be\nable to seamlessly transform data to meet all your business needs and\nexpectations. Data transformation tools play the role of \u201cT\u201d in ETL. Of\ncourse, ETL stands for Extract, Transform, Load. The overall process of ETL\nlooks something like this:\n\n  * **Extract** : Data gets extracted from one or more specified locations. \n\n  * **Transform:** The extracted data is transformed and prepared to be readable for its next intended destination. \n\n  * **Load:** After transforming the data, it is sent and loaded into its new data warehouse. Once in the warehouse, the data can be used for all analytic and reporting purposes.\n\n**Related Reading:** ******In-House ETL vs Integrate.io: Comparison &\nOverview**\n\nSome of the top platforms offering data transformation tools in 2023 include:\n\n  * **Integrate.** **io:** Leading the pack is Integrate.io, a comprehensive data integration platform designed for both technical and non-technical users. What sets Integrate.io apart is its intuitive visual interface, allowing users to design and execute complex data workflows quickly using no-code data pipelines.\n\nKey Feature: Integrate.io boasts a rich library of pre-built integrations,\nenabling seamless connectivity with a wide array of data sources and\ndestinations. This ensures businesses can quickly set up and automate their\ndata pipelines, reducing the time-to-insight.\n\n  * **Airflow:** an open-source platform, has gained traction for its flexibility and scalability. Developed by Airbnb, it's designed to design, schedule, and monitor data workflows.\n\nKey Feature: Airflow's dynamic workflow creation capability stands out. It\nallows users to define workflows as code, ensuring flexibility and version\ncontrol.\n\n  * **EasyMorph:** designed to focus on self-service, enabling users without technical expertise to transform and automate their data tasks.\n\nKey Feature: Its interactive interface allows users to design transformations\nvisually. The transformations are applied in real-time, offering immediate\nfeedback and reducing iteration time.\n\n  * **Dataform:** brings structure and order to data transformation in big data warehouses. It's particularly favored by teams looking to collaborate on large datasets.\n\nKey Feature: Dataform's centralized data modeling is noteworthy. It allows\nteams to define standardized datasets, ensuring consistency and reliability\nacross the board.\n\n  * **Matillion:** a cloud-native data transformation tool explicitly designed for modern cloud data warehouses like Snowflake, Google BigQuery, and Amazon Redshift.\n\nKey Feature: Matillion's standout feature is its push-down processing\napproach. Instead of extracting data, it brings transformation logic to the\ndata source, ensuring efficient processing and reduced data movement.\n\n  * **RudderStack:** positions itself as a customer data platform, focusing on routing data to various destinations after transformation.\n\nKey Feature: Its ability to build and activate audiences in real-time is a\ngame-changer. RudderStack allows businesses to send transformed data to\nmarketing and analytics tools quickly.\n\n  * **Trifacta:** known for its data wrangling capabilities, turning messy data into structured, usable datasets.\n\nKey Feature: Trifacta's predictive transformation is a standout. It suggests\npotential transformations based on the data's patterns, significantly\nsimplifying the wrangling process.\n\nWhile all the tools mentioned above have unique strengths, Integrate.io offers\na versatile choice, catering to a broad spectrum of data transformation needs.\nHowever, the best tool often depends on specific business requirements, data\nvolumes, and technical expertise. When looking to make data-driven decisions,\ninvesting in a robust data transformation tool is not just a choice; it's a\nnecessity.\n\n## How Integrate.io Can Help\n\nTo code or not to code, that is the question. With Integrate.io, you don't\nneed to grapple with Pig, SQL, or Java code to fix bugs. Our cloud-based ETL\nplatform allows you to execute basic and advanced transformations efficiently.\n\nIntegrate.io facilitates agility; with the platform, you can integrate\nmultiple data sources and retrieve insights from your data in real-time. You\ncan use reliable data to optimize your algorithms and achieve business\nagility. Another benefit? The right ETL platform can save you money on OpEx\nand CapEx costs **.** Integrate.io's solution is cloud-based, so you don't\nneed to rely on IT talent to maintain expensive infrastructure.\n\nFinally, Integrate.io provides network, system, and physical security and is\nSOC2 compliant. Our physical infrastructure utilizes AWS technology and has\naccreditations for ISO 27001, Sarbanes-Oxley, PCI Level 1, SOC 1, and SOC\n2/SSAE 16/ISAE 3402. The platform also complies with the dictates of\ninternational privacy laws.\n\nAre you curious how Integrate.io can help with all your data transformation\nneeds? Contact our team today to schedule a 14-day demo or pilot and see how\nwe can help you reach your goals.\n\n### The Unified Stack for Modern Data Teams\n\n##### Get a personalized platform demo & 30-minute Q&A session with a Solution\nEngineer\n\nGET A LIVE DEMO\n\n__Big Data\n\n__Tags: data transformation,  etl\n\nShare This Blog Post\n\n####  Related Readings\n\n#### Why Data Literacy is Essential for a Data-Driven Future\n\nJanuary 09, 2024\n\nby Donal Tobin\n\nIntroduction The importance of data literacy transcends industries and roles,\nmaking it a non-negotiable skill in many sectors, including healthcare,\nbusiness intelligence, and technology. In healthcare, for instance, not\nonly...\n\n#### The Top 6 BI Visualization Tools\n\nJanuary 03, 2024\n\nby Terence Bennett\n\nExplore the top BI visualization tools for your business. Compare features,\nease of use, and find the right fit for your data needs!\n\n#### Data Ingestion Framework Guide\n\nJanuary 02, 2024\n\nby Integrate.io\n\nExplore the essentials of data ingestion, its challenges, & top tools for\nefficient data pipeline management in 2024\n\n### Subscribe To  \nThe Stack Newsletter\n\nhello@integrate.io  \n+1-888-884-6405\n\n__ __ __ __\n\n\u00a92023 Integrate.io\n\nSIGN UP FOR \u201cTHE STACK\u201d - OUR MONTHLY NEWSLETTER\n\n##### Solutions\n\n  * Solutions Home\n  * Connectors\n  * Marketing\n  * Sales\n  * Support\n  * Developers\n  * Release Notes\n\n##### Support\n\n  * Blog\n  * Live Chat\n  * Support & Resources\n  * Developers\n  * Documentation\n  * Documentation API\n  * Security\n  * Service Status\n  * Privacy Policy\n  * Terms of Service\n  * Glossary\n\n##### Company\n\n  * Customers\n  * White Papers\n  * Webinars\n  * About\n  * Partners\n  * Join Us\n\n##### Language\n\n  * English\n  * \u65e5\u672c\u8a9e\n  * \ud55c\uad6d\uc5b4\n\n\u00a92023 Integrate.io\n\nx\n\n#### Get the Integrate.io Newsletter\n\nSubscribe\n\nDon\u2019t worry, we hate spam as much as you do.\n\nI'm already subscribed. No thanks.\n\n\u00d7\n\n## Choose Your Free Trial\n\n##### ETL & Reverse ETL\n\n###### Formerly Xplenty\n\nLow-code ETL with 220+ data transformations to prepare your data for insights\nand reporting.\n\nStart My Trial>>ETL\n\n##### ELT & CDC\n\n###### Formerly FlyData\n\nReplicate data to your warehouses giving you real-time access to all of your\ncritical data.\n\nStart My Trial>> CDC\n\n##### API Generation\n\n###### Formerly DreamFactory\n\nGenerate a REST API on any data source in seconds to power data products.\n\nStart My Trial>> DF\n\nGet Help Choosing The Right Trial\n\n",
    "links": "[{\"link\": \"https://www.integrate.io/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/product/etl/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/product/cdc/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/product/api-services/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/product/data-observability/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/solutions/superpowers/\", \"text\": \"Our Superpowers\\n        \"}, {\"link\": \"https://www.integrate.io/integrations/\", \"text\": \"Connectors\"}, {\"link\": \"https://www.integrate.io/pricing/\", \"text\": \"Pricing\\n      \"}, {\"link\": \"https://www.integrate.io/blog/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/compare/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/support/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/security/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/customers/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/books-and-guides/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/webinars/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/support/#documentation\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/about/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/demo/\", \"text\": \"GET A DEMO\"}, {\"link\": \"https://www.integrate.io/demo/\", \"text\": \"GET A DEMO\"}, {\"link\": \"https://www.integrate.io/solutions/\", \"text\": \"Solutions Home\"}, {\"link\": \"https://www.integrate.io/integrations/\", \"text\": \"Connectors\"}, {\"link\": \"https://www.integrate.io/solutions/media-and-entertainment/\", \"text\": \"Marketing\"}, {\"link\": \"https://www.integrate.io/solutions/business-intelligence/\", \"text\": \"Sales\"}, {\"link\": \"https://www.integrate.io/solutions/customer-360/\", \"text\": \"Support\"}, {\"link\": \"https://www.integrate.io/solutions/engineering-team/\", \"text\": \"Developers\"}, {\"link\": \"https://www.integrate.io/product/changelog/\", \"text\": \"Release Notes\"}, {\"link\": \"https://www.integrate.io/blog/\", \"text\": \"Blog\"}, {\"link\": \"https://www.integrate.io/support/\", \"text\": \"Support & Resources\"}, {\"link\": \"https://www.integrate.io/support/#documentation\", \"text\": \"Documentation\"}, {\"link\": \"https://www.integrate.io/docs/api/\", \"text\": \"Documentation API\"}, {\"link\": \"https://www.integrate.io/security/\", \"text\": \"Security\"}, {\"link\": \"https://www.integrate.io/privacy/\", \"text\": \"Privacy Policy\"}, {\"link\": \"https://www.integrate.io/tos/\", \"text\": \"Terms of Service\"}, {\"link\": \"https://www.integrate.io/glossary/\", \"text\": \"Glossary\"}, {\"link\": \"https://www.integrate.io/customers/\", \"text\": \"Customers\"}, {\"link\": \"https://www.integrate.io/books-and-guides/\", \"text\": \"White Papers\"}, {\"link\": \"https://www.integrate.io/webinars/\", \"text\": \"Webinars\"}, {\"link\": \"https://www.integrate.io/about/\", \"text\": \"About\"}, {\"link\": \"https://www.integrate.io/partners/\", \"text\": \"Partners\"}, {\"link\": \"https://www.integrate.io/careers/\", \"text\": \"Join Us\"}, {\"link\": \"https://www.integrate.io/blog/\", \"text\": \"English\"}, {\"link\": \"https://www.integrate.io/jp/blog/\", \"text\": \"\u65e5\u672c\u8a9e\"}, {\"link\": \"https://www.integrate.io/ko/blog/\", \"text\": \"\ud55c\uad6d\uc5b4\"}, {\"link\": \"https://www.integrate.io/etltrial/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/elttrial/\", \"text\": \"\"}, {\"link\": \"https://www.integrate.io/demo/\", \"text\": \"\\n          Get Help Choosing The Right Trial\\n        \"}]"
}