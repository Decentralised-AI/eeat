{
    "summary": "\u2261\n\n#  Stephen Wolfram\n\nWritings\n\n  *     * ABOUT\n    * WRITINGS\n    * PUBLICATIONS\n    * MEDIA\n    * SCRAPBOOK\n    * CONTACT\n  *     *     *     *     *     * \n\nRecent |\n\nCategories\n\n  * Artificial Intelligence\n  * Big Picture\n  * Companies & Business\n  * Computational Science\n  * Computational Thinking\n  * Data Science\n  * Education\n  * Future Perspectives\n  * Historical Perspectives\n  * Language & Communication\n  * Life & Times\n  * Life Science\n  * Mathematica\n  * Mathematics\n  * New Kind of Science\n  * New Technology\n  * Personal Analytics\n  * Philosophy\n  * Physics\n  * Ruliology\n  * Software Design\n  * Wolfram|Alpha\n  * Wolfram Language\n  * Other\n\n| \u00d7\n\nContents\n\n  * Top\n  * It's Just Adding One Word at a Time\n  * Where Do the Probabilities Come From?\n  * What Is a Model?\n  * Models for Human-Like Tasks\n  * Neural Nets\n  * Machine Learning, and the Training of Neural Nets\n  * The Practice and Lore of Neural Net Training\n  * \"Surely a Network That's Big Enough Can Do Anything!\" \n  * The Concept of Embeddings\n  * Inside ChatGPT\n  * The Training of ChatGPT\n  * Beyond Basic Training\n  * What Really Lets ChatGPT Work?\n  * Meaning Space and Semantic Laws of Motion\n  * Semantic Grammar and the Power of Computational Language\n  * So ... What Is ChatGPT Doing, and Why Does It Work?\n  * Thanks\n  * Additional Resources\n\nWhat Is ChatGPT Doing \u2026 and Why Does It Work?\n\nORDER ONLINE\n\nSee also:  \n**_\u201cLLM Tech Comes to Wolfram Language\u201d_  \u00bb**  \n ** _A discussion about the history of neural nets_  \u00bb**\n\n# What Is ChatGPT Doing \u2026 and Why Does It Work?\n\nFebruary 14, 2023\n\n## It\u2019s Just Adding One Word at a Time\n\nThat ChatGPT can automatically generate something that reads even\nsuperficially like human-written text is remarkable, and unexpected. But how\ndoes it do it? And why does it work? My purpose here is to give a rough\noutline of what\u2019s going on inside ChatGPT\u2014and then to explore why it is that\nit can do so well in producing what we might consider to be meaningful text. I\nshould say at the outset that I\u2019m going to focus on the big picture of what\u2019s\ngoing on\u2014and while I\u2019ll mention some engineering details, I won\u2019t get deeply\ninto them. (And the essence of what I\u2019ll say applies just as well to other\ncurrent \u201clarge language models\u201d [LLMs] as to ChatGPT.)\n\nThe first thing to explain is that what ChatGPT is always fundamentally trying\nto do is to produce a \u201creasonable continuation\u201d of whatever text it\u2019s got so\nfar, where by \u201creasonable\u201d we mean \u201cwhat one might expect someone to write\nafter seeing what people have written on billions of webpages, etc.\u201d\n\nSo let\u2019s say we\u2019ve got the text \u201c _The best thing about AI is its ability to_\n\u201d. Imagine scanning billions of pages of human-written text (say on the web\nand in digitized books) and finding all instances of this text\u2014then seeing\nwhat word comes next what fraction of the time. ChatGPT effectively does\nsomething like this, except that (as I\u2019ll explain) it doesn\u2019t look at literal\ntext; it looks for things that in a certain sense \u201cmatch in meaning\u201d. But the\nend result is that it produces a ranked list of words that might follow,\ntogether with \u201cprobabilities\u201d:\n\nAnd the remarkable thing is that when ChatGPT does something like write an\nessay what it\u2019s essentially doing is just asking over and over again \u201cgiven\nthe text so far, what should the next word be?\u201d\u2014and each time adding a word.\n(More precisely, as I\u2019ll explain, it\u2019s adding a \u201ctoken\u201d, which could be just a\npart of a word, which is why it can sometimes \u201cmake up new words\u201d.)\n\nBut, OK, at each step it gets a list of words with probabilities. But which\none should it actually pick to add to the essay (or whatever) that it\u2019s\nwriting? One might think it should be the \u201chighest-ranked\u201d word (i.e. the one\nto which the highest \u201cprobability\u201d was assigned). But this is where a bit of\nvoodoo begins to creep in. Because for some reason\u2014that maybe one day we\u2019ll\nhave a scientific-style understanding of\u2014if we always pick the highest-ranked\nword, we\u2019ll typically get a very \u201cflat\u201d essay, that never seems to \u201cshow any\ncreativity\u201d (and even sometimes repeats word for word). But if sometimes (at\nrandom) we pick lower-ranked words, we get a \u201cmore interesting\u201d essay.\n\nThe fact that there\u2019s randomness here means that if we use the same prompt\nmultiple times, we\u2019re likely to get different essays each time. And, in\nkeeping with the idea of voodoo, there\u2019s a particular so-called \u201ctemperature\u201d\nparameter that determines how often lower-ranked words will be used, and for\nessay generation, it turns out that a \u201ctemperature\u201d of 0.8 seems best. (It\u2019s\nworth emphasizing that there\u2019s no \u201ctheory\u201d being used here; it\u2019s just a matter\nof what\u2019s been found to work in practice. And for example the concept of\n\u201ctemperature\u201d is there because exponential distributions familiar from\nstatistical physics happen to be being used, but there\u2019s no \u201cphysical\u201d\nconnection\u2014at least so far as we know.)\n\nBefore we go on I should explain that for purposes of exposition I\u2019m mostly\nnot going to use the full system that\u2019s in ChatGPT; instead I\u2019ll usually work\nwith a simpler GPT-2 system, which has the nice feature that it\u2019s small enough\nto be able to run on a standard desktop computer. And so for essentially\neverything I show I\u2019ll be able to include explicit Wolfram Language code that\nyou can immediately run on your computer. (Click any picture here to copy the\ncode behind it.)\n\nFor example, here\u2019s how to get the table of probabilities above. First, we\nhave to retrieve the underlying \u201clanguage model\u201d neural net:\n\nLater on, we\u2019ll look inside this neural net, and talk about how it works. But\nfor now we can just apply this \u201cnet model\u201d as a black box to our text so far,\nand ask for the top 5 words by probability that the model says should follow:\n\nThis takes that result and makes it into an explicit formatted \u201cdataset\u201d:\n\nHere\u2019s what happens if one repeatedly \u201capplies the model\u201d\u2014at each step adding\nthe word that has the top probability (specified in this code as the\n\u201cdecision\u201d from the model):\n\nWhat happens if one goes on longer? In this (\u201czero temperature\u201d) case what\ncomes out soon gets rather confused and repetitive:\n\nBut what if instead of always picking the \u201ctop\u201d word one sometimes randomly\npicks \u201cnon-top\u201d words (with the \u201crandomness\u201d corresponding to \u201ctemperature\u201d\n0.8)? Again one can build up text:\n\nAnd every time one does this, different random choices will be made, and the\ntext will be different\u2014as in these 5 examples:\n\nIt\u2019s worth pointing out that even at the first step there are a lot of\npossible \u201cnext words\u201d to choose from (at temperature 0.8), though their\nprobabilities fall off quite quickly (and, yes, the straight line on this log-\nlog plot corresponds to an _n_ \u20131 \u201cpower-law\u201d decay that\u2019s very characteristic\nof the general statistics of language):\n\nSo what happens if one goes on longer? Here\u2019s a random example. It\u2019s better\nthan the top-word (zero temperature) case, but still at best a bit weird:\n\nThis was done with the simplest GPT-2 model (from 2019). With the newer and\nbigger GPT-3 models the results are better. Here\u2019s the top-word (zero\ntemperature) text produced with the same \u201cprompt\u201d, but with the biggest GPT-3\nmodel:\n\nAnd here\u2019s a random example at \u201ctemperature 0.8\u201d:\n\n## Where Do the Probabilities Come From?\n\nOK, so ChatGPT always picks its next word based on probabilities. But where do\nthose probabilities come from? Let\u2019s start with a simpler problem. Let\u2019s\nconsider generating English text one letter (rather than word) at a time. How\ncan we work out what the probability for each letter should be?\n\nA very minimal thing we could do is just take a sample of English text, and\ncalculate how often different letters occur in it. So, for example, this\ncounts letters in the Wikipedia article on \u201ccats\u201d:\n\nAnd this does the same thing for \u201cdogs\u201d:\n\nThe results are similar, but not the same (\u201co\u201d is no doubt more common in the\n\u201cdogs\u201d article because, after all, it occurs in the word \u201cdog\u201d itself). Still,\nif we take a large enough sample of English text we can expect to eventually\nget at least fairly consistent results:\n\nHere\u2019s a sample of what we get if we just generate a sequence of letters with\nthese probabilities:\n\nWe can break this into \u201cwords\u201d by adding in spaces as if they were letters\nwith a certain probability:\n\nWe can do a slightly better job of making \u201cwords\u201d by forcing the distribution\nof \u201cword lengths\u201d to agree with what it is in English:\n\nWe didn\u2019t happen to get any \u201cactual words\u201d here, but the results are looking\nslightly better. To go further, though, we need to do more than just pick each\nletter separately at random. And, for example, we know that if we have a \u201cq\u201d,\nthe next letter basically has to be \u201cu\u201d.\n\nHere\u2019s a plot of the probabilities for letters on their own:\n\nAnd here\u2019s a plot that shows the probabilities of pairs of letters (\u201c2-grams\u201d)\nin typical English text. The possible first letters are shown across the page,\nthe second letters down the page:\n\nAnd we see here, for example, that the \u201cq\u201d column is blank (zero probability)\nexcept on the \u201cu\u201d row. OK, so now instead of generating our \u201cwords\u201d a single\nletter at a time, let\u2019s generate them looking at two letters at a time, using\nthese \u201c2-gram\u201d probabilities. Here\u2019s a sample of the result\u2014which happens to\ninclude a few \u201cactual words\u201d:\n\nWith sufficiently much English text we can get pretty good estimates not just\nfor probabilities of single letters or pairs of letters (2-grams), but also\nfor longer runs of letters. And if we generate \u201crandom words\u201d with\nprogressively longer _n_ -gram probabilities, we see that they get\nprogressively \u201cmore realistic\u201d:\n\nBut let\u2019s now assume\u2014more or less as ChatGPT does\u2014that we\u2019re dealing with\nwhole words, not letters. There are about 40,000 reasonably commonly used\nwords in English. And by looking at a large corpus of English text (say a few\nmillion books, with altogether a few hundred billion words), we can get an\nestimate of how common each word is. And using this we can start generating\n\u201csentences\u201d, in which each word is independently picked at random, with the\nsame probability that it appears in the corpus. Here\u2019s a sample of what we\nget:\n\nNot surprisingly, this is nonsense. So how can we do better? Just like with\nletters, we can start taking into account not just probabilities for single\nwords but probabilities for pairs or longer _n_ -grams of words. Doing this\nfor pairs, here are 5 examples of what we get, in all cases starting from the\nword \u201ccat\u201d:\n\nIt\u2019s getting slightly more \u201csensible looking\u201d. And we might imagine that if we\nwere able to use sufficiently long _n_ -grams we\u2019d basically \u201cget a\nChatGPT\u201d\u2014in the sense that we\u2019d get something that would generate essay-length\nsequences of words with the \u201ccorrect overall essay probabilities\u201d. But here\u2019s\nthe problem: there just isn\u2019t even close to enough English text that\u2019s ever\nbeen written to be able to deduce those probabilities.\n\nIn a crawl of the web there might be a few hundred billion words; in books\nthat have been digitized there might be another hundred billion words. But\nwith 40,000 common words, even the number of possible 2-grams is already 1.6\nbillion\u2014and the number of possible 3-grams is 60 trillion. So there\u2019s no way\nwe can estimate the probabilities even for all of these from text that\u2019s out\nthere. And by the time we get to \u201cessay fragments\u201d of 20 words, the number of\npossibilities is larger than the number of particles in the universe, so in a\nsense they could never all be written down.\n\nSo what can we do? The big idea is to make a model that lets us estimate the\nprobabilities with which sequences should occur\u2014even though we\u2019ve never\nexplicitly seen those sequences in the corpus of text we\u2019ve looked at. And at\nthe core of ChatGPT is precisely a so-called \u201clarge language model\u201d (LLM)\nthat\u2019s been built to do a good job of estimating those probabilities.\n\n## What Is a Model?\n\nSay you want to know (as Galileo did back in the late 1500s) how long it\u2019s\ngoing to take a cannon ball dropped from each floor of the Tower of Pisa to\nhit the ground. Well, you could just measure it in each case and make a table\nof the results. Or you could do what is the essence of theoretical science:\nmake a model that gives some kind of procedure for computing the answer rather\nthan just measuring and remembering each case.\n\nLet\u2019s imagine we have (somewhat idealized) data for how long the cannon ball\ntakes to fall from various floors:\n\nHow do we figure out how long it\u2019s going to take to fall from a floor we don\u2019t\nexplicitly have data about? In this particular case, we can use known laws of\nphysics to work it out. But say all we\u2019ve got is the data, and we don\u2019t know\nwhat underlying laws govern it. Then we might make a mathematical guess, like\nthat perhaps we should use a straight line as a model:\n\nWe could pick different straight lines. But this is the one that\u2019s on average\nclosest to the data we\u2019re given. And from this straight line we can estimate\nthe time to fall for any floor.\n\nHow did we know to try using a straight line here? At some level we didn\u2019t.\nIt\u2019s just something that\u2019s mathematically simple, and we\u2019re used to the fact\nthat lots of data we measure turns out to be well fit by mathematically simple\nthings. We could try something mathematically more complicated\u2014say _a_ \\+ _b_\n_x_ \\+ _c_ _x_ 2\u2014and then in this case we do better:\n\nThings can go quite wrong, though. Like here\u2019s the best we can do with _a_ \\+\n_b_ / _x_ \\+ _c_ sin( _x_ ):\n\nIt is worth understanding that there\u2019s never a \u201cmodel-less model\u201d. Any model\nyou use has some particular underlying structure\u2014then a certain set of \u201cknobs\nyou can turn\u201d (i.e. parameters you can set) to fit your data. And in the case\nof ChatGPT, lots of such \u201cknobs\u201d are used\u2014actually, 175 billion of them.\n\nBut the remarkable thing is that the underlying structure of ChatGPT\u2014with\n\u201cjust\u201d that many parameters\u2014is sufficient to make a model that computes next-\nword probabilities \u201cwell enough\u201d to give us reasonable essay-length pieces of\ntext.\n\n## Models for Human-Like Tasks\n\nThe example we gave above involves making a model for numerical data that\nessentially comes from simple physics\u2014where we\u2019ve known for several centuries\nthat \u201csimple mathematics applies\u201d. But for ChatGPT we have to make a model of\nhuman-language text of the kind produced by a human brain. And for something\nlike that we don\u2019t (at least yet) have anything like \u201csimple mathematics\u201d. So\nwhat might a model of it be like?\n\nBefore we talk about language, let\u2019s talk about another human-like task:\nrecognizing images. And as a simple example of this, let\u2019s consider images of\ndigits (and, yes, this is a classic machine learning example):\n\nOne thing we could do is get a bunch of sample images for each digit:\n\nThen to find out if an image we\u2019re given as input corresponds to a particular\ndigit we could just do an explicit pixel-by-pixel comparison with the samples\nwe have. But as humans we certainly seem to do something better\u2014because we can\nstill recognize digits, even when they\u2019re for example handwritten, and have\nall sorts of modifications and distortions:\n\nWhen we made a model for our numerical data above, we were able to take a\nnumerical value _x_ that we were given, and just compute _a + b x_ for\nparticular _a_ and _b_. So if we treat the gray-level value of each pixel here\nas some variable _x i_ is there some function of all those variables that\u2014when\nevaluated\u2014tells us what digit the image is of? It turns out that it\u2019s possible\nto construct such a function. Not surprisingly, it\u2019s not particularly simple,\nthough. And a typical example might involve perhaps half a million\nmathematical operations.\n\nBut the end result is that if we feed the collection of pixel values for an\nimage into this function, out will come the number specifying which digit we\nhave an image of. Later, we\u2019ll talk about how such a function can be\nconstructed, and the idea of neural nets. But for now let\u2019s treat the function\nas black box, where we feed in images of, say, handwritten digits (as arrays\nof pixel values) and we get out the numbers these correspond to:\n\nBut what\u2019s really going on here? Let\u2019s say we progressively blur a digit. For\na little while our function still \u201crecognizes\u201d it, here as a \u201c2\u201d. But soon it\n\u201closes it\u201d, and starts giving the \u201cwrong\u201d result:\n\nBut why do we say it\u2019s the \u201cwrong\u201d result? In this case, we know we got all\nthe images by blurring a \u201c2\u201d. But if our goal is to produce a model of what\nhumans can do in recognizing images, the real question to ask is what a human\nwould have done if presented with one of those blurred images, without knowing\nwhere it came from.\n\nAnd we have a \u201cgood model\u201d if the results we get from our function typically\nagree with what a human would say. And the nontrivial scientific fact is that\nfor an image-recognition task like this we now basically know how to construct\nfunctions that do this.\n\nCan we \u201cmathematically prove\u201d that they work? Well, no. Because to do that\nwe\u2019d have to have a mathematical theory of what we humans are doing. Take the\n\u201c2\u201d image and change a few pixels. We might imagine that with only a few\npixels \u201cout of place\u201d we should still consider the image a \u201c2\u201d. But how far\nshould that go? It\u2019s a question of human visual perception. And, yes, the\nanswer would no doubt be different for bees or octopuses\u2014and potentially\nutterly different for putative aliens.\n\n## Neural Nets\n\nOK, so how do our typical models for tasks like image recognition actually\nwork? The most popular\u2014and successful\u2014current approach uses neural nets.\nInvented\u2014in a form remarkably close to their use today\u2014in the 1940s, neural\nnets can be thought of as simple idealizations of how brains seem to work.\n\nIn human brains there are about 100 billion neurons (nerve cells), each\ncapable of producing an electrical pulse up to perhaps a thousand times a\nsecond. The neurons are connected in a complicated net, with each neuron\nhaving tree-like branches allowing it to pass electrical signals to perhaps\nthousands of other neurons. And in a rough approximation, whether any given\nneuron produces an electrical pulse at a given moment depends on what pulses\nit\u2019s received from other neurons\u2014with different connections contributing with\ndifferent \u201cweights\u201d.\n\nWhen we \u201csee an image\u201d what\u2019s happening is that when photons of light from the\nimage fall on (\u201cphotoreceptor\u201d) cells at the back of our eyes they produce\nelectrical signals in nerve cells. These nerve cells are connected to other\nnerve cells, and eventually the signals go through a whole sequence of layers\nof neurons. And it\u2019s in this process that we \u201crecognize\u201d the image, eventually\n\u201cforming the thought\u201d that we\u2019re \u201cseeing a 2\u201d (and maybe in the end doing\nsomething like saying the word \u201ctwo\u201d out loud).\n\nThe \u201cblack-box\u201d function from the previous section is a \u201cmathematicized\u201d\nversion of such a neural net. It happens to have 11 layers (though only 4\n\u201ccore layers\u201d):\n\nThere\u2019s nothing particularly \u201ctheoretically derived\u201d about this neural net;\nit\u2019s just something that\u2014back in 1998\u2014was constructed as a piece of\nengineering, and found to work. (Of course, that\u2019s not much different from how\nwe might describe our brains as having been produced through the process of\nbiological evolution.)\n\nOK, but how does a neural net like this \u201crecognize things\u201d? The key is the\nnotion of attractors. Imagine we\u2019ve got handwritten images of 1\u2019s and 2\u2019s:\n\nWe somehow want all the 1\u2019s to \u201cbe attracted to one place\u201d, and all the 2\u2019s to\n\u201cbe attracted to another place\u201d. Or, put a different way, if an image is\nsomehow \u201ccloser to being a 1\u201d than to being a 2, we want it to end up in the\n\u201c1 place\u201d and vice versa.\n\nAs a straightforward analogy, let\u2019s say we have certain positions in the\nplane, indicated by dots (in a real-life setting they might be positions of\ncoffee shops). Then we might imagine that starting from any point on the plane\nwe\u2019d always want to end up at the closest dot (i.e. we\u2019d always go to the\nclosest coffee shop). We can represent this by dividing the plane into regions\n(\u201cattractor basins\u201d) separated by idealized \u201cwatersheds\u201d:\n\nWe can think of this as implementing a kind of \u201crecognition task\u201d in which\nwe\u2019re not doing something like identifying what digit a given image \u201clooks\nmost like\u201d\u2014but rather we\u2019re just, quite directly, seeing what dot a given\npoint is closest to. (The \u201cVoronoi diagram\u201d setup we\u2019re showing here separates\npoints in 2D Euclidean space; the digit recognition task can be thought of as\ndoing something very similar\u2014but in a 784-dimensional space formed from the\ngray levels of all the pixels in each image.)\n\nSo how do we make a neural net \u201cdo a recognition task\u201d? Let\u2019s consider this\nvery simple case:\n\nOur goal is to take an \u201cinput\u201d corresponding to a position { _x_ , _y_ }\u2014and\nthen to \u201crecognize\u201d it as whichever of the three points it\u2019s closest to. Or,\nin other words, we want the neural net to compute a function of { _x_ , _y_ }\nlike:\n\nSo how do we do this with a neural net? Ultimately a neural net is a connected\ncollection of idealized \u201cneurons\u201d\u2014usually arranged in layers\u2014with a simple\nexample being:\n\nEach \u201cneuron\u201d is effectively set up to evaluate a simple numerical function.\nAnd to \u201cuse\u201d the network, we simply feed numbers (like our coordinates _x_ and\n_y_ ) in at the top, then have neurons on each layer \u201cevaluate their\nfunctions\u201d and feed the results forward through the network\u2014eventually\nproducing the final result at the bottom:\n\nIn the traditional (biologically inspired) setup each neuron effectively has a\ncertain set of \u201cincoming connections\u201d from the neurons on the previous layer,\nwith each connection being assigned a certain \u201cweight\u201d (which can be a\npositive or negative number). The value of a given neuron is determined by\nmultiplying the values of \u201cprevious neurons\u201d by their corresponding weights,\nthen adding these up and adding a constant\u2014and finally applying a\n\u201cthresholding\u201d (or \u201cactivation\u201d) function. In mathematical terms, if a neuron\nhas inputs _x_ = { _x_ 1, _x_ 2 \u2026} then we compute _f_ [ _w_ . _x_ \\+ _b_ ],\nwhere the weights _w_ and constant _b_ are generally chosen differently for\neach neuron in the network; the function _f_ is usually the same.\n\nComputing _w_ . _x_ \\+ _b_ is just a matter of matrix multiplication and\naddition. The \u201cactivation function\u201d _f_ introduces nonlinearity (and\nultimately is what leads to nontrivial behavior). Various activation functions\ncommonly get used; here we\u2019ll just use `Ramp` (or ReLU):\n\nFor each task we want the neural net to perform (or, equivalently, for each\noverall function we want it to evaluate) we\u2019ll have different choices of\nweights. (And\u2014as we\u2019ll discuss later\u2014these weights are normally determined by\n\u201ctraining\u201d the neural net using machine learning from examples of the outputs\nwe want.)\n\nUltimately, every neural net just corresponds to some overall mathematical\nfunction\u2014though it may be messy to write out. For the example above, it would\nbe:\n\nThe neural net of ChatGPT also just corresponds to a mathematical function\nlike this\u2014but effectively with billions of terms.\n\nBut let\u2019s go back to individual neurons. Here are some examples of the\nfunctions a neuron with two inputs (representing coordinates _x_ and _y_ ) can\ncompute with various choices of weights and constants (and `Ramp` as\nactivation function):\n\nBut what about the larger network from above? Well, here\u2019s what it computes:\n\nIt\u2019s not quite \u201cright\u201d, but it\u2019s close to the \u201cnearest point\u201d function we\nshowed above.\n\nLet\u2019s see what happens with some other neural nets. In each case, as we\u2019ll\nexplain later, we\u2019re using machine learning to find the best choice of\nweights. Then we\u2019re showing here what the neural net with those weights\ncomputes:\n\nBigger networks generally do better at approximating the function we\u2019re aiming\nfor. And in the \u201cmiddle of each attractor basin\u201d we typically get exactly the\nanswer we want. But at the boundaries\u2014where the neural net \u201chas a hard time\nmaking up its mind\u201d\u2014things can be messier.\n\nWith this simple mathematical-style \u201crecognition task\u201d it\u2019s clear what the\n\u201cright answer\u201d is. But in the problem of recognizing handwritten digits, it\u2019s\nnot so clear. What if someone wrote a \u201c2\u201d so badly it looked like a \u201c7\u201d, etc.?\nStill, we can ask how a neural net distinguishes digits\u2014and this gives an\nindication:\n\nCan we say \u201cmathematically\u201d how the network makes its distinctions? Not\nreally. It\u2019s just \u201cdoing what the neural net does\u201d. But it turns out that that\nnormally seems to agree fairly well with the distinctions we humans make.\n\nLet\u2019s take a more elaborate example. Let\u2019s say we have images of cats and\ndogs. And we have a neural net that\u2019s been trained to distinguish them. Here\u2019s\nwhat it might do on some examples:\n\nNow it\u2019s even less clear what the \u201cright answer\u201d is. What about a dog dressed\nin a cat suit? Etc. Whatever input it\u2019s given the neural net will generate an\nanswer, and in a way reasonably consistent with how humans might. As I\u2019ve said\nabove, that\u2019s not a fact we can \u201cderive from first principles\u201d. It\u2019s just\nsomething that\u2019s empirically been found to be true, at least in certain\ndomains. But it\u2019s a key reason why neural nets are useful: that they somehow\ncapture a \u201chuman-like\u201d way of doing things.\n\nShow yourself a picture of a cat, and ask \u201cWhy is that a cat?\u201d. Maybe you\u2019d\nstart saying \u201cWell, I see its pointy ears, etc.\u201d But it\u2019s not very easy to\nexplain how you recognized the image as a cat. It\u2019s just that somehow your\nbrain figured that out. But for a brain there\u2019s no way (at least yet) to \u201cgo\ninside\u201d and see how it figured it out. What about for an (artificial) neural\nnet? Well, it\u2019s straightforward to see what each \u201cneuron\u201d does when you show a\npicture of a cat. But even to get a basic visualization is usually very\ndifficult.\n\nIn the final net that we used for the \u201cnearest point\u201d problem above there are\n17 neurons. In the net for recognizing handwritten digits there are 2190. And\nin the net we\u2019re using to recognize cats and dogs there are 60,650. Normally\nit would be pretty difficult to visualize what amounts to 60,650-dimensional\nspace. But because this is a network set up to deal with images, many of its\nlayers of neurons are organized into arrays, like the arrays of pixels it\u2019s\nlooking at.\n\nAnd if we take a typical cat image\n\nthen we can represent the states of neurons at the first layer by a collection\nof derived images\u2014many of which we can readily interpret as being things like\n\u201cthe cat without its background\u201d, or \u201cthe outline of the cat\u201d:\n\nBy the 10th layer it\u2019s harder to interpret what\u2019s going on:\n\nBut in general we might say that the neural net is \u201cpicking out certain\nfeatures\u201d (maybe pointy ears are among them), and using these to determine\nwhat the image is of. But are those features ones for which we have names\u2014like\n\u201cpointy ears\u201d? Mostly not.\n\nAre our brains using similar features? Mostly we don\u2019t know. But it\u2019s notable\nthat the first few layers of a neural net like the one we\u2019re showing here seem\nto pick out aspects of images (like edges of objects) that seem to be similar\nto ones we know are picked out by the first level of visual processing in\nbrains.\n\nBut let\u2019s say we want a \u201ctheory of cat recognition\u201d in neural nets. We can\nsay: \u201cLook, this particular net does it\u201d\u2014and immediately that gives us some\nsense of \u201chow hard a problem\u201d it is (and, for example, how many neurons or\nlayers might be needed). But at least as of now we don\u2019t have a way to \u201cgive a\nnarrative description\u201d of what the network is doing. And maybe that\u2019s because\nit truly is computationally irreducible, and there\u2019s no general way to find\nwhat it does except by explicitly tracing each step. Or maybe it\u2019s just that\nwe haven\u2019t \u201cfigured out the science\u201d, and identified the \u201cnatural laws\u201d that\nallow us to summarize what\u2019s going on.\n\nWe\u2019ll encounter the same kinds of issues when we talk about generating\nlanguage with ChatGPT. And again it\u2019s not clear whether there are ways to\n\u201csummarize what it\u2019s doing\u201d. But the richness and detail of language (and our\nexperience with it) may allow us to get further than with images.\n\n## Machine Learning, and the Training of Neural Nets\n\nWe\u2019ve been talking so far about neural nets that \u201calready know\u201d how to do\nparticular tasks. But what makes neural nets so useful (presumably also in\nbrains) is that not only can they in principle do all sorts of tasks, but they\ncan be incrementally \u201ctrained from examples\u201d to do those tasks.\n\nWhen we make a neural net to distinguish cats from dogs we don\u2019t effectively\nhave to write a program that (say) explicitly finds whiskers; instead we just\nshow lots of examples of what\u2019s a cat and what\u2019s a dog, and then have the\nnetwork \u201cmachine learn\u201d from these how to distinguish them.\n\nAnd the point is that the trained network \u201cgeneralizes\u201d from the particular\nexamples it\u2019s shown. Just as we\u2019ve seen above, it isn\u2019t simply that the\nnetwork recognizes the particular pixel pattern of an example cat image it was\nshown; rather it\u2019s that the neural net somehow manages to distinguish images\non the basis of what we consider to be some kind of \u201cgeneral catness\u201d.\n\nSo how does neural net training actually work? Essentially what we\u2019re always\ntrying to do is to find weights that make the neural net successfully\nreproduce the examples we\u2019ve given. And then we\u2019re relying on the neural net\nto \u201cinterpolate\u201d (or \u201cgeneralize\u201d) \u201cbetween\u201d these examples in a \u201creasonable\u201d\nway.\n\nLet\u2019s look at a problem even simpler than the nearest-point one above. Let\u2019s\njust try to get a neural net to learn the function:\n\nFor this task, we\u2019ll need a network that has just one input and one output,\nlike:\n\nBut what weights, etc. should we be using? With every possible set of weights\nthe neural net will compute some function. And, for example, here\u2019s what it\ndoes with a few randomly chosen sets of weights:\n\nAnd, yes, we can plainly see that in none of these cases does it get even\nclose to reproducing the function we want. So how do we find weights that will\nreproduce the function?\n\nThe basic idea is to supply lots of \u201cinput \u2192 output\u201d examples to \u201clearn\nfrom\u201d\u2014and then to try to find weights that will reproduce these examples.\nHere\u2019s the result of doing that with progressively more examples:\n\nAt each stage in this \u201ctraining\u201d the weights in the network are progressively\nadjusted\u2014and we see that eventually we get a network that successfully\nreproduces the function we want. So how do we adjust the weights? The basic\nidea is at each stage to see \u201chow far away we are\u201d from getting the function\nwe want\u2014and then to update the weights in such a way as to get closer.\n\nTo find out \u201chow far away we are\u201d we compute what\u2019s usually called a \u201closs\nfunction\u201d (or sometimes \u201ccost function\u201d). Here we\u2019re using a simple (L2) loss\nfunction that\u2019s just the sum of the squares of the differences between the\nvalues we get, and the true values. And what we see is that as our training\nprocess progresses, the loss function progressively decreases (following a\ncertain \u201clearning curve\u201d that\u2019s different for different tasks)\u2014until we reach\na point where the network (at least to a good approximation) successfully\nreproduces the function we want:\n\nAlright, so the last essential piece to explain is how the weights are\nadjusted to reduce the loss function. As we\u2019ve said, the loss function gives\nus a \u201cdistance\u201d between the values we\u2019ve got, and the true values. But the\n\u201cvalues we\u2019ve got\u201d are determined at each stage by the current version of\nneural net\u2014and by the weights in it. But now imagine that the weights are\nvariables\u2014say _w i_. We want to find out how to adjust the values of these\nvariables to minimize the loss that depends on them.\n\nFor example, imagine (in an incredible simplification of typical neural nets\nused in practice) that we have just two weights _w_ 1 and _w_ 2. Then we might\nhave a loss that as a function of _w_ 1 and _w_ 2 looks like this:\n\nNumerical analysis provides a variety of techniques for finding the minimum in\ncases like this. But a typical approach is just to progressively follow the\npath of steepest descent from whatever previous _w_ 1, _w_ 2 we had:\n\nLike water flowing down a mountain, all that\u2019s guaranteed is that this\nprocedure will end up at some local minimum of the surface (\u201ca mountain\nlake\u201d); it might well not reach the ultimate global minimum.\n\nIt\u2019s not obvious that it would be feasible to find the path of the steepest\ndescent on the \u201cweight landscape\u201d. But calculus comes to the rescue. As we\nmentioned above, one can always think of a neural net as computing a\nmathematical function\u2014that depends on its inputs, and its weights. But now\nconsider differentiating with respect to these weights. It turns out that the\nchain rule of calculus in effect lets us \u201cunravel\u201d the operations done by\nsuccessive layers in the neural net. And the result is that we can\u2014at least in\nsome local approximation\u2014\u201cinvert\u201d the operation of the neural net, and\nprogressively find weights that minimize the loss associated with the output.\n\nThe picture above shows the kind of minimization we might need to do in the\nunrealistically simple case of just 2 weights. But it turns out that even with\nmany more weights (ChatGPT uses 175 billion) it\u2019s still possible to do the\nminimization, at least to some level of approximation. And in fact the big\nbreakthrough in \u201cdeep learning\u201d that occurred around 2011 was associated with\nthe discovery that in some sense it can be easier to do (at least approximate)\nminimization when there are lots of weights involved than when there are\nfairly few.\n\nIn other words\u2014somewhat counterintuitively\u2014it can be easier to solve more\ncomplicated problems with neural nets than simpler ones. And the rough reason\nfor this seems to be that when one has a lot of \u201cweight variables\u201d one has a\nhigh-dimensional space with \u201clots of different directions\u201d that can lead one\nto the minimum\u2014whereas with fewer variables it\u2019s easier to end up getting\nstuck in a local minimum (\u201cmountain lake\u201d) from which there\u2019s no \u201cdirection to\nget out\u201d.\n\nIt\u2019s worth pointing out that in typical cases there are many different\ncollections of weights that will all give neural nets that have pretty much\nthe same performance. And usually in practical neural net training there are\nlots of random choices made\u2014that lead to \u201cdifferent-but-equivalent solutions\u201d,\nlike these:\n\nBut each such \u201cdifferent solution\u201d will have at least slightly different\nbehavior. And if we ask, say, for an \u201cextrapolation\u201d outside the region where\nwe gave training examples, we can get dramatically different results:\n\nBut which of these is \u201cright\u201d? There\u2019s really no way to say. They\u2019re all\n\u201cconsistent with the observed data\u201d. But they all correspond to different\n\u201cinnate\u201d ways to \u201cthink about\u201d what to do \u201coutside the box\u201d. And some may seem\n\u201cmore reasonable\u201d to us humans than others.\n\n## The Practice and Lore of Neural Net Training\n\nParticularly over the past decade, there\u2019ve been many advances in the art of\ntraining neural nets. And, yes, it is basically an art. Sometimes\u2014especially\nin retrospect\u2014one can see at least a glimmer of a \u201cscientific explanation\u201d for\nsomething that\u2019s being done. But mostly things have been discovered by trial\nand error, adding ideas and tricks that have progressively built a significant\nlore about how to work with neural nets.\n\nThere are several key parts. First, there\u2019s the matter of what architecture of\nneural net one should use for a particular task. Then there\u2019s the critical\nissue of how one\u2019s going to get the data on which to train the neural net. And\nincreasingly one isn\u2019t dealing with training a net from scratch: instead a new\nnet can either directly incorporate another already-trained net, or at least\ncan use that net to generate more training examples for itself.\n\nOne might have thought that for every particular kind of task one would need a\ndifferent architecture of neural net. But what\u2019s been found is that the same\narchitecture often seems to work even for apparently quite different tasks. At\nsome level this reminds one of the idea of universal computation (and my\nPrinciple of Computational Equivalence), but, as I\u2019ll discuss later, I think\nit\u2019s more a reflection of the fact that the tasks we\u2019re typically trying to\nget neural nets to do are \u201chuman-like\u201d ones\u2014and neural nets can capture quite\ngeneral \u201chuman-like processes\u201d.\n\nIn earlier days of neural nets, there tended to be the idea that one should\n\u201cmake the neural net do as little as possible\u201d. For example, in converting\nspeech to text it was thought that one should first analyze the audio of the\nspeech, break it into phonemes, etc. But what was found is that\u2014at least for\n\u201chuman-like tasks\u201d\u2014it\u2019s usually better just to try to train the neural net on\nthe \u201cend-to-end problem\u201d, letting it \u201cdiscover\u201d the necessary intermediate\nfeatures, encodings, etc. for itself.\n\nThere was also the idea that one should introduce complicated individual\ncomponents into the neural net, to let it in effect \u201cexplicitly implement\nparticular algorithmic ideas\u201d. But once again, this has mostly turned out not\nto be worthwhile; instead, it\u2019s better just to deal with very simple\ncomponents and let them \u201corganize themselves\u201d (albeit usually in ways we can\u2019t\nunderstand) to achieve (presumably) the equivalent of those algorithmic ideas.\n\nThat\u2019s not to say that there are no \u201cstructuring ideas\u201d that are relevant for\nneural nets. Thus, for example, having 2D arrays of neurons with local\nconnections seems at least very useful in the early stages of processing\nimages. And having patterns of connectivity that concentrate on \u201clooking back\nin sequences\u201d seems useful\u2014as we\u2019ll see later\u2014in dealing with things like\nhuman language, for example in ChatGPT.\n\nBut an important feature of neural nets is that\u2014like computers in\ngeneral\u2014they\u2019re ultimately just dealing with data. And current neural\nnets\u2014with current approaches to neural net training\u2014specifically deal with\narrays of numbers. But in the course of processing, those arrays can be\ncompletely rearranged and reshaped. And as an example, the network we used for\nidentifying digits above starts with a 2D \u201cimage-like\u201d array, quickly\n\u201cthickening\u201d to many channels, but then \u201cconcentrating down\u201d into a 1D array\nthat will ultimately contain elements representing the different possible\noutput digits:\n\nBut, OK, how can one tell how big a neural net one will need for a particular\ntask? It\u2019s something of an art. At some level the key thing is to know \u201chow\nhard the task is\u201d. But for human-like tasks that\u2019s typically very hard to\nestimate. Yes, there may be a systematic way to do the task very\n\u201cmechanically\u201d by computer. But it\u2019s hard to know if there are what one might\nthink of as tricks or shortcuts that allow one to do the task at least at a\n\u201chuman-like level\u201d vastly more easily. It might take enumerating a giant game\ntree to \u201cmechanically\u201d play a certain game; but there might be a much easier\n(\u201cheuristic\u201d) way to achieve \u201chuman-level play\u201d.\n\nWhen one\u2019s dealing with tiny neural nets and simple tasks one can sometimes\nexplicitly see that one \u201ccan\u2019t get there from here\u201d. For example, here\u2019s the\nbest one seems to be able to do on the task from the previous section with a\nfew small neural nets:\n\nAnd what we see is that if the net is too small, it just can\u2019t reproduce the\nfunction we want. But above some size, it has no problem\u2014at least if one\ntrains it for long enough, with enough examples. And, by the way, these\npictures illustrate a piece of neural net lore: that one can often get away\nwith a smaller network if there\u2019s a \u201csqueeze\u201d in the middle that forces\neverything to go through a smaller intermediate number of neurons. (It\u2019s also\nworth mentioning that \u201cno-intermediate-layer\u201d\u2014or so-called\n\u201cperceptron\u201d\u2014networks can only learn essentially linear functions\u2014but as soon\nas there\u2019s even one intermediate layer it\u2019s always in principle possible to\napproximate any function arbitrarily well, at least if one has enough neurons,\nthough to make it feasibly trainable one typically has some kind of\nregularization or normalization.)\n\nOK, so let\u2019s say one\u2019s settled on a certain neural net architecture. Now\nthere\u2019s the issue of getting data to train the network with. And many of the\npractical challenges around neural nets\u2014and machine learning in general\u2014center\non acquiring or preparing the necessary training data. In many cases\n(\u201csupervised learning\u201d) one wants to get explicit examples of inputs and the\noutputs one is expecting from them. Thus, for example, one might want images\ntagged by what\u2019s in them, or some other attribute. And maybe one will have to\nexplicitly go through\u2014usually with great effort\u2014and do the tagging. But very\noften it turns out to be possible to piggyback on something that\u2019s already\nbeen done, or use it as some kind of proxy. And so, for example, one might use\nalt tags that have been provided for images on the web. Or, in a different\ndomain, one might use closed captions that have been created for videos.\nOr\u2014for language translation training\u2014one might use parallel versions of\nwebpages or other documents that exist in different languages.\n\nHow much data do you need to show a neural net to train it for a particular\ntask? Again, it\u2019s hard to estimate from first principles. Certainly the\nrequirements can be dramatically reduced by using \u201ctransfer learning\u201d to\n\u201ctransfer in\u201d things like lists of important features that have already been\nlearned in another network. But generally neural nets need to \u201csee a lot of\nexamples\u201d to train well. And at least for some tasks it\u2019s an important piece\nof neural net lore that the examples can be incredibly repetitive. And indeed\nit\u2019s a standard strategy to just show a neural net all the examples one has,\nover and over again. In each of these \u201ctraining rounds\u201d (or \u201cepochs\u201d) the\nneural net will be in at least a slightly different state, and somehow\n\u201creminding it\u201d of a particular example is useful in getting it to \u201cremember\nthat example\u201d. (And, yes, perhaps this is analogous to the usefulness of\nrepetition in human memorization.)\n\nBut often just repeating the same example over and over again isn\u2019t enough.\nIt\u2019s also necessary to show the neural net variations of the example. And it\u2019s\na feature of neural net lore that those \u201cdata augmentation\u201d variations don\u2019t\nhave to be sophisticated to be useful. Just slightly modifying images with\nbasic image processing can make them essentially \u201cas good as new\u201d for neural\nnet training. And, similarly, when one\u2019s run out of actual video, etc. for\ntraining self-driving cars, one can go on and just get data from running\nsimulations in a model videogame-like environment without all the detail of\nactual real-world scenes.\n\nHow about something like ChatGPT? Well, it has the nice feature that it can do\n\u201cunsupervised learning\u201d, making it much easier to get it examples to train\nfrom. Recall that the basic task for ChatGPT is to figure out how to continue\na piece of text that it\u2019s been given. So to get it \u201ctraining examples\u201d all one\nhas to do is get a piece of text, and mask out the end of it, and then use\nthis as the \u201cinput to train from\u201d\u2014with the \u201coutput\u201d being the complete,\nunmasked piece of text. We\u2019ll discuss this more later, but the main point is\nthat\u2014unlike, say, for learning what\u2019s in images\u2014there\u2019s no \u201cexplicit tagging\u201d\nneeded; ChatGPT can in effect just learn directly from whatever examples of\ntext it\u2019s given.\n\nOK, so what about the actual learning process in a neural net? In the end it\u2019s\nall about determining what weights will best capture the training examples\nthat have been given. And there are all sorts of detailed choices and\n\u201chyperparameter settings\u201d (so called because the weights can be thought of as\n\u201cparameters\u201d) that can be used to tweak how this is done. There are different\nchoices of loss function (sum of squares, sum of absolute values, etc.). There\nare different ways to do loss minimization (how far in weight space to move at\neach step, etc.). And then there are questions like how big a \u201cbatch\u201d of\nexamples to show to get each successive estimate of the loss one\u2019s trying to\nminimize. And, yes, one can apply machine learning (as we do, for example, in\nWolfram Language) to automate machine learning\u2014and to automatically set things\nlike hyperparameters.\n\nBut in the end the whole process of training can be characterized by seeing\nhow the loss progressively decreases (as in this Wolfram Language progress\nmonitor for a small training):\n\nAnd what one typically sees is that the loss decreases for a while, but\neventually flattens out at some constant value. If that value is sufficiently\nsmall, then the training can be considered successful; otherwise it\u2019s probably\na sign one should try changing the network architecture.\n\nCan one tell how long it should take for the \u201clearning curve\u201d to flatten out?\nLike for so many other things, there seem to be approximate power-law scaling\nrelationships that depend on the size of neural net and amount of data one\u2019s\nusing. But the general conclusion is that training a neural net is hard\u2014and\ntakes a lot of computational effort. And as a practical matter, the vast\nmajority of that effort is spent doing operations on arrays of numbers, which\nis what GPUs are good at\u2014which is why neural net training is typically limited\nby the availability of GPUs.\n\nIn the future, will there be fundamentally better ways to train neural nets\u2014or\ngenerally do what neural nets do? Almost certainly, I think. The fundamental\nidea of neural nets is to create a flexible \u201ccomputing fabric\u201d out of a large\nnumber of simple (essentially identical) components\u2014and to have this \u201cfabric\u201d\nbe one that can be incrementally modified to learn from examples. In current\nneural nets, one\u2019s essentially using the ideas of calculus\u2014applied to real\nnumbers\u2014to do that incremental modification. But it\u2019s increasingly clear that\nhaving high-precision numbers doesn\u2019t matter; 8 bits or less might be enough\neven with current methods.\n\nWith computational systems like cellular automata that basically operate in\nparallel on many individual bits it\u2019s never been clear how to do this kind of\nincremental modification, but there\u2019s no reason to think it isn\u2019t possible.\nAnd in fact, much like with the \u201cdeep-learning breakthrough of 2012\u201d it may be\nthat such incremental modification will effectively be easier in more\ncomplicated cases than in simple ones.\n\nNeural nets\u2014perhaps a bit like brains\u2014are set up to have an essentially fixed\nnetwork of neurons, with what\u2019s modified being the strength (\u201cweight\u201d) of\nconnections between them. (Perhaps in at least young brains significant\nnumbers of wholly new connections can also grow.) But while this might be a\nconvenient setup for biology, it\u2019s not at all clear that it\u2019s even close to\nthe best way to achieve the functionality we need. And something that involves\nthe equivalent of progressive network rewriting (perhaps reminiscent of our\nPhysics Project) might well ultimately be better.\n\nBut even within the framework of existing neural nets there\u2019s currently a\ncrucial limitation: neural net training as it\u2019s now done is fundamentally\nsequential, with the effects of each batch of examples being propagated back\nto update the weights. And indeed with current computer hardware\u2014even taking\ninto account GPUs\u2014most of a neural net is \u201cidle\u201d most of the time during\ntraining, with just one part at a time being updated. And in a sense this is\nbecause our current computers tend to have memory that is separate from their\nCPUs (or GPUs). But in brains it\u2019s presumably different\u2014with every \u201cmemory\nelement\u201d (i.e. neuron) also being a potentially active computational element.\nAnd if we could set up our future computer hardware this way it might become\npossible to do training much more efficiently.\n\n## \u201cSurely a Network That\u2019s Big Enough Can Do Anything!\u201d\n\nThe capabilities of something like ChatGPT seem so impressive that one might\nimagine that if one could just \u201ckeep going\u201d and train larger and larger neural\nnetworks, then they\u2019d eventually be able to \u201cdo everything\u201d. And if one\u2019s\nconcerned with things that are readily accessible to immediate human thinking,\nit\u2019s quite possible that this is the case. But the lesson of the past several\nhundred years of science is that there are things that can be figured out by\nformal processes, but aren\u2019t readily accessible to immediate human thinking.\n\nNontrivial mathematics is one big example. But the general case is really\ncomputation. And ultimately the issue is the phenomenon of computational\nirreducibility. There are some computations which one might think would take\nmany steps to do, but which can in fact be \u201creduced\u201d to something quite\nimmediate. But the discovery of computational irreducibility implies that this\ndoesn\u2019t always work. And instead there are processes\u2014probably like the one\nbelow\u2014where to work out what happens inevitably requires essentially tracing\neach computational step:\n\nThe kinds of things that we normally do with our brains are presumably\nspecifically chosen to avoid computational irreducibility. It takes special\neffort to do math in one\u2019s brain. And it\u2019s in practice largely impossible to\n\u201cthink through\u201d the steps in the operation of any nontrivial program just in\none\u2019s brain.\n\nBut of course for that we have computers. And with computers we can readily do\nlong, computationally irreducible things. And the key point is that there\u2019s in\ngeneral no shortcut for these.\n\nYes, we could memorize lots of specific examples of what happens in some\nparticular computational system. And maybe we could even see some\n(\u201ccomputationally reducible\u201d) patterns that would allow us to do a little\ngeneralization. But the point is that computational irreducibility means that\nwe can never guarantee that the unexpected won\u2019t happen\u2014and it\u2019s only by\nexplicitly doing the computation that you can tell what actually happens in\nany particular case.\n\nAnd in the end there\u2019s just a fundamental tension between learnability and\ncomputational irreducibility. Learning involves in effect compressing data by\nleveraging regularities. But computational irreducibility implies that\nultimately there\u2019s a limit to what regularities there may be.\n\nAs a practical matter, one can imagine building little computational\ndevices\u2014like cellular automata or Turing machines\u2014into trainable systems like\nneural nets. And indeed such devices can serve as good \u201ctools\u201d for the neural\nnet\u2014like Wolfram|Alpha can be a good tool for ChatGPT. But computational\nirreducibility implies that one can\u2019t expect to \u201cget inside\u201d those devices and\nhave them learn.\n\nOr put another way, there\u2019s an ultimate tradeoff between capability and\ntrainability: the more you want a system to make \u201ctrue use\u201d of its\ncomputational capabilities, the more it\u2019s going to show computational\nirreducibility, and the less it\u2019s going to be trainable. And the more it\u2019s\nfundamentally trainable, the less it\u2019s going to be able to do sophisticated\ncomputation.\n\n(For ChatGPT as it currently is, the situation is actually much more extreme,\nbecause the neural net used to generate each token of output is a pure \u201cfeed-\nforward\u201d network, without loops, and therefore has no ability to do any kind\nof computation with nontrivial \u201ccontrol flow\u201d.)\n\nOf course, one might wonder whether it\u2019s actually important to be able to do\nirreducible computations. And indeed for much of human history it wasn\u2019t\nparticularly important. But our modern technological world has been built on\nengineering that makes use of at least mathematical computations\u2014and\nincreasingly also more general computations. And if we look at the natural\nworld, it\u2019s full of irreducible computation\u2014that we\u2019re slowly understanding\nhow to emulate and use for our technological purposes.\n\nYes, a neural net can certainly notice the kinds of regularities in the\nnatural world that we might also readily notice with \u201cunaided human thinking\u201d.\nBut if we want to work out things that are in the purview of mathematical or\ncomputational science the neural net isn\u2019t going to be able to do it\u2014unless it\neffectively \u201cuses as a tool\u201d an \u201cordinary\u201d computational system.\n\nBut there\u2019s something potentially confusing about all of this. In the past\nthere were plenty of tasks\u2014including writing essays\u2014that we\u2019ve assumed were\nsomehow \u201cfundamentally too hard\u201d for computers. And now that we see them done\nby the likes of ChatGPT we tend to suddenly think that computers must have\nbecome vastly more powerful\u2014in particular surpassing things they were already\nbasically able to do (like progressively computing the behavior of\ncomputational systems like cellular automata).\n\nBut this isn\u2019t the right conclusion to draw. Computationally irreducible\nprocesses are still computationally irreducible, and are still fundamentally\nhard for computers\u2014even if computers can readily compute their individual\nsteps. And instead what we should conclude is that tasks\u2014like writing\nessays\u2014that we humans could do, but we didn\u2019t think computers could do, are\nactually in some sense computationally easier than we thought.\n\nIn other words, the reason a neural net can be successful in writing an essay\nis because writing an essay turns out to be a \u201ccomputationally shallower\u201d\nproblem than we thought. And in a sense this takes us closer to \u201chaving a\ntheory\u201d of how we humans manage to do things like writing essays, or in\ngeneral deal with language.\n\nIf you had a big enough neural net then, yes, you might be able to do whatever\nhumans can readily do. But you wouldn\u2019t capture what the natural world in\ngeneral can do\u2014or that the tools that we\u2019ve fashioned from the natural world\ncan do. And it\u2019s the use of those tools\u2014both practical and conceptual\u2014that\nhave allowed us in recent centuries to transcend the boundaries of what\u2019s\naccessible to \u201cpure unaided human thought\u201d, and capture for human purposes\nmore of what\u2019s out there in the physical and computational universe.\n\n## The Concept of Embeddings\n\nNeural nets\u2014at least as they\u2019re currently set up\u2014are fundamentally based on\nnumbers. So if we\u2019re going to to use them to work on something like text we\u2019ll\nneed a way to represent our text with numbers. And certainly we could start\n(essentially as ChatGPT does) by just assigning a number to every word in the\ndictionary. But there\u2019s an important idea\u2014that\u2019s for example central to\nChatGPT\u2014that goes beyond that. And it\u2019s the idea of \u201cembeddings\u201d. One can\nthink of an embedding as a way to try to represent the \u201cessence\u201d of something\nby an array of numbers\u2014with the property that \u201cnearby things\u201d are represented\nby nearby numbers.\n\nAnd so, for example, we can think of a word embedding as trying to lay out\nwords in a kind of \u201cmeaning space\u201d in which words that are somehow \u201cnearby in\nmeaning\u201d appear nearby in the embedding. The actual embeddings that are\nused\u2014say in ChatGPT\u2014tend to involve large lists of numbers. But if we project\ndown to 2D, we can show examples of how words are laid out by the embedding:\n\nAnd, yes, what we see does remarkably well in capturing typical everyday\nimpressions. But how can we construct such an embedding? Roughly the idea is\nto look at large amounts of text (here 5 billion words from the web) and then\nsee \u201chow similar\u201d the \u201cenvironments\u201d are in which different words appear. So,\nfor example, \u201calligator\u201d and \u201ccrocodile\u201d will often appear almost\ninterchangeably in otherwise similar sentences, and that means they\u2019ll be\nplaced nearby in the embedding. But \u201cturnip\u201d and \u201ceagle\u201d won\u2019t tend to appear\nin otherwise similar sentences, so they\u2019ll be placed far apart in the\nembedding.\n\nBut how does one actually implement something like this using neural nets?\nLet\u2019s start by talking about embeddings not for words, but for images. We want\nto find some way to characterize images by lists of numbers in such a way that\n\u201cimages we consider similar\u201d are assigned similar lists of numbers.\n\nHow do we tell if we should \u201cconsider images similar\u201d? Well, if our images\nare, say, of handwritten digits we might \u201cconsider two images similar\u201d if they\nare of the same digit. Earlier we discussed a neural net that was trained to\nrecognize handwritten digits. And we can think of this neural net as being set\nup so that in its final output it puts images into 10 different bins, one for\neach digit.\n\nBut what if we \u201cintercept\u201d what\u2019s going on inside the neural net before the\nfinal \u201cit\u2019s a \u20184\u2019\u201d decision is made? We might expect that inside the neural\nnet there are numbers that characterize images as being \u201cmostly 4-like but a\nbit 2-like\u201d or some such. And the idea is to pick up such numbers to use as\nelements in an embedding.\n\nSo here\u2019s the concept. Rather than directly trying to characterize \u201cwhat image\nis near what other image\u201d, we instead consider a well-defined task (in this\ncase digit recognition) for which we can get explicit training data\u2014then use\nthe fact that in doing this task the neural net implicitly has to make what\namount to \u201cnearness decisions\u201d. So instead of us ever explicitly having to\ntalk about \u201cnearness of images\u201d we\u2019re just talking about the concrete question\nof what digit an image represents, and then we\u2019re \u201cleaving it to the neural\nnet\u201d to implicitly determine what that implies about \u201cnearness of images\u201d.\n\nSo how in more detail does this work for the digit recognition network? We can\nthink of the network as consisting of 11 successive layers, that we might\nsummarize iconically like this (with activation functions shown as separate\nlayers):\n\nAt the beginning we\u2019re feeding into the first layer actual images, represented\nby 2D arrays of pixel values. And at the end\u2014from the last layer\u2014we\u2019re getting\nout an array of 10 values, which we can think of saying \u201chow certain\u201d the\nnetwork is that the image corresponds to each of the digits 0 through 9.\n\nFeed in the image and the values of the neurons in that last layer are:\n\nIn other words, the neural net is by this point \u201cincredibly certain\u201d that this\nimage is a 4\u2014and to actually get the output \u201c4\u201d we just have to pick out the\nposition of the neuron with the largest value.\n\nBut what if we look one step earlier? The very last operation in the network\nis a so-called softmax which tries to \u201cforce certainty\u201d. But before that\u2019s\nbeen applied the values of the neurons are:\n\nThe neuron representing \u201c4\u201d still has the highest numerical value. But there\u2019s\nalso information in the values of the other neurons. And we can expect that\nthis list of numbers can in a sense be used to characterize the \u201cessence\u201d of\nthe image\u2014and thus to provide something we can use as an embedding. And so,\nfor example, each of the 4\u2019s here has a slightly different \u201csignature\u201d (or\n\u201cfeature embedding\u201d)\u2014all very different from the 8\u2019s:\n\nHere we\u2019re essentially using 10 numbers to characterize our images. But it\u2019s\noften better to use much more than that. And for example in our digit\nrecognition network we can get an array of 500 numbers by tapping into the\npreceding layer. And this is probably a reasonable array to use as an \u201cimage\nembedding\u201d.\n\nIf we want to make an explicit visualization of \u201cimage space\u201d for handwritten\ndigits we need to \u201creduce the dimension\u201d, effectively by projecting the\n500-dimensional vector we\u2019ve got into, say, 3D space:\n\nWe\u2019ve just talked about creating a characterization (and thus embedding) for\nimages based effectively on identifying the similarity of images by\ndetermining whether (according to our training set) they correspond to the\nsame handwritten digit. And we can do the same thing much more generally for\nimages if we have a training set that identifies, say, which of 5000 common\ntypes of object (cat, dog, chair, \u2026) each image is of. And in this way we can\nmake an image embedding that\u2019s \u201canchored\u201d by our identification of common\nobjects, but then \u201cgeneralizes around that\u201d according to the behavior of the\nneural net. And the point is that insofar as that behavior aligns with how we\nhumans perceive and interpret images, this will end up being an embedding that\n\u201cseems right to us\u201d, and is useful in practice in doing \u201chuman-judgement-like\u201d\ntasks.\n\nOK, so how do we follow the same kind of approach to find embeddings for\nwords? The key is to start from a task about words for which we can readily do\ntraining. And the standard such task is \u201cword prediction\u201d. Imagine we\u2019re given\n\u201cthe ___ cat\u201d. Based on a large corpus of text (say, the text content of the\nweb), what are the probabilities for different words that might \u201cfill in the\nblank\u201d? Or, alternatively, given \u201c___ black ___\u201d what are the probabilities\nfor different \u201cflanking words\u201d?\n\nHow do we set this problem up for a neural net? Ultimately we have to\nformulate everything in terms of numbers. And one way to do this is just to\nassign a unique number to each of the 50,000 or so common words in English.\nSo, for example, \u201cthe\u201d might be 914, and \u201c cat\u201d (with a space before it) might\nbe 3542. (And these are the actual numbers used by GPT-2.) So for the \u201cthe ___\ncat\u201d problem, our input might be {914, 3542}. What should the output be like?\nWell, it should be a list of 50,000 or so numbers that effectively give the\nprobabilities for each of the possible \u201cfill-in\u201d words. And once again, to\nfind an embedding, we want to \u201cintercept\u201d the \u201cinsides\u201d of the neural net just\nbefore it \u201creaches its conclusion\u201d\u2014and then pick up the list of numbers that\noccur there, and that we can think of as \u201ccharacterizing each word\u201d.\n\nOK, so what do those characterizations look like? Over the past 10 years\nthere\u2019ve been a sequence of different systems developed (word2vec, GloVe,\nBERT, GPT, \u2026), each based on a different neural net approach. But ultimately\nall of them take words and characterize them by lists of hundreds to thousands\nof numbers.\n\nIn their raw form, these \u201cembedding vectors\u201d are quite uninformative. For\nexample, here\u2019s what GPT-2 produces as the raw embedding vectors for three\nspecific words:\n\nIf we do things like measure distances between these vectors, then we can find\nthings like \u201cnearnesses\u201d of words. Later we\u2019ll discuss in more detail what we\nmight consider the \u201ccognitive\u201d significance of such embeddings. But for now\nthe main point is that we have a way to usefully turn words into \u201cneural-net-\nfriendly\u201d collections of numbers.\n\nBut actually we can go further than just characterizing words by collections\nof numbers; we can also do this for sequences of words, or indeed whole blocks\nof text. And inside ChatGPT that\u2019s how it\u2019s dealing with things. It takes the\ntext it\u2019s got so far, and generates an embedding vector to represent it. Then\nits goal is to find the probabilities for different words that might occur\nnext. And it represents its answer for this as a list of numbers that\nessentially give the probabilities for each of the 50,000 or so possible\nwords.\n\n(Strictly, ChatGPT does not deal with words, but rather with\n\u201ctokens\u201d\u2014convenient linguistic units that might be whole words, or might just\nbe pieces like \u201cpre\u201d or \u201cing\u201d or \u201cized\u201d. Working with tokens makes it easier\nfor ChatGPT to handle rare, compound and non-English words, and, sometimes,\nfor better or worse, to invent new words.)\n\n## Inside ChatGPT\n\nOK, so we\u2019re finally ready to discuss what\u2019s inside ChatGPT. And, yes,\nultimately, it\u2019s a giant neural net\u2014currently a version of the so-called GPT-3\nnetwork with 175 billion weights. In many ways this is a neural net very much\nlike the other ones we\u2019ve discussed. But it\u2019s a neural net that\u2019s particularly\nset up for dealing with language. And its most notable feature is a piece of\nneural net architecture called a \u201ctransformer\u201d.\n\nIn the first neural nets we discussed above, every neuron at any given layer\nwas basically connected (at least with some weight) to every neuron on the\nlayer before. But this kind of fully connected network is (presumably)\noverkill if one\u2019s working with data that has particular, known structure. And\nthus, for example, in the early stages of dealing with images, it\u2019s typical to\nuse so-called convolutional neural nets (\u201cconvnets\u201d) in which neurons are\neffectively laid out on a grid analogous to the pixels in the image\u2014and\nconnected only to neurons nearby on the grid.\n\nThe idea of transformers is to do something at least somewhat similar for\nsequences of tokens that make up a piece of text. But instead of just defining\na fixed region in the sequence over which there can be connections,\ntransformers instead introduce the notion of \u201cattention\u201d\u2014and the idea of\n\u201cpaying attention\u201d more to some parts of the sequence than others. Maybe one\nday it\u2019ll make sense to just start a generic neural net and do all\ncustomization through training. But at least as of now it seems to be critical\nin practice to \u201cmodularize\u201d things\u2014as transformers do, and probably as our\nbrains also do.\n\nOK, so what does ChatGPT (or, rather, the GPT-3 network on which it\u2019s based)\nactually do? Recall that its overall goal is to continue text in a\n\u201creasonable\u201d way, based on what it\u2019s seen from the training it\u2019s had (which\nconsists in looking at billions of pages of text from the web, etc.) So at any\ngiven point, it\u2019s got a certain amount of text\u2014and its goal is to come up with\nan appropriate choice for the next token to add.\n\nIt operates in three basic stages. First, it takes the sequence of tokens that\ncorresponds to the text so far, and finds an embedding (i.e. an array of\nnumbers) that represents these. Then it operates on this embedding\u2014in a\n\u201cstandard neural net way\u201d, with values \u201crippling through\u201d successive layers in\na network\u2014to produce a new embedding (i.e. a new array of numbers). It then\ntakes the last part of this array and generates from it an array of about\n50,000 values that turn into probabilities for different possible next tokens.\n(And, yes, it so happens that there are about the same number of tokens used\nas there are common words in English, though only about 3000 of the tokens are\nwhole words, and the rest are fragments.)\n\nA critical point is that every part of this pipeline is implemented by a\nneural network, whose weights are determined by end-to-end training of the\nnetwork. In other words, in effect nothing except the overall architecture is\n\u201cexplicitly engineered\u201d; everything is just \u201clearned\u201d from training data.\n\nThere are, however, plenty of details in the way the architecture is set\nup\u2014reflecting all sorts of experience and neural net lore. And\u2014even though\nthis is definitely going into the weeds\u2014I think it\u2019s useful to talk about some\nof those details, not least to get a sense of just what goes into building\nsomething like ChatGPT.\n\nFirst comes the embedding module. Here\u2019s a schematic Wolfram Language\nrepresentation for it for GPT-2:\n\nThe input is a vector of _n_ tokens (represented as in the previous section by\nintegers from 1 to about 50,000). Each of these tokens is converted (by a\nsingle-layer neural net) into an embedding vector (of length 768 for GPT-2 and\n12,288 for ChatGPT\u2019s GPT-3). Meanwhile, there\u2019s a \u201csecondary pathway\u201d that\ntakes the sequence of (integer) positions for the tokens, and from these\nintegers creates another embedding vector. And finally the embedding vectors\nfrom the token value and the token position are added together\u2014to produce the\nfinal sequence of embedding vectors from the embedding module.\n\nWhy does one just add the token-value and token-position embedding vectors\ntogether? I don\u2019t think there\u2019s any particular science to this. It\u2019s just that\nvarious different things have been tried, and this is one that seems to work.\nAnd it\u2019s part of the lore of neural nets that\u2014in some sense\u2014so long as the\nsetup one has is \u201croughly right\u201d it\u2019s usually possible to home in on details\njust by doing sufficient training, without ever really needing to \u201cunderstand\nat an engineering level\u201d quite how the neural net has ended up configuring\nitself.\n\nHere\u2019s what the embedding module does, operating on the string _hello hello\nhello hello hello hello hello hello hello hello bye bye bye bye bye bye bye\nbye bye bye_ :\n\nThe elements of the embedding vector for each token are shown down the page,\nand across the page we see first a run of \u201c _hello_ \u201d embeddings, followed by\na run of \u201c _bye_ \u201d ones. The second array above is the positional\nembedding\u2014with its somewhat-random-looking structure being just what \u201chappened\nto be learned\u201d (in this case in GPT-2).\n\nOK, so after the embedding module comes the \u201cmain event\u201d of the transformer: a\nsequence of so-called \u201cattention blocks\u201d (12 for GPT-2, 96 for ChatGPT\u2019s\nGPT-3). It\u2019s all pretty complicated\u2014and reminiscent of typical large hard-to-\nunderstand engineering systems, or, for that matter, biological systems. But\nanyway, here\u2019s a schematic representation of a single \u201cattention block\u201d (for\nGPT-2):\n\nWithin each such attention block there are a collection of \u201cattention heads\u201d\n(12 for GPT-2, 96 for ChatGPT\u2019s GPT-3)\u2014each of which operates independently on\ndifferent chunks of values in the embedding vector. (And, yes, we don\u2019t know\nany particular reason why it\u2019s a good idea to split up the embedding vector,\nor what the different parts of it \u201cmean\u201d; this is just one of those things\nthat\u2019s been \u201cfound to work\u201d.)\n\nOK, so what do the attention heads do? Basically they\u2019re a way of \u201clooking\nback\u201d in the sequence of tokens (i.e. in the text produced so far), and\n\u201cpackaging up the past\u201d in a form that\u2019s useful for finding the next token. In\nthe first section above we talked about using 2-gram probabilities to pick\nwords based on their immediate predecessors. What the \u201cattention\u201d mechanism in\ntransformers does is to allow \u201cattention to\u201d even much earlier words\u2014thus\npotentially capturing the way, say, verbs can refer to nouns that appear many\nwords before them in a sentence.\n\nAt a more detailed level, what an attention head does is to recombine chunks\nin the embedding vectors associated with different tokens, with certain\nweights. And so, for example, the 12 attention heads in the first attention\nblock (in GPT-2) have the following (\u201clook-back-all-the-way-to-the-beginning-\nof-the-sequence-of-tokens\u201d) patterns of \u201crecombination weights\u201d for the \u201c\n_hello_ , _bye_ \u201d string above:\n\nAfter being processed by the attention heads, the resulting \u201cre-weighted\nembedding vector\u201d (of length 768 for GPT-2 and length 12,288 for ChatGPT\u2019s\nGPT-3) is passed through a standard \u201cfully connected\u201d neural net layer. It\u2019s\nhard to get a handle on what this layer is doing. But here\u2019s a plot of the\n768\u00d7768 matrix of weights it\u2019s using (here for GPT-2):\n\nTaking 64\u00d764 moving averages, some (random-walk-ish) structure begins to\nemerge:\n\nWhat determines this structure? Ultimately it\u2019s presumably some \u201cneural net\nencoding\u201d of features of human language. But as of now, what those features\nmight be is quite unknown. In effect, we\u2019re \u201copening up the brain of ChatGPT\u201d\n(or at least GPT-2) and discovering, yes, it\u2019s complicated in there, and we\ndon\u2019t understand it\u2014even though in the end it\u2019s producing recognizable human\nlanguage.\n\nOK, so after going through one attention block, we\u2019ve got a new embedding\nvector\u2014which is then successively passed through additional attention blocks\n(a total of 12 for GPT-2; 96 for GPT-3). Each attention block has its own\nparticular pattern of \u201cattention\u201d and \u201cfully connected\u201d weights. Here for\nGPT-2 are the sequence of attention weights for the \u201chello, bye\u201d input, for\nthe first attention head:\n\nAnd here are the (moving-averaged) \u201cmatrices\u201d for the fully connected layers:\n\nCuriously, even though these \u201cmatrices of weights\u201d in different attention\nblocks look quite similar, the distributions of the sizes of weights can be\nsomewhat different (and are not always Gaussian):\n\nSo after going through all these attention blocks what is the net effect of\nthe transformer? Essentially it\u2019s to transform the original collection of\nembeddings for the sequence of tokens to a final collection. And the\nparticular way ChatGPT works is then to pick up the last embedding in this\ncollection, and \u201cdecode\u201d it to produce a list of probabilities for what token\nshould come next.\n\nSo that\u2019s in outline what\u2019s inside ChatGPT. It may seem complicated (not least\nbecause of its many inevitably somewhat arbitrary \u201cengineering choices\u201d), but\nactually the ultimate elements involved are remarkably simple. Because in the\nend what we\u2019re dealing with is just a neural net made of \u201cartificial neurons\u201d,\neach doing the simple operation of taking a collection of numerical inputs,\nand then combining them with certain weights.\n\nThe original input to ChatGPT is an array of numbers (the embedding vectors\nfor the tokens so far), and what happens when ChatGPT \u201cruns\u201d to produce a new\ntoken is just that these numbers \u201cripple through\u201d the layers of the neural\nnet, with each neuron \u201cdoing its thing\u201d and passing the result to neurons on\nthe next layer. There\u2019s no looping or \u201cgoing back\u201d. Everything just \u201cfeeds\nforward\u201d through the network.\n\nIt\u2019s a very different setup from a typical computational system\u2014like a Turing\nmachine\u2014in which results are repeatedly \u201creprocessed\u201d by the same\ncomputational elements. Here\u2014at least in generating a given token of\noutput\u2014each computational element (i.e. neuron) is used only once.\n\nBut there is in a sense still an \u201couter loop\u201d that reuses computational\nelements even in ChatGPT. Because when ChatGPT is going to generate a new\ntoken, it always \u201creads\u201d (i.e. takes as input) the whole sequence of tokens\nthat come before it, including tokens that ChatGPT itself has \u201cwritten\u201d\npreviously. And we can think of this setup as meaning that ChatGPT does\u2014at\nleast at its outermost level\u2014involve a \u201cfeedback loop\u201d, albeit one in which\nevery iteration is explicitly visible as a token that appears in the text that\nit generates.\n\nBut let\u2019s come back to the core of ChatGPT: the neural net that\u2019s being\nrepeatedly used to generate each token. At some level it\u2019s very simple: a\nwhole collection of identical artificial neurons. And some parts of the\nnetwork just consist of (\u201cfully connected\u201d) layers of neurons in which every\nneuron on a given layer is connected (with some weight) to every neuron on the\nlayer before. But particularly with its transformer architecture, ChatGPT has\nparts with more structure, in which only specific neurons on different layers\nare connected. (Of course, one could still say that \u201call neurons are\nconnected\u201d\u2014but some just have zero weight.)\n\nIn addition, there are aspects of the neural net in ChatGPT that aren\u2019t most\nnaturally thought of as just consisting of \u201chomogeneous\u201d layers. And for\nexample\u2014as the iconic summary above indicates\u2014inside an attention block there\nare places where \u201cmultiple copies are made\u201d of incoming data, each then going\nthrough a different \u201cprocessing path\u201d, potentially involving a different\nnumber of layers, and only later recombining. But while this may be a\nconvenient representation of what\u2019s going on, it\u2019s always at least in\nprinciple possible to think of \u201cdensely filling in\u201d layers, but just having\nsome weights be zero.\n\nIf one looks at the longest path through ChatGPT, there are about 400 (core)\nlayers involved\u2014in some ways not a huge number. But there are millions of\nneurons\u2014with a total of 175 billion connections and therefore 175 billion\nweights. And one thing to realize is that every time ChatGPT generates a new\ntoken, it has to do a calculation involving every single one of these weights.\nImplementationally these calculations can be somewhat organized \u201cby layer\u201d\ninto highly parallel array operations that can conveniently be done on GPUs.\nBut for each token that\u2019s produced, there still have to be 175 billion\ncalculations done (and in the end a bit more)\u2014so that, yes, it\u2019s not\nsurprising that it can take a while to generate a long piece of text with\nChatGPT.\n\nBut in the end, the remarkable thing is that all these operations\u2014individually\nas simple as they are\u2014can somehow together manage to do such a good \u201chuman-\nlike\u201d job of generating text. It has to be emphasized again that (at least so\nfar as we know) there\u2019s no \u201cultimate theoretical reason\u201d why anything like\nthis should work. And in fact, as we\u2019ll discuss, I think we have to view this\nas a\u2014potentially surprising\u2014scientific discovery: that somehow in a neural net\nlike ChatGPT\u2019s it\u2019s possible to capture the essence of what human brains\nmanage to do in generating language.\n\n## The Training of ChatGPT\n\nOK, so we\u2019ve now given an outline of how ChatGPT works once it\u2019s set up. But\nhow did it get set up? How were all those 175 billion weights in its neural\nnet determined? Basically they\u2019re the result of very large-scale training,\nbased on a huge corpus of text\u2014on the web, in books, etc.\u2014written by humans.\nAs we\u2019ve said, even given all that training data, it\u2019s certainly not obvious\nthat a neural net would be able to successfully produce \u201chuman-like\u201d text.\nAnd, once again, there seem to be detailed pieces of engineering needed to\nmake that happen. But the big surprise\u2014and discovery\u2014of ChatGPT is that it\u2019s\npossible at all. And that\u2014in effect\u2014a neural net with \u201cjust\u201d 175 billion\nweights can make a \u201creasonable model\u201d of text humans write.\n\nIn modern times, there\u2019s lots of text written by humans that\u2019s out there in\ndigital form. The public web has at least several billion human-written pages,\nwith altogether perhaps a trillion words of text. And if one includes non-\npublic webpages, the numbers might be at least 100 times larger. So far, more\nthan 5 million digitized books have been made available (out of 100 million or\nso that have ever been published), giving another 100 billion or so words of\ntext. And that\u2019s not even mentioning text derived from speech in videos, etc.\n(As a personal comparison, my total lifetime output of published material has\nbeen a bit under 3 million words, and over the past 30 years I\u2019ve written\nabout 15 million words of email, and altogether typed perhaps 50 million\nwords\u2014and in just the past couple of years I\u2019ve spoken more than 10 million\nwords on livestreams. And, yes, I\u2019ll train a bot from all of that.)\n\nBut, OK, given all this data, how does one train a neural net from it? The\nbasic process is very much as we discussed it in the simple examples above.\nYou present a batch of examples, and then you adjust the weights in the\nnetwork to minimize the error (\u201closs\u201d) that the network makes on those\nexamples. The main thing that\u2019s expensive about \u201cback propagating\u201d from the\nerror is that each time you do this, every weight in the network will\ntypically change at least a tiny bit, and there are just a lot of weights to\ndeal with. (The actual \u201cback computation\u201d is typically only a small constant\nfactor harder than the forward one.)\n\nWith modern GPU hardware, it\u2019s straightforward to compute the results from\nbatches of thousands of examples in parallel. But when it comes to actually\nupdating the weights in the neural net, current methods require one to do this\nbasically batch by batch. (And, yes, this is probably where actual brains\u2014with\ntheir combined computation and memory elements\u2014have, for now, at least an\narchitectural advantage.)\n\nEven in the seemingly simple cases of learning numerical functions that we\ndiscussed earlier, we found we often had to use millions of examples to\nsuccessfully train a network, at least from scratch. So how many examples does\nthis mean we\u2019ll need in order to train a \u201chuman-like language\u201d model? There\ndoesn\u2019t seem to be any fundamental \u201ctheoretical\u201d way to know. But in practice\nChatGPT was successfully trained on a few hundred billion words of text.\n\nSome of the text it was fed several times, some of it only once. But somehow\nit \u201cgot what it needed\u201d from the text it saw. But given this volume of text to\nlearn from, how large a network should it require to \u201clearn it well\u201d? Again,\nwe don\u2019t yet have a fundamental theoretical way to say. Ultimately\u2014as we\u2019ll\ndiscuss further below\u2014there\u2019s presumably a certain \u201ctotal algorithmic content\u201d\nto human language and what humans typically say with it. But the next question\nis how efficient a neural net will be at implementing a model based on that\nalgorithmic content. And again we don\u2019t know\u2014although the success of ChatGPT\nsuggests it\u2019s reasonably efficient.\n\nAnd in the end we can just note that ChatGPT does what it does using a couple\nhundred billion weights\u2014comparable in number to the total number of words (or\ntokens) of training data it\u2019s been given. In some ways it\u2019s perhaps surprising\n(though empirically observed also in smaller analogs of ChatGPT) that the\n\u201csize of the network\u201d that seems to work well is so comparable to the \u201csize of\nthe training data\u201d. After all, it\u2019s certainly not that somehow \u201cinside\nChatGPT\u201d all that text from the web and books and so on is \u201cdirectly stored\u201d.\nBecause what\u2019s actually inside ChatGPT are a bunch of numbers\u2014with a bit less\nthan 10 digits of precision\u2014that are some kind of distributed encoding of the\naggregate structure of all that text.\n\nPut another way, we might ask what the \u201ceffective information content\u201d is of\nhuman language and what\u2019s typically said with it. There\u2019s the raw corpus of\nexamples of language. And then there\u2019s the representation in the neural net of\nChatGPT. That representation is very likely far from the \u201calgorithmically\nminimal\u201d representation (as we\u2019ll discuss below). But it\u2019s a representation\nthat\u2019s readily usable by the neural net. And in this representation it seems\nthere\u2019s in the end rather little \u201ccompression\u201d of the training data; it seems\non average to basically take only a bit less than one neural net weight to\ncarry the \u201cinformation content\u201d of a word of training data.\n\nWhen we run ChatGPT to generate text, we\u2019re basically having to use each\nweight once. So if there are _n_ weights, we\u2019ve got of order _n_ computational\nsteps to do\u2014though in practice many of them can typically be done in parallel\nin GPUs. But if we need about _n_ words of training data to set up those\nweights, then from what we\u2019ve said above we can conclude that we\u2019ll need about\n_n_ 2 computational steps to do the training of the network\u2014which is why, with\ncurrent methods, one ends up needing to talk about billion-dollar training\nefforts.\n\n## Beyond Basic Training\n\nThe majority of the effort in training ChatGPT is spent \u201cshowing it\u201d large\namounts of existing text from the web, books, etc. But it turns out there\u2019s\nanother\u2014apparently rather important\u2014part too.\n\nAs soon as it\u2019s finished its \u201craw training\u201d from the original corpus of text\nit\u2019s been shown, the neural net inside ChatGPT is ready to start generating\nits own text, continuing from prompts, etc. But while the results from this\nmay often seem reasonable, they tend\u2014particularly for longer pieces of text\u2014to\n\u201cwander off\u201d in often rather non-human-like ways. It\u2019s not something one can\nreadily detect, say, by doing traditional statistics on the text. But it\u2019s\nsomething that actual humans reading the text easily notice.\n\nAnd a key idea in the construction of ChatGPT was to have another step after\n\u201cpassively reading\u201d things like the web: to have actual humans actively\ninteract with ChatGPT, see what it produces, and in effect give it feedback on\n\u201chow to be a good chatbot\u201d. But how can the neural net use that feedback? The\nfirst step is just to have humans rate results from the neural net. But then\nanother neural net model is built that attempts to predict those ratings. But\nnow this prediction model can be run\u2014essentially like a loss function\u2014on the\noriginal network, in effect allowing that network to be \u201ctuned up\u201d by the\nhuman feedback that\u2019s been given. And the results in practice seem to have a\nbig effect on the success of the system in producing \u201chuman-like\u201d output.\n\nIn general, it\u2019s interesting how little \u201cpoking\u201d the \u201coriginally trained\u201d\nnetwork seems to need to get it to usefully go in particular directions. One\nmight have thought that to have the network behave as if it\u2019s \u201clearned\nsomething new\u201d one would have to go in and run a training algorithm, adjusting\nweights, and so on.\n\nBut that\u2019s not the case. Instead, it seems to be sufficient to basically tell\nChatGPT something one time\u2014as part of the prompt you give\u2014and then it can\nsuccessfully make use of what you told it when it generates text. And once\nagain, the fact that this works is, I think, an important clue in\nunderstanding what ChatGPT is \u201creally doing\u201d and how it relates to the\nstructure of human language and thinking.\n\nThere\u2019s certainly something rather human-like about it: that at least once\nit\u2019s had all that pre-training you can tell it something just once and it can\n\u201cremember it\u201d\u2014at least \u201clong enough\u201d to generate a piece of text using it. So\nwhat\u2019s going on in a case like this? It could be that \u201ceverything you might\ntell it is already in there somewhere\u201d\u2014and you\u2019re just leading it to the right\nspot. But that doesn\u2019t seem plausible. Instead, what seems more likely is\nthat, yes, the elements are already in there, but the specifics are defined by\nsomething like a \u201ctrajectory between those elements\u201d and that\u2019s what you\u2019re\nintroducing when you tell it something.\n\nAnd indeed, much like for humans, if you tell it something bizarre and\nunexpected that completely doesn\u2019t fit into the framework it knows, it doesn\u2019t\nseem like it\u2019ll successfully be able to \u201cintegrate\u201d this. It can \u201cintegrate\u201d\nit only if it\u2019s basically riding in a fairly simple way on top of the\nframework it already has.\n\nIt\u2019s also worth pointing out again that there are inevitably \u201calgorithmic\nlimits\u201d to what the neural net can \u201cpick up\u201d. Tell it \u201cshallow\u201d rules of the\nform \u201cthis goes to that\u201d, etc., and the neural net will most likely be able to\nrepresent and reproduce these just fine\u2014and indeed what it \u201calready knows\u201d\nfrom language will give it an immediate pattern to follow. But try to give it\nrules for an actual \u201cdeep\u201d computation that involves many potentially\ncomputationally irreducible steps and it just won\u2019t work. (Remember that at\neach step it\u2019s always just \u201cfeeding data forward\u201d in its network, never\nlooping except by virtue of generating new tokens.)\n\nOf course, the network can learn the answer to specific \u201cirreducible\u201d\ncomputations. But as soon as there are combinatorial numbers of possibilities,\nno such \u201ctable-lookup-style\u201d approach will work. And so, yes, just like\nhumans, it\u2019s time then for neural nets to \u201creach out\u201d and use actual\ncomputational tools. (And, yes, Wolfram|Alpha and Wolfram Language are\nuniquely suitable, because they\u2019ve been built to \u201ctalk about things in the\nworld\u201d, just like the language-model neural nets.)\n\n## What Really Lets ChatGPT Work?\n\nHuman language\u2014and the processes of thinking involved in generating it\u2014have\nalways seemed to represent a kind of pinnacle of complexity. And indeed it\u2019s\nseemed somewhat remarkable that human brains\u2014with their network of a \u201cmere\u201d\n100 billion or so neurons (and maybe 100 trillion connections) could be\nresponsible for it. Perhaps, one might have imagined, there\u2019s something more\nto brains than their networks of neurons\u2014like some new layer of undiscovered\nphysics. But now with ChatGPT we\u2019ve got an important new piece of information:\nwe know that a pure, artificial neural network with about as many connections\nas brains have neurons is capable of doing a surprisingly good job of\ngenerating human language.\n\nAnd, yes, that\u2019s still a big and complicated system\u2014with about as many neural\nnet weights as there are words of text currently available out there in the\nworld. But at some level it still seems difficult to believe that all the\nrichness of language and the things it can talk about can be encapsulated in\nsuch a finite system. Part of what\u2019s going on is no doubt a reflection of the\nubiquitous phenomenon (that first became evident in the example of rule 30)\nthat computational processes can in effect greatly amplify the apparent\ncomplexity of systems even when their underlying rules are simple. But,\nactually, as we discussed above, neural nets of the kind used in ChatGPT tend\nto be specifically constructed to restrict the effect of this phenomenon\u2014and\nthe computational irreducibility associated with it\u2014in the interest of making\ntheir training more accessible.\n\nSo how is it, then, that something like ChatGPT can get as far as it does with\nlanguage? The basic answer, I think, is that language is at a fundamental\nlevel somehow simpler than it seems. And this means that ChatGPT\u2014even with its\nultimately straightforward neural net structure\u2014is successfully able to\n\u201ccapture the essence\u201d of human language and the thinking behind it. And\nmoreover, in its training, ChatGPT has somehow \u201cimplicitly discovered\u201d\nwhatever regularities in language (and thinking) make this possible.\n\nThe success of ChatGPT is, I think, giving us evidence of a fundamental and\nimportant piece of science: it\u2019s suggesting that we can expect there to be\nmajor new \u201claws of language\u201d\u2014and effectively \u201claws of thought\u201d\u2014out there to\ndiscover. In ChatGPT\u2014built as it is as a neural net\u2014those laws are at best\nimplicit. But if we could somehow make the laws explicit, there\u2019s the\npotential to do the kinds of things ChatGPT does in vastly more direct,\nefficient\u2014and transparent\u2014ways.\n\nBut, OK, so what might these laws be like? Ultimately they must give us some\nkind of prescription for how language\u2014and the things we say with it\u2014are put\ntogether. Later we\u2019ll discuss how \u201clooking inside ChatGPT\u201d may be able to give\nus some hints about this, and how what we know from building computational\nlanguage suggests a path forward. But first let\u2019s discuss two long-known\nexamples of what amount to \u201claws of language\u201d\u2014and how they relate to the\noperation of ChatGPT.\n\nThe first is the syntax of language. Language is not just a random jumble of\nwords. Instead, there are (fairly) definite grammatical rules for how words of\ndifferent kinds can be put together: in English, for example, nouns can be\npreceded by adjectives and followed by verbs, but typically two nouns can\u2019t be\nright next to each other. Such grammatical structure can (at least\napproximately) be captured by a set of rules that define how what amount to\n\u201cparse trees\u201d can be put together:\n\nChatGPT doesn\u2019t have any explicit \u201cknowledge\u201d of such rules. But somehow in\nits training it implicitly \u201cdiscovers\u201d them\u2014and then seems to be good at\nfollowing them. So how does this work? At a \u201cbig picture\u201d level it\u2019s not\nclear. But to get some insight it\u2019s perhaps instructive to look at a much\nsimpler example.\n\nConsider a \u201clanguage\u201d formed from sequences of (\u2019s and )\u2019s, with a grammar\nthat specifies that parentheses should always be balanced, as represented by a\nparse tree like:\n\nCan we train a neural net to produce \u201cgrammatically correct\u201d parenthesis\nsequences? There are various ways to handle sequences in neural nets, but\nlet\u2019s use transformer nets, as ChatGPT does. And given a simple transformer\nnet, we can start feeding it grammatically correct parenthesis sequences as\ntraining examples. A subtlety (which actually also appears in ChatGPT\u2019s\ngeneration of human language) is that in addition to our \u201ccontent tokens\u201d\n(here \u201c(\u201d and \u201c)\u201d) we have to include an \u201cEnd\u201d token, that\u2019s generated to\nindicate that the output shouldn\u2019t continue any further (i.e. for ChatGPT,\nthat one\u2019s reached the \u201cend of the story\u201d).\n\nIf we set up a transformer net with just one attention block with 8 heads and\nfeature vectors of length 128 (ChatGPT also uses feature vectors of length\n128, but has 96 attention blocks, each with 96 heads) then it doesn\u2019t seem\npossible to get it to learn much about parenthesis language. But with 2\nattention blocks, the learning process seems to converge\u2014at least after 10\nmillion or so examples have been given (and, as is common with transformer\nnets, showing yet more examples just seems to degrade its performance).\n\nSo with this network, we can do the analog of what ChatGPT does, and ask for\nprobabilities for what the next token should be\u2014in a parenthesis sequence:\n\nAnd in the first case, the network is \u201cpretty sure\u201d that the sequence can\u2019t\nend here\u2014which is good, because if it did, the parentheses would be left\nunbalanced. In the second case, however, it \u201ccorrectly recognizes\u201d that the\nsequence can end here, though it also \u201cpoints out\u201d that it\u2019s possible to\n\u201cstart again\u201d, putting down a \u201c(\u201d, presumably with a \u201c)\u201d to follow. But, oops,\neven with its 400,000 or so laboriously trained weights, it says there\u2019s a 15%\nprobability to have \u201c)\u201d as the next token\u2014which isn\u2019t right, because that\nwould necessarily lead to an unbalanced parenthesis.\n\nHere\u2019s what we get if we ask the network for the highest-probability\ncompletions for progressively longer sequences of (\u2019s:\n\nAnd, yes, up to a certain length the network does just fine. But then it\nstarts failing. It\u2019s a pretty typical kind of thing to see in a \u201cprecise\u201d\nsituation like this with a neural net (or with machine learning in general).\nCases that a human \u201ccan solve in a glance\u201d the neural net can solve too. But\ncases that require doing something \u201cmore algorithmic\u201d (e.g. explicitly\ncounting parentheses to see if they\u2019re closed) the neural net tends to somehow\nbe \u201ctoo computationally shallow\u201d to reliably do. (By the way, even the full\ncurrent ChatGPT has a hard time correctly matching parentheses in long\nsequences.)\n\nSo what does this mean for things like ChatGPT and the syntax of a language\nlike English? The parenthesis language is \u201caustere\u201d\u2014and much more of an\n\u201calgorithmic story\u201d. But in English it\u2019s much more realistic to be able to\n\u201cguess\u201d what\u2019s grammatically going to fit on the basis of local choices of\nwords and other hints. And, yes, the neural net is much better at this\u2014even\nthough perhaps it might miss some \u201cformally correct\u201d case that, well, humans\nmight miss as well. But the main point is that the fact that there\u2019s an\noverall syntactic structure to the language\u2014with all the regularity that\nimplies\u2014in a sense limits \u201chow much\u201d the neural net has to learn. And a key\n\u201cnatural-science-like\u201d observation is that the transformer architecture of\nneural nets like the one in ChatGPT seems to successfully be able to learn the\nkind of nested-tree-like syntactic structure that seems to exist (at least in\nsome approximation) in all human languages.\n\nSyntax provides one kind of constraint on language. But there are clearly\nmore. A sentence like \u201cInquisitive electrons eat blue theories for fish\u201d is\ngrammatically correct but isn\u2019t something one would normally expect to say,\nand wouldn\u2019t be considered a success if ChatGPT generated it\u2014because, well,\nwith the normal meanings for the words in it, it\u2019s basically meaningless.\n\nBut is there a general way to tell if a sentence is meaningful? There\u2019s no\ntraditional overall theory for that. But it\u2019s something that one can think of\nChatGPT as having implicitly \u201cdeveloped a theory for\u201d after being trained with\nbillions of (presumably meaningful) sentences from the web, etc.\n\nWhat might this theory be like? Well, there\u2019s one tiny corner that\u2019s basically\nbeen known for two millennia, and that\u2019s logic. And certainly in the\nsyllogistic form in which Aristotle discovered it, logic is basically a way of\nsaying that sentences that follow certain patterns are reasonable, while\nothers are not. Thus, for example, it\u2019s reasonable to say \u201cAll X are Y. This\nis not Y, so it\u2019s not an X\u201d (as in \u201cAll fishes are blue. This is not blue, so\nit\u2019s not a fish.\u201d). And just as one can somewhat whimsically imagine that\nAristotle discovered syllogistic logic by going (\u201cmachine-learning-style\u201d)\nthrough lots of examples of rhetoric, so too one can imagine that in the\ntraining of ChatGPT it will have been able to \u201cdiscover syllogistic logic\u201d by\nlooking at lots of text on the web, etc. (And, yes, while one can therefore\nexpect ChatGPT to produce text that contains \u201ccorrect inferences\u201d based on\nthings like syllogistic logic, it\u2019s a quite different story when it comes to\nmore sophisticated formal logic\u2014and I think one can expect it to fail here for\nthe same kind of reasons it fails in parenthesis matching.)\n\nBut beyond the narrow example of logic, what can be said about how to\nsystematically construct (or recognize) even plausibly meaningful text? Yes,\nthere are things like Mad Libs that use very specific \u201cphrasal templates\u201d. But\nsomehow ChatGPT implicitly has a much more general way to do it. And perhaps\nthere\u2019s nothing to be said about how it can be done beyond \u201csomehow it happens\nwhen you have 175 billion neural net weights\u201d. But I strongly suspect that\nthere\u2019s a much simpler and stronger story.\n\n## Meaning Space and Semantic Laws of Motion\n\nWe discussed above that inside ChatGPT any piece of text is effectively\nrepresented by an array of numbers that we can think of as coordinates of a\npoint in some kind of \u201clinguistic feature space\u201d. So when ChatGPT continues a\npiece of text this corresponds to tracing out a trajectory in linguistic\nfeature space. But now we can ask what makes this trajectory correspond to\ntext we consider meaningful. And might there perhaps be some kind of \u201csemantic\nlaws of motion\u201d that define\u2014or at least constrain\u2014how points in linguistic\nfeature space can move around while preserving \u201cmeaningfulness\u201d?\n\nSo what is this linguistic feature space like? Here\u2019s an example of how single\nwords (here, common nouns) might get laid out if we project such a feature\nspace down to 2D:\n\nWe saw another example above based on words representing plants and animals.\nBut the point in both cases is that \u201csemantically similar words\u201d are placed\nnearby.\n\nAs another example, here\u2019s how words corresponding to different parts of\nspeech get laid out:\n\nOf course, a given word doesn\u2019t in general just have \u201cone meaning\u201d (or\nnecessarily correspond to just one part of speech). And by looking at how\nsentences containing a word lay out in feature space, one can often \u201ctease\napart\u201d different meanings\u2014as in the example here for the word \u201ccrane\u201d (bird or\nmachine?):\n\nOK, so it\u2019s at least plausible that we can think of this feature space as\nplacing \u201cwords nearby in meaning\u201d close in this space. But what kind of\nadditional structure can we identify in this space? Is there for example some\nkind of notion of \u201cparallel transport\u201d that would reflect \u201cflatness\u201d in the\nspace? One way to get a handle on that is to look at analogies:\n\nAnd, yes, even when we project down to 2D, there\u2019s often at least a \u201chint of\nflatness\u201d, though it\u2019s certainly not universally seen.\n\nSo what about trajectories? We can look at the trajectory that a prompt for\nChatGPT follows in feature space\u2014and then we can see how ChatGPT continues\nthat:\n\nThere\u2019s certainly no \u201cgeometrically obvious\u201d law of motion here. And that\u2019s\nnot at all surprising; we fully expect this to be a considerably more\ncomplicated story. And, for example, it\u2019s far from obvious that even if there\nis a \u201csemantic law of motion\u201d to be found, what kind of embedding (or, in\neffect, what \u201cvariables\u201d) it\u2019ll most naturally be stated in.\n\nIn the picture above, we\u2019re showing several steps in the \u201ctrajectory\u201d\u2014where at\neach step we\u2019re picking the word that ChatGPT considers the most probable (the\n\u201czero temperature\u201d case). But we can also ask what words can \u201ccome next\u201d with\nwhat probabilities at a given point:\n\nAnd what we see in this case is that there\u2019s a \u201cfan\u201d of high-probability words\nthat seems to go in a more or less definite direction in feature space. What\nhappens if we go further? Here are the successive \u201cfans\u201d that appear as we\n\u201cmove along\u201d the trajectory:\n\nHere\u2019s a 3D representation, going for a total of 40 steps:\n\nAnd, yes, this seems like a mess\u2014and doesn\u2019t do anything to particularly\nencourage the idea that one can expect to identify \u201cmathematical-physics-like\u201d\n\u201csemantic laws of motion\u201d by empirically studying \u201cwhat ChatGPT is doing\ninside\u201d. But perhaps we\u2019re just looking at the \u201cwrong variables\u201d (or wrong\ncoordinate system) and if only we looked at the right one, we\u2019d immediately\nsee that ChatGPT is doing something \u201cmathematical-physics-simple\u201d like\nfollowing geodesics. But as of now, we\u2019re not ready to \u201cempirically decode\u201d\nfrom its \u201cinternal behavior\u201d what ChatGPT has \u201cdiscovered\u201d about how human\nlanguage is \u201cput together\u201d.\n\n## Semantic Grammar and the Power of Computational Language\n\nWhat does it take to produce \u201cmeaningful human language\u201d? In the past, we\nmight have assumed it could be nothing short of a human brain. But now we know\nit can be done quite respectably by the neural net of ChatGPT. Still, maybe\nthat\u2019s as far as we can go, and there\u2019ll be nothing simpler\u2014or more human\nunderstandable\u2014that will work. But my strong suspicion is that the success of\nChatGPT implicitly reveals an important \u201cscientific\u201d fact: that there\u2019s\nactually a lot more structure and simplicity to meaningful human language than\nwe ever knew\u2014and that in the end there may be even fairly simple rules that\ndescribe how such language can be put together.\n\nAs we mentioned above, syntactic grammar gives rules for how words\ncorresponding to things like different parts of speech can be put together in\nhuman language. But to deal with meaning, we need to go further. And one\nversion of how to do this is to think about not just a syntactic grammar for\nlanguage, but also a semantic one.\n\nFor purposes of syntax, we identify things like nouns and verbs. But for\npurposes of semantics, we need \u201cfiner gradations\u201d. So, for example, we might\nidentify the concept of \u201cmoving\u201d, and the concept of an \u201cobject\u201d that\n\u201cmaintains its identity independent of location\u201d. There are endless specific\nexamples of each of these \u201csemantic concepts\u201d. But for the purposes of our\nsemantic grammar, we\u2019ll just have some general kind of rule that basically\nsays that \u201cobjects\u201d can \u201cmove\u201d. There\u2019s a lot to say about how all this might\nwork (some of which I\u2019ve said before). But I\u2019ll content myself here with just\na few remarks that indicate some of the potential path forward.\n\nIt\u2019s worth mentioning that even if a sentence is perfectly OK according to the\nsemantic grammar, that doesn\u2019t mean it\u2019s been realized (or even could be\nrealized) in practice. \u201cThe elephant traveled to the Moon\u201d would doubtless\n\u201cpass\u201d our semantic grammar, but it certainly hasn\u2019t been realized (at least\nyet) in our actual world\u2014though it\u2019s absolutely fair game for a fictional\nworld.\n\nWhen we start talking about \u201csemantic grammar\u201d we\u2019re soon led to ask \u201cWhat\u2019s\nunderneath it?\u201d What \u201cmodel of the world\u201d is it assuming? A syntactic grammar\nis really just about the construction of language from words. But a semantic\ngrammar necessarily engages with some kind of \u201cmodel of the world\u201d\u2014something\nthat serves as a \u201cskeleton\u201d on top of which language made from actual words\ncan be layered.\n\nUntil recent times, we might have imagined that (human) language would be the\nonly general way to describe our \u201cmodel of the world\u201d. Already a few centuries\nago there started to be formalizations of specific kinds of things, based\nparticularly on mathematics. But now there\u2019s a much more general approach to\nformalization: computational language.\n\nAnd, yes, that\u2019s been my big project over the course of more than four decades\n(as now embodied in the Wolfram Language): to develop a precise symbolic\nrepresentation that can talk as broadly as possible about things in the world,\nas well as abstract things that we care about. And so, for example, we have\nsymbolic representations for cities and molecules and images and neural\nnetworks, and we have built-in knowledge about how to compute about those\nthings.\n\nAnd, after decades of work, we\u2019ve covered a lot of areas in this way. But in\nthe past, we haven\u2019t particularly dealt with \u201ceveryday discourse\u201d. In \u201cI\nbought two pounds of apples\u201d we can readily represent (and do nutrition and\nother computations on) the \u201ctwo pounds of apples\u201d. But we don\u2019t (quite yet)\nhave a symbolic representation for \u201cI bought\u201d.\n\nIt\u2019s all connected to the idea of semantic grammar\u2014and the goal of having a\ngeneric symbolic \u201cconstruction kit\u201d for concepts, that would give us rules for\nwhat could fit together with what, and thus for the \u201cflow\u201d of what we might\nturn into human language.\n\nBut let\u2019s say we had this \u201csymbolic discourse language\u201d. What would we do with\nit? We could start off doing things like generating \u201clocally meaningful text\u201d.\nBut ultimately we\u2019re likely to want more \u201cglobally meaningful\u201d results\u2014which\nmeans \u201ccomputing\u201d more about what can actually exist or happen in the world\n(or perhaps in some consistent fictional world).\n\nRight now in Wolfram Language we have a huge amount of built-in computational\nknowledge about lots of kinds of things. But for a complete symbolic discourse\nlanguage we\u2019d have to build in additional \u201ccalculi\u201d about general things in\nthe world: if an object moves from A to B and from B to C, then it\u2019s moved\nfrom A to C, etc.\n\nGiven a symbolic discourse language we might use it to make \u201cstandalone\nstatements\u201d. But we can also use it to ask questions about the world,\n\u201cWolfram|Alpha style\u201d. Or we can use it to state things that we \u201cwant to make\nso\u201d, presumably with some external actuation mechanism. Or we can use it to\nmake assertions\u2014perhaps about the actual world, or perhaps about some specific\nworld we\u2019re considering, fictional or otherwise.\n\nHuman language is fundamentally imprecise, not least because it isn\u2019t\n\u201ctethered\u201d to a specific computational implementation, and its meaning is\nbasically defined just by a \u201csocial contract\u201d between its users. But\ncomputational language, by its nature, has a certain fundamental\nprecision\u2014because in the end what it specifies can always be \u201cunambiguously\nexecuted on a computer\u201d. Human language can usually get away with a certain\nvagueness. (When we say \u201cplanet\u201d does it include exoplanets or not, etc.?) But\nin computational language we have to be precise and clear about all the\ndistinctions we\u2019re making.\n\nIt\u2019s often convenient to leverage ordinary human language in making up names\nin computational language. But the meanings they have in computational\nlanguage are necessarily precise\u2014and might or might not cover some particular\nconnotation in typical human language usage.\n\nHow should one figure out the fundamental \u201contology\u201d suitable for a general\nsymbolic discourse language? Well, it\u2019s not easy. Which is perhaps why little\nhas been done since the primitive beginnings Aristotle made more than two\nmillennia ago. But it really helps that today we now know so much about how to\nthink about the world computationally (and it doesn\u2019t hurt to have a\n\u201cfundamental metaphysics\u201d from our Physics Project and the idea of the\nruliad).\n\nBut what does all this mean in the context of ChatGPT? From its training\nChatGPT has effectively \u201cpieced together\u201d a certain (rather impressive)\nquantity of what amounts to semantic grammar. But its very success gives us a\nreason to think that it\u2019s going to be feasible to construct something more\ncomplete in computational language form. And, unlike what we\u2019ve so far figured\nout about the innards of ChatGPT, we can expect to design the computational\nlanguage so that it\u2019s readily understandable to humans.\n\nWhen we talk about semantic grammar, we can draw an analogy to syllogistic\nlogic. At first, syllogistic logic was essentially a collection of rules about\nstatements expressed in human language. But (yes, two millennia later) when\nformal logic was developed, the original basic constructs of syllogistic logic\ncould now be used to build huge \u201cformal towers\u201d that include, for example, the\noperation of modern digital circuitry. And so, we can expect, it will be with\nmore general semantic grammar. At first, it may just be able to deal with\nsimple patterns, expressed, say, as text. But once its whole computational\nlanguage framework is built, we can expect that it will be able to be used to\nerect tall towers of \u201cgeneralized semantic logic\u201d, that allow us to work in a\nprecise and formal way with all sorts of things that have never been\naccessible to us before, except just at a \u201cground-floor level\u201d through human\nlanguage, with all its vagueness.\n\nWe can think of the construction of computational language\u2014and semantic\ngrammar\u2014as representing a kind of ultimate compression in representing things.\nBecause it allows us to talk about the essence of what\u2019s possible, without,\nfor example, dealing with all the \u201cturns of phrase\u201d that exist in ordinary\nhuman language. And we can view the great strength of ChatGPT as being\nsomething a bit similar: because it too has in a sense \u201cdrilled through\u201d to\nthe point where it can \u201cput language together in a semantically meaningful\nway\u201d without concern for different possible turns of phrase.\n\nSo what would happen if we applied ChatGPT to underlying computational\nlanguage? The computational language can describe what\u2019s possible. But what\ncan still be added is a sense of \u201cwhat\u2019s popular\u201d\u2014based for example on reading\nall that content on the web. But then\u2014underneath\u2014operating with computational\nlanguage means that something like ChatGPT has immediate and fundamental\naccess to what amount to ultimate tools for making use of potentially\nirreducible computations. And that makes it a system that can not only\n\u201cgenerate reasonable text\u201d, but can expect to work out whatever can be worked\nout about whether that text actually makes \u201ccorrect\u201d statements about the\nworld\u2014or whatever it\u2019s supposed to be talking about.\n\n## So \u2026 What Is ChatGPT Doing, and Why Does It Work?\n\nThe basic concept of ChatGPT is at some level rather simple. Start from a huge\nsample of human-created text from the web, books, etc. Then train a neural net\nto generate text that\u2019s \u201clike this\u201d. And in particular, make it able to start\nfrom a \u201cprompt\u201d and then continue with text that\u2019s \u201clike what it\u2019s been\ntrained with\u201d.\n\nAs we\u2019ve seen, the actual neural net in ChatGPT is made up of very simple\nelements\u2014though billions of them. And the basic operation of the neural net is\nalso very simple, consisting essentially of passing input derived from the\ntext it\u2019s generated so far \u201conce through its elements\u201d (without any loops,\netc.) for every new word (or part of a word) that it generates.\n\nBut the remarkable\u2014and unexpected\u2014thing is that this process can produce text\nthat\u2019s successfully \u201clike\u201d what\u2019s out there on the web, in books, etc. And not\nonly is it coherent human language, it also \u201csays things\u201d that \u201cfollow its\nprompt\u201d making use of content it\u2019s \u201cread\u201d. It doesn\u2019t always say things that\n\u201cglobally make sense\u201d (or correspond to correct computations)\u2014because\n(without, for example, accessing the \u201ccomputational superpowers\u201d of\nWolfram|Alpha) it\u2019s just saying things that \u201csound right\u201d based on what things\n\u201csounded like\u201d in its training material.\n\nThe specific engineering of ChatGPT has made it quite compelling. But\nultimately (at least until it can use outside tools) ChatGPT is \u201cmerely\u201d\npulling out some \u201ccoherent thread of text\u201d from the \u201cstatistics of\nconventional wisdom\u201d that it\u2019s accumulated. But it\u2019s amazing how human-like\nthe results are. And as I\u2019ve discussed, this suggests something that\u2019s at\nleast scientifically very important: that human language (and the patterns of\nthinking behind it) are somehow simpler and more \u201claw like\u201d in their structure\nthan we thought. ChatGPT has implicitly discovered it. But we can potentially\nexplicitly expose it, with semantic grammar, computational language, etc.\n\nWhat ChatGPT does in generating text is very impressive\u2014and the results are\nusually very much like what we humans would produce. So does this mean ChatGPT\nis working like a brain? Its underlying artificial-neural-net structure was\nultimately modeled on an idealization of the brain. And it seems quite likely\nthat when we humans generate language many aspects of what\u2019s going on are\nquite similar.\n\nWhen it comes to training (AKA learning) the different \u201chardware\u201d of the brain\nand of current computers (as well as, perhaps, some undeveloped algorithmic\nideas) forces ChatGPT to use a strategy that\u2019s probably rather different (and\nin some ways much less efficient) than the brain. And there\u2019s something else\nas well: unlike even in typical algorithmic computation, ChatGPT doesn\u2019t\ninternally \u201chave loops\u201d or \u201crecompute on data\u201d. And that inevitably limits its\ncomputational capability\u2014even with respect to current computers, but\ndefinitely with respect to the brain.\n\nIt\u2019s not clear how to \u201cfix that\u201d and still maintain the ability to train the\nsystem with reasonable efficiency. But to do so will presumably allow a future\nChatGPT to do even more \u201cbrain-like things\u201d. Of course, there are plenty of\nthings that brains don\u2019t do so well\u2014particularly involving what amount to\nirreducible computations. And for these both brains and things like ChatGPT\nhave to seek \u201coutside tools\u201d\u2014like Wolfram Language.\n\nBut for now it\u2019s exciting to see what ChatGPT has already been able to do. At\nsome level it\u2019s a great example of the fundamental scientific fact that large\nnumbers of simple computational elements can do remarkable and unexpected\nthings. But it also provides perhaps the best impetus we\u2019ve had in two\nthousand years to understand better just what the fundamental character and\nprinciples might be of that central feature of the human condition that is\nhuman language and the processes of thinking behind it.\n\n## Thanks\n\nI\u2019ve been following the development of neural nets now for about 43 years, and\nduring that time I\u2019ve interacted with many people about them. Among them\u2014some\nfrom long ago, some from recently, and some across many years\u2014have been:\nGiulio Alessandrini, Dario Amodei, Etienne Bernard, Taliesin Beynon, Sebastian\nBodenstein, Greg Brockman, Jack Cowan, Pedro Domingos, Jesse Galef, Roger\nGermundsson, Robert Hecht-Nielsen, Geoff Hinton, John Hopfield, Yann LeCun,\nJerry Lettvin, Jerome Louradour, Marvin Minsky, Eric Mjolsness, Cayden Pierce,\nTomaso Poggio, Matteo Salvarezza, Terry Sejnowski, Oliver Selfridge, Gordon\nShaw, Jonas Sj\u00f6berg, Ilya Sutskever, Gerry Tesauro and Timothee Verdier. For\nhelp with this piece, I\u2019d particularly like to thank Giulio Alessandrini and\nBrad Klee.\n\n## Additional Resources\n\n\u201cMachine Learning for Middle Schoolers\u201d \u00bb\n\n_Introduction to Machine Learning \u00bb_\n\nWolfram U Machine Learning Courses \u00bb\n\nCite this as\n\nStephen Wolfram (2023), \"What Is ChatGPT Doing ... and Why Does It Work?,\"\nStephen Wolfram Writings. writings.stephenwolfram.com/2023/02/what-is-chatgpt-\ndoing-and-why-does-it-work.\n\nText\n\nStephen Wolfram (2023), \"What Is ChatGPT Doing ... and Why Does It Work?,\"\nStephen Wolfram Writings. writings.stephenwolfram.com/2023/02/what-is-chatgpt-\ndoing-and-why-does-it-work.\n\nCMS\n\nWolfram, Stephen. \"What Is ChatGPT Doing ... and Why Does It Work?.\" Stephen\nWolfram Writings. February 14, 2023. writings.stephenwolfram.com/2023/02/what-\nis-chatgpt-doing-and-why-does-it-work.\n\nAPA\n\nWolfram, S. (2023, February 14). What is ChatGPT doing ... and why does it\nwork?. Stephen Wolfram Writings. writings.stephenwolfram.com/2023/02/what-is-\nchatgpt-doing-and-why-does-it-work.\n\nPosted in: Artificial Intelligence, Language & Communication, New Technology\n\nPlease enter your comment (at least 5 characters).\n\nName (required)\n\nEmail (will not be published; required)\n\nPlease enter your name.\n\nPlease enter a valid email address.\n\nWebsite\n\n###  53 comments\n\n\u00ab Older Comments\n\n  1. Very impressive, succinct yet eloquent narrative to what otherwise could be relegated to alchemy or voodoo science. Thank you Steve and the Team!! Much obliged to your enlightenment!\n\nNicos Kekchidis\n\nJune 2, 2023 at 7:55 pm\n\n  2. This should be a must read for anyone who wants to understand Neural Networks \\ LLMs. Clear explanation of a complicated subject. Thank you Stephen and the team for making us a bit smarter.\n\nYuri\n\nJune 6, 2023 at 9:26 am\n\n  3. Thank you very much Stephen Wolfram & team for this meaningful, enlightening and rather long* \ud83d\ude42 article.\n\nAs a \u201claymen\u201d (run of the mill backend software engineer) I finally got the\nbasic concepts behind GPT/LLM models and AI/ML in general.  \nI very much appreciate that you left out the usual buzzword bingo of the AI/ML\ncommunity.\n\n* I appreciate that the subject matter probably makes it impossible to condense the wealth of information provided here even more than you already did and still provide the reader with the ability to understand the topics presented.\n\nGreetings from Germany\n\nPaul Schmidt\n\nJune 15, 2023 at 5:27 am\n\n\u00ab Older Comments\n\nRelated Writings\n\nThe Story Continues: Announcing Version 14 of Wolfram Language and Mathematica\n\nJanuary 9, 2024\n\nObserver Theory\n\nDecember 11, 2023\n\nHow to Think Computationally about AI, the Universe and Everything\n\nOctober 27, 2023\n\nRemembering Doug Lenat (1950\u20132023) and His Quest to Capture the World with\nLogic\n\nSeptember 5, 2023\n\nRelated Writings\n\nThe Story Continues: Announcing Version 14 of Wolfram Language and Mathematica\n\nJanuary 9, 2024\n\nObserver Theory\n\nDecember 11, 2023\n\nHow to Think Computationally about AI, the Universe and Everything\n\nOctober 27, 2023\n\nRemembering Doug Lenat (1950\u20132023) and His Quest to Capture the World with\nLogic\n\nSeptember 5, 2023\n\nRecent Writings\n\nThe Story Continues: Announcing Version 14 of Wolfram Language and Mathematica\n\nJanuary 9, 2024\n\nObserver Theory\n\nDecember 11, 2023\n\nAggregation and Tiling as Multicomputational Processes\n\nNovember 3, 2023\n\nHow to Think Computationally about AI, the Universe and Everything\n\nOctober 27, 2023\n\nExpression Evaluation and Fundamental Physics\n\nSeptember 29, 2023\n\nAll by date\n\nPopular Categories\n\n  * Artificial Intelligence\n  * Big Picture\n  * Companies & Business\n  * Computational Science\n  * Computational Thinking\n  * Data Science\n  * Education\n  * Future Perspectives\n  * Historical Perspectives\n  * Language & Communication\n  * Life & Times\n  * Life Science\n  * Mathematica\n  * Mathematics\n  * New Kind of Science\n  * New Technology\n  * Personal Analytics\n  * Philosophy\n  * Physics\n  * Ruliology\n  * Software Design\n  * Wolfram|Alpha\n  * Wolfram Language\n  * Other\n\nWritings by Year\n\n  * 2024\n  * 2023\n  * 2022\n  * 2021\n  * 2020\n  * 2019\n  * 2018\n  * 2017\n  * 2016\n  * 2015\n  * 2014\n  * 2013\n  * 2012\n  * 2011\n  * 2010\n  * 2009\n  * 2008\n  * 2007\n  * 2006\n  * 2004\n  * 2003\n  * All\n\n\u00a9 Stephen Wolfram, LLC | Open content:  (code:  )  | Terms | RSS\n\n__Enable JavaScript to interact with content and submit forms on Wolfram\nwebsites.Learn how \u00bb\n\n____\n\nThis website uses cookies to optimize your experience with our services on the\nsite, as described in our Privacy Policy.\n\nAccept & Close\n\n __ __\n\n",
    "links": "[{\"link\": \"https://twitter.com/stephen_wolfram\", \"text\": \"\"}, {\"link\": \"https://www.facebook.com/Stephen-Wolfram-188916357807416/\", \"text\": \"\"}, {\"link\": \"https://www.linkedin.com/in/stephenwolfram\", \"text\": \"\"}, {\"link\": \"https://writings.stephenwolfram.com/category/artificial-intelligence\", \"text\": \"Artificial Intelligence\"}, {\"link\": \"https://writings.stephenwolfram.com/category/big-picture\", \"text\": \"Big Picture\"}, {\"link\": \"https://writings.stephenwolfram.com/category/companies-and-business\", \"text\": \"Companies & Business\"}, {\"link\": \"https://writings.stephenwolfram.com/category/computational-science\", \"text\": \"Computational Science\"}, {\"link\": \"https://writings.stephenwolfram.com/category/computational-thinking\", \"text\": \"Computational Thinking\"}, {\"link\": \"https://writings.stephenwolfram.com/category/data-science\", \"text\": \"Data Science\"}, {\"link\": \"https://writings.stephenwolfram.com/category/education\", \"text\": \"Education\"}, {\"link\": \"https://writings.stephenwolfram.com/category/future-perspectives\", \"text\": \"Future Perspectives\"}, {\"link\": \"https://writings.stephenwolfram.com/category/historical-perspectives\", \"text\": \"Historical Perspectives\"}, {\"link\": \"https://writings.stephenwolfram.com/category/language-and-communication\", \"text\": \"Language & Communication\"}, {\"link\": \"https://writings.stephenwolfram.com/category/life-and-times\", \"text\": \"Life & Times\"}, {\"link\": \"https://writings.stephenwolfram.com/category/life-science\", \"text\": \"Life Science\"}, {\"link\": \"https://writings.stephenwolfram.com/category/mathematica\", \"text\": \"Mathematica\"}, {\"link\": \"https://writings.stephenwolfram.com/category/mathematics\", \"text\": \"Mathematics\"}, {\"link\": \"https://writings.stephenwolfram.com/category/new-kind-of-science\", \"text\": \"New Kind of Science\"}, {\"link\": \"https://writings.stephenwolfram.com/category/new-technology\", \"text\": \"New Technology\"}, {\"link\": \"https://writings.stephenwolfram.com/category/personal-analytics\", \"text\": \"Personal Analytics\"}, {\"link\": \"https://writings.stephenwolfram.com/category/philosophy\", \"text\": \"Philosophy\"}, {\"link\": \"https://writings.stephenwolfram.com/category/physics\", \"text\": \"Physics\"}, {\"link\": \"https://writings.stephenwolfram.com/category/ruliology\", \"text\": \"Ruliology\"}, {\"link\": \"https://writings.stephenwolfram.com/category/software-design\", \"text\": \"Software Design\"}, {\"link\": \"https://writings.stephenwolfram.com/category/wolfram-alpha\", \"text\": \"Wolfram|Alpha\"}, {\"link\": \"https://writings.stephenwolfram.com/category/wolfram-language\", \"text\": \"Wolfram Language\"}, {\"link\": \"https://writings.stephenwolfram.com/category/other\", \"text\": \"Other\"}, {\"link\": \"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#its-just-adding-one-word-at-a-time\", \"text\": \"In the first section above\"}, {\"link\": \"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/comment-page-1/#comments\", \"text\": \"\u00ab Older Comments\"}, {\"link\": \"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/comment-page-2/#comment-2070615\", \"text\": \"June 2, 2023 at 7:55 pm\"}, {\"link\": \"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/comment-page-2/#comment-2071934\", \"text\": \"June 6, 2023 at 9:26 am\"}, {\"link\": \"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/comment-page-2/#comment-2075053\", \"text\": \"June 15, 2023 at 5:27 am\"}, {\"link\": \"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/comment-page-1/#comments\", \"text\": \"\u00ab Older Comments\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date\", \"text\": \"All by date\"}, {\"link\": \"https://writings.stephenwolfram.com/category/artificial-intelligence\", \"text\": \"Artificial Intelligence\"}, {\"link\": \"https://writings.stephenwolfram.com/category/big-picture\", \"text\": \"Big Picture\"}, {\"link\": \"https://writings.stephenwolfram.com/category/companies-and-business\", \"text\": \"Companies & Business\"}, {\"link\": \"https://writings.stephenwolfram.com/category/computational-science\", \"text\": \"Computational Science\"}, {\"link\": \"https://writings.stephenwolfram.com/category/computational-thinking\", \"text\": \"Computational Thinking\"}, {\"link\": \"https://writings.stephenwolfram.com/category/data-science\", \"text\": \"Data Science\"}, {\"link\": \"https://writings.stephenwolfram.com/category/education\", \"text\": \"Education\"}, {\"link\": \"https://writings.stephenwolfram.com/category/future-perspectives\", \"text\": \"Future Perspectives\"}, {\"link\": \"https://writings.stephenwolfram.com/category/historical-perspectives\", \"text\": \"Historical Perspectives\"}, {\"link\": \"https://writings.stephenwolfram.com/category/language-and-communication\", \"text\": \"Language & Communication\"}, {\"link\": \"https://writings.stephenwolfram.com/category/life-and-times\", \"text\": \"Life & Times\"}, {\"link\": \"https://writings.stephenwolfram.com/category/life-science\", \"text\": \"Life Science\"}, {\"link\": \"https://writings.stephenwolfram.com/category/mathematica\", \"text\": \"Mathematica\"}, {\"link\": \"https://writings.stephenwolfram.com/category/mathematics\", \"text\": \"Mathematics\"}, {\"link\": \"https://writings.stephenwolfram.com/category/new-kind-of-science\", \"text\": \"New Kind of Science\"}, {\"link\": \"https://writings.stephenwolfram.com/category/new-technology\", \"text\": \"New Technology\"}, {\"link\": \"https://writings.stephenwolfram.com/category/personal-analytics\", \"text\": \"Personal Analytics\"}, {\"link\": \"https://writings.stephenwolfram.com/category/philosophy\", \"text\": \"Philosophy\"}, {\"link\": \"https://writings.stephenwolfram.com/category/physics\", \"text\": \"Physics\"}, {\"link\": \"https://writings.stephenwolfram.com/category/ruliology\", \"text\": \"Ruliology\"}, {\"link\": \"https://writings.stephenwolfram.com/category/software-design\", \"text\": \"Software Design\"}, {\"link\": \"https://writings.stephenwolfram.com/category/wolfram-alpha\", \"text\": \"Wolfram|Alpha\"}, {\"link\": \"https://writings.stephenwolfram.com/category/wolfram-language\", \"text\": \"Wolfram Language\"}, {\"link\": \"https://writings.stephenwolfram.com/category/other\", \"text\": \"Other\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2024\", \"text\": \"2024\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2023\", \"text\": \"2023\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2022\", \"text\": \"2022\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2021\", \"text\": \"2021\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2020\", \"text\": \"2020\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2019\", \"text\": \"2019\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2018\", \"text\": \"2018\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2017\", \"text\": \"2017\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2016\", \"text\": \"2016\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2015\", \"text\": \"2015\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2014\", \"text\": \"2014\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2013\", \"text\": \"2013\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2012\", \"text\": \"2012\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2011\", \"text\": \"2011\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2010\", \"text\": \"2010\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2009\", \"text\": \"2009\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2008\", \"text\": \"2008\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2007\", \"text\": \"2007\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2006\", \"text\": \"2006\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2004\", \"text\": \"2004\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/#2003\", \"text\": \"2003\"}, {\"link\": \"https://writings.stephenwolfram.com/all-by-date/\", \"text\": \"All\"}, {\"link\": \"https://creativecommons.org/licenses/by-sa/4.0/\", \"text\": \"\"}, {\"link\": \"https://creativecommons.org/licenses/by-nc-sa/4.0/\", \"text\": \"\"}, {\"link\": \"https://en.wikipedia.org/wiki/Wikipedia:Citing_sources\", \"text\": \"\"}, {\"link\": \"https://writings.stephenwolfram.com/terms\", \"text\": \"Terms\"}, {\"link\": \"https://writings.stephenwolfram.com/feed/\", \"text\": \"RSS\"}, {\"link\": \"https://www.enable-javascript.com/\", \"text\": \"Learn how\u00a0\u00bb\"}]"
}