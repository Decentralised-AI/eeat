{
    "summary": "Join our workshop on \"Accelerating AI with Continuous Delivery\" on Jan 31st \u2192\n\nSemaphore\n\n  * Product\n  * Customers\n  * Resources\n  * Blog\n\nLogin Sign up\n\nBlog\n\n  * Software Engineering\n  * Product News\n  * Podcast\n  * Greatest Hits\n\nwrite with us\n\nSearch over 600 articles\n\n\u0394\n\nJoin our workshop on \"Accelerating AI with Continuous Delivery\" on Jan 31st \u2192\n\nSemaphore\n\n  * Product\n  * Customers\n  * Resources\n  * Blog\n  * Sign up\n\nSearch for:\n\n12 Jul 2023 \u00b7 Software Engineering\n\n# Word Embeddings: Giving Your ChatBot Context For Better Answers\n\nWritten by:\n\nTomas Fernandez\n\nTomas Fernandez\n\nI picked up most of my skills during the years I worked at IBM. Was a DBA,\ndeveloper, and cloud engineer for a time. After that, I went into freelancing,\nwhere I found the passion for writing. Now, I'm a full-time writer at\nSemaphore.\n\nReviewed by:\n\nDan Ackerson\n\nDan Ackerson\n\nI picked up most of my soft/hardware troubleshooting skills in the US Army. A\ndecade of Java development drove me to operations, scaling infrastructure to\ncope with the thundering herd. Engineering coach and CTO of Teleclinic.\n\n22 min read\n\nShare this\n\n  *   *   * \n\nContents\n\nThere is no doubt that OpenAI\u2019s ChatGPT is exceptionally intelligent \u2014 it has\npassed the lawyer\u2019s bar test, it possesses knowledge akin to a doctor, and\nsome tests have clocked its IQ at 155. However, it tends to fabricate\ninformation instead of conceding ignorance. This tendency, coupled with the\nfact that its knowledge ceases in 2021, poses challenges in building\nspecialized products using the GPT API. How can we surmount these obstacles?\nHow can we impart new knowledge to a model like GPT-3? My goal is to address\nthese questions by constructing a question-answering bot employing Python, the\nOpenAI API, and word embeddings.\n\n## What I Will Be Building\n\nI intend to create a bot that generates continuous integration pipelines from\na prompt, which, as you may know, are formated with YAML in Semaphore CI/CD.\n\nHere is an example of the bot in action:\n\nScreenshot of the running program. On the screen, the command `python query.py\n\"Create a CI pipeline that builds and uploads a Docker image to Docker Hub\"`\nis executed, and the program prints out YAML corresponding to a CI pipeline\nthat performs the requested action.\n\n\u200b\n\nIn the spirit of projects like DocsGPT, My AskAI, and Libraria, I plan to\n\u201cteach\u201d the GPT-3 model about Semaphore and how to generate pipeline\nconfiguration files. I will achieve this by leveraging the existing\ndocumentation.\n\nI will not assume prior knowledge of bot building and will maintain clean code\nso that you can adapt it to your requirements.\n\n## Prerequisites\n\nYou do not need experience in coding a bot or knowledge of neural networks to\nfollow this tutorial. However, you will need:\n\n  * Python 3.\n  * A Pinecone account (sign up for the Starter plan for free).\n  * An OpenAI API Key (paid, requires a credit card); new users can experiment with $5 in free credit during the first 3 months.\n\n## But ChatGPT Can\u2019t Learn, Can It?\n\nChatGPT, or more accurately, GPT-3 and GPT-4, the Large Language Models (LLMs)\npowering them, have been trained on a massive dataset with a cutoff date\naround September 2021.\n\nIn essence, GPT-3 knows very little about events beyond that date. We can\nverify this with a simple prompt:\n\n\u200b\n\nChatGPT doesn\u2019t know who won the World Cup in 2022.\n\n\u200b\n\nWhile some OpenAI models can undergo fine-tuning, the more advanced models,\nsuch as the ones were interested in, cannot; we cannot augment their training\ndata.\n\nHow can we get answers from GPT-3 beyond its training data? We can exploit its\ntext comprehension abilities by enhancing the prompt with relevant context. In\nthe example below, I provide context from FIFA\u2019s official site, and the\nresponse now looks much better:\n\n\u200b\n\nWith the supplied context, ChatGPT can answer accurately.\n\n\u200b\n\nWe can deduce from this that the model can respond to any prompt if given\nenough relevant context. The question remains: how can we supply relevant\ncontext given any arbitrary prompt? To address this, we need to explore what\n**word embeddings** are.\n\n## What Are Word Embeddings?\n\nIn the context of language models, an embedding is a way of representing\nwords, sentences, or entire documents as vectors or lists of numbers.\n\nTo calculate word embeddings, we will need a neural network such as word2vec\nor text-embedding-ada-002. These networks have been trained on massive amounts\nof text and can find relationships between words by analyzing the patterns\nthat appear in their training data.\n\nLet\u2019s say we have the following words:\n\n  * Cat\n  * Dog\n  * Ball\n  * House\n\nImagine we use one of these word embedding networks to calculate the vectors\nfor each word. For example:\n\nWord| Vector| Context  \n---|---|---  \nCat| [0.1, 0.2, 0.3, 0.4, 0.5]| Animals, objects, small things  \nDog| [0.6, 0.7, 0.8, 0.9, 1.0]| Animals, objects, large things  \nBall| [0.2, 0.4, 0.6, 0.8, 1.0]| Objects, toys, small things  \nHouse| [0.3, 0.6, 0.9, 1.2, 1.5]| Buildings, homes, large things  \n  \nOnce we have the vectors for each word, we can use them to represent the\nmeaning of the text. For example, the sentence \u201cThe cat chased the ball\u201d can\nbe represented as the vector [0.1, 0.2, 0.3, 0.4, 0.5] + [0.2, 0.4, 0.6, 0.8,\n1.0] = [0.3, 0.6, 0.9, 1.2, 1.5]. This vector represents a sentence that is\nabout an animal chasing an object.\n\nWord embeddings can be visualized as multidimensional spaces where words or\nsentences with similar meanings are close together. We can compute the\n\u201cdistance\u201d between vectors to find similar meanings for any input text.\n\n\u200b\n\n3D representation of word embeddings as vector spaces. In reality, these\nspaces can have hundreds or thousands of dimensions. Source: Meet AI\u2019s\nMultitool: Vector Embeddings\n\n\u200b\n\nThe actual mathematics behind all this is beyond me. However, the key takeaway\nis that **vector operations allow us to manipulate or determine meaning using\nmaths**. Take the vector that represents the word \u201cqueen,\u201d subtract \u201cwoman\u201d\nand add the \u201cman\u201d vector. The result should be a vector in the vicinity of\n\u201cking.\u201d If we add \u201cson\u201d instead, we should get somewhere close to \u201cprince.\u201d\n\n### Embedding Neural Networks with Tokens\n\nSo far, we have discussed embedding neural networks taking words as inputs and\nnumbers as outputs. However, many modern networks have moved from processing\nwords to processing tokens.\n\nA **token** is the smallest unit of text that can be processed by the model.\nTokens can be words, characters, punctuation marks, symbols, or parts of\nwords.\n\nWe can see how words are converted to tokens by experimenting with the OpenAI\nonline tokenizer, which uses Byte-Pair Encoding (BPE) to convert text to\ntokens and represent each one with a number:\n\n\u200b\n\nThere is often a 1-to-1 relationship between tokens and words. Most tokens\ninclude the word and a leading space. However, there are special cases like\n\u201cembedding,\u201d which consists of two tokens, \u201cembed\u201d and \u201cding,\u201d or\n\u201ccapabilities,\u201d which consists of four tokens. If you click \u201cToken IDs,\u201d you\ncan see the model\u2019s numeric representation of each token.\n\n\u200b\n\n## Designing a Smarter Bot Using Word Embeddings\n\nNow that we have an understanding of what embeddings are, the next question\nis: **how can they help us build a smarter bot?**\n\nFirst, let\u2019s consider what happens when we use the GPT-3 API directly. The\nuser issues a prompt, and the model responds to the best of its ability.\n\nDiagram showing interaction between user and GPT-3. User sends a prompt, the\nmodel responds.\n\nHowever, when we add context to the equation, things change. For example, when\nI asked ChatGPT about the winner of the World Cup after providing context, it\nmade all the difference.\n\nSo, the plan to build a smarter bot is as follows:\n\n  1. Intercept the user\u2019s prompt.\n  2. Calculate the word embeddings for that prompt, yielding a vector.\n  3. Search a database for documents near the vector, as they should be semantically relevant to the initial prompt.\n  4. Send the original prompt to GPT-3, along with any relevant context.\n  5. Forward GPT-3\u2019s response to the user.\n\nLet\u2019s begin like most projects do: with the database.\n\n## Creating a Knowledge Database with Word Embeddings\n\nAs usual, you can find the code for this tutorial in the following repository:\n\ntomfern / semaphore-demo-pipelinegpt\n\nOur context database must include the original documentation and their\nrespective vectors. In principle, we can employ any type of database for this\ntask, but a **vector database** is the optimal tool for the job.\n\nVector databases are specialized databases designed to store and retrieve\nhigh-dimensional vector data. Instead of employing a query language such as\nSQL for searching, we supply a vector and request the N closest neighbors.\n\nTo generate the vectors, we will use text-embedding-ada-002 from OpenAI, as it\nis the fastest and most cost-effective model on offer. The model converts the\ninput text into tokens and uses an attention mechanism known as transformer to\nlearn their relationships. The output of this neural network is vectors\nrepresenting the meaning of the text.\n\nTo create a context database, I will:\n\n  1. Collect all the source documentation.\n  2. Filter out irrelevant documents.\n  3. Calculate the embeddings for each document.\n  4. Store the vectors, original text, and any other relevant metadata in the database.\n\n### Converting Documents into Vectors\n\nFirst, I must initialize an environment file with the OpenAI API key. This\nfile should never be committed to version control, as the API key is private\nand tied to your account.\n\n    \n    \n    export OPENAI_API_KEY=YOUR_API_KEY\n\nNext, let\u2019s create a virtualenv for the Python application:\n\n    \n    \n    $ virtualenv venv\n    $ source venv/bin/activate\n    $ source .env\n\nAnd install the OpenAI package:\n\n    \n    \n    $ pip install openai numpy\n\nLet\u2019s try calculating the embedding for the string \u201cDocker Container\u201d. You can\nrun this on the Python REPL or as a Python script:\n\n    \n    \n    $ python\n    \n    >>> import openai\n    \n    >>> embeddings = openai.Embedding.create(input=\"Docker Containers\", engine=\"text-embedding-ada-002\")\n    \n    >>> embeddings\n    \n    <OpenAIObject list at 0x105ede9f0> JSON: {\n      \"data\": [\n        {\n          \"embedding\": [\n        -0.00530336843803525,\n        0.0013223182177171111,\n        \n        ... 1533 more items ...,\n        \n        -0.015645816922187805\n        ],\n          \"index\": 0,\n          \"object\": \"embedding\"\n        }\n      ],\n      \"model\": \"text-embedding-ada-002-v2\",\n      \"object\": \"list\",\n      \"usage\": {\n        \"prompt_tokens\": 2,\n        \"total_tokens\": 2\n      }\n    }\n\nAs you can see, OpenAI\u2019s model responds with an `embedding` list containing\n1536 items \u2014 the vector size for the text-embedding-ada-002 network.\n\n## Storing the embeddings in Pinecone\n\nWhile there are multiple vector database engines to choose from, like Chroma\nwhich is open-source, I chose Pinecone because its a managed database with a\nfree tier, which makes things simpler. Their Starter plan is more than capable\nof handling all the data I will need.\n\nAfter creating my Pinecone account and retrieving my API key and environment,\nI add both values to my `.env` file.\n\n\u200b\n\nPinecone API Key generation screenshot\n\n\u200bNow `.env` should contain my Pinecone and OpenAI secrets.\n\n    \n    \n    export OPENAI_API_KEY=YOUR_API_KEY\n    \n    # Pinecone secrets\n    export PINECONE_API_KEY=YOUR_API_KEY\n    export PINECONE_ENVIRONMENT=YOUR_PINECONE_DATACENTER\n\nThen, I install the Pinecone client for Python:\n\n    \n    \n    $ pip install pinecone-client\n\nI need to initialize a database; these are the contents of the `db_create.py`\nscript:\n\n    \n    \n    # db_create.py\n    \n    import pinecone\n    import openai\n    import os\n    \n    index_name = \"semaphore\"\n    embed_model = \"text-embedding-ada-002\"\n    \n    api_key = os.getenv(\"PINECONE_API_KEY\")\n    env = os.getenv(\"PINECONE_ENVIRONMENT\")\n    pinecone.init(api_key=api_key, environment=env)\n    \n    embedding = openai.Embedding.create(\n        input=[\n            \"Sample document text goes here\",\n            \"there will be several phrases in each batch\"\n        ], engine=embed_model\n    )\n    \n    if index_name not in pinecone.list_indexes():\n        print(\"Creating pinecone index: \" + index_name)\n        pinecone.create_index(\n            index_name,\n            dimension=len(embedding['data'][0]['embedding']),\n            metric='cosine',\n            metadata_config={'indexed': ['source', 'id']}\n        )\n\nThe script can take a few minutes to create the database.\n\n    \n    \n    $ python db_create.py\n\nNext, I will install the tiktoken package, which is used to calculate how many\ntokens the source documents have. This is important because the embedding\nmodel can only handle up to 8191 tokens.\n\n    \n    \n    $ pip install tiktoken\n\nWhile installing packages, let\u2019s also install `tqdm` to produce a nice-looking\nprogress bar.\n\n    \n    \n    $ pip install tqdm\n\nNow I need to upload the documents to the database. The script for this will\nbe called `index_docs.py`. Let\u2019s start by importing the required modules and\ndefining some constants:\n\n    \n    \n    # index_docs.py\n    \n    # Pinecone db name and upload batch size\n    index_name = 'semaphore'\n    upsert_batch_size = 20\n    \n    # OpenAI embedding and tokenizer models\n    embed_model = \"text-embedding-ada-002\"\n    encoding_model = \"cl100k_base\"\n    max_tokens_model = 8191\n\nNext, we\u2019ll need a function to count tokens. There is a token counter example\non OpenAI page:\n\n    \n    \n    import tiktoken\n    def num_tokens_from_string(string: str) -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_model)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n\nFinally, I\u2019ll need some filtering functions to convert the original document\ninto usable examples, so I\u2019ll extract every YAML snippet from all files:\n\n    \n    \n    import re\n    def extract_yaml(text: str) -> str:\n        \"\"\"Returns list with all the YAML code blocks found in text.\"\"\"\n        matches = [m.group(1) for m in re.finditer(\"```yaml([\\w\\W]*?)```\", text)]\n        return matches\n\nI\u2019m done with the functions. Next, this will load the files in memory and\nextract the examples:\n\n    \n    \n    from tqdm import tqdm\n    import sys\n    import os\n    import pathlib\n    \n    repo_path = sys.argv[1]\n    repo_path = os.path.abspath(repo_path)\n    repo = pathlib.Path(repo_path)\n    \n    markdown_files = list(repo.glob(\"**/*.md\")) + list(\n        repo.glob(\"**/*.mdx\")\n    )\n    \n    print(f\"Extracting YAML from Markdown files in {repo_path}\")\n    new_data = []\n    for i in tqdm(range(0, len(markdown_files))):\n        markdown_file = markdown_files[i]\n        with open(markdown_file, \"r\") as f:\n            relative_path = markdown_file.relative_to(repo_path)\n            text = str(f.read())\n            if text == '':\n                continue\n            yamls = extract_yaml(text)\n            j = 0\n            for y in yamls:\n                j = j+1\n                new_data.append({\n                    \"source\": str(relative_path),\n                    \"text\": y,\n                    \"id\": f\"github.com/semaphore/docs/{relative_path}[{j}]\"\n                })\n\nAt this point, all the YAMLs should be stored in the `new_data` list. The\nfinal step is to upload the embeddings into Pinecone.\n\n    \n    \n    import pinecone\n    import openai\n    \n    api_key = os.getenv(\"PINECONE_API_KEY\")\n    env = os.getenv(\"PINECONE_ENVIRONMENT\")\n    pinecone.init(api_key=api_key, enviroment=env)\n    index = pinecone.Index(index_name)\n    \n    print(f\"Creating embeddings and uploading vectors to database\")\n    for i in tqdm(range(0, len(new_data), upsert_batch_size)):\n        \n        i_end = min(len(new_data), i+upsert_batch_size)\n        meta_batch = new_data[i:i_end]\n        ids_batch = [x['id'] for x in meta_batch]\n        texts = [x['text'] for x in meta_batch]\n    \n        embedding = openai.Embedding.create(input=texts, engine=embed_model)\n        embeds = [record['embedding'] for record in embedding['data']]\n    \n        # clean metadata before upserting\n        meta_batch = [{\n            'id': x['id'],\n            'text': x['text'],\n            'source': x['source']\n        } for x in meta_batch] \n    \n        to_upsert = list(zip(ids_batch, embeds, meta_batch))\n        index.upsert(vectors=to_upsert)\n\nAs a reference, you can find the full index_docs.py file in the demo\nrepository\n\nLet\u2019s run the index script to finish with the database setup:\n\n    \n    \n    $ git clone https://github.com/semaphoreci/docs.git /tmp/docs\n    $ source .env\n    $ python index_docs.py /tmp/docs\n\n### Testing the database\n\nThe Pinecone dashboard should show vectors in the database.\n\n\u200b\n\nScreenshot of Pinecone dashboard showing the database with a total of 79\nvectors\n\n\u200bWe can query the database with the following code, which you can run as a\nscript or in the Python REPL directly:\n\n    \n    \n    $ source .env\n    $ python\n    \n    >>> import os\n    >>> import pinecone\n    >>> import openai\n    \n    # Compute embeddings for string \"Docker Container\"\n    >>> embeddings = openai.Embedding.create(input=\"Docker Containers\", engine=\"text-embedding-ada-002\")\n    \n    \n    # Connect to database\n    >>> index_name = \"semaphore\"\n    >>> api_key = os.getenv(\"PINECONE_API_KEY\")\n    >>> env = os.getenv(\"PINECONE_ENVIRONMENT\")\n    >>> pinecone.init(api_key=api_key, environment=env)\n    >>> index = pinecone.Index(index_name)\n    \n    # Query database\n    >>> matches = index.query(embeddings['data'][0]['embedding'], top_k=1, include_metadata=True)\n    \n    >>> matches['matches'][0]\n    {'id': 'github.com/semaphore/docs/docs/ci-cd-environment/docker-authentication.md[3]',\n     'metadata': {'id': 'github.com/semaphore/docs/docs/ci-cd-environment/docker-authentication.md[3]',\n                  'source': 'docs/ci-cd-environment/docker-authentication.md',\n                  'text': '\\n'\n                          '# .semaphore/semaphore.yml\\n'\n                          'version: v1.0\\n'\n                          'name: Using a Docker image\\n'\n                          'agent:\\n'\n                          '  machine:\\n'\n                          '    type: e1-standard-2\\n'\n                          '    os_image: ubuntu1804\\n'\n                          '\\n'\n                          'blocks:\\n'\n                          '  - name: Run container from Docker Hub\\n'\n                          '    task:\\n'\n                          '      jobs:\\n'\n                          '      - name: Authenticate docker pull\\n'\n                          '        commands:\\n'\n                          '          - checkout\\n'\n                          '          - echo $DOCKERHUB_PASSWORD | docker login '\n                          '--username \"$DOCKERHUB_USERNAME\" --password-stdin\\n'\n                          '          - docker pull <repository>/<image>\\n'\n                          '          - docker images\\n'\n                          '          - docker run <repository>/<image>\\n'\n                          '      secrets:\\n'\n                          '      - name: docker-hub\\n'},\n     'score': 0.796259582,\n     'values': []}\n\nAs you can see, the first match is the YAML for a Semaphore pipeline that\npulls a Docker image and runs it. It\u2019s a good start since it\u2019s relevant to our\n\u201cDocker Containers\u201d search string. You may get a different result, but as long\nas it is Docker-related, it should be ok.\n\n## Building the bot\n\nWe have the data, and we know how to query it. Let\u2019s put it to work in the\nbot.\n\nThe steps for processing the prompt is:\n\n  1. Take the user\u2019s prompt.\n  2. Calculate its vector.\n  3. Retrieve relevant context from the database.\n  4. Send the user\u2019s prompt along with context to GPT-3.\n  5. Forward the model\u2019s response to the user.\n\nAs usual, I\u2019ll start by defining some constants in `complete.py`, the bot\u2019s\nmain script:\n\n    \n    \n    # complete.py\n    \n    # Pinecone database name, number of matched to retrieve\n    # cutoff similarity score, and how much tokens as context\n    index_name = 'semaphore'\n    context_cap_per_query = 30\n    match_min_score = 0.75\n    context_tokens_per_query = 3000\n    \n    # OpenAI LLM model parameters\n    chat_engine_model = \"gpt-3.5-turbo\"\n    max_tokens_model = 4096\n    temperature = 0.2 \n    embed_model = \"text-embedding-ada-002\"\n    encoding_model_messages = \"gpt-3.5-turbo-0301\"\n    encoding_model_strings = \"cl100k_base\"\n    \n    import pinecone\n    import os\n    \n    # Connect with Pinecone db and index\n    api_key = os.getenv(\"PINECONE_API_KEY\")\n    env = os.getenv(\"PINECONE_ENVIRONMENT\")\n    pinecone.init(api_key=api_key, environment=env)\n    index = pinecone.Index(index_name)\n\nNext, I\u2019ll add functions to count tokens as shown in the OpenAI examples. The\nfirst function counts tokens in a string, while the second counts tokens in\nmessages. We\u2019ll see messages in detail in a bit. For now, let\u2019s just say it\u2019s\na structure that keeps the state of the conversation in memory.\n\n    \n    \n    import tiktoken\n    \n    def num_tokens_from_string(string: str) -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_model_strings)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n    \n    \n    def num_tokens_from_messages(messages):\n        \"\"\"Returns the number of tokens used by a list of messages. Compatible with  model \"\"\"\n    \n        try:\n            encoding = tiktoken.encoding_for_model(encoding_model_messages)\n        except KeyError:\n            encoding = tiktoken.get_encoding(encoding_model_strings)\n    \n        num_tokens = 0\n        for message in messages:\n            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            for key, value in message.items():\n                num_tokens += len(encoding.encode(value))\n                if key == \"name\":  # if there's a name, the role is omitted\n                    num_tokens += -1  # role is always required and always 1 token\n        num_tokens += 2  # every reply is primed with <im_start>assistant\n        return num_tokens\n\nThe following function takes the original prompt and context strings to return\nan enriched prompt for GPT-3:\n\n    \n    \n    def get_prompt(query: str, context: str) -> str:\n        \"\"\"Return the prompt with query and context.\"\"\"\n        return (\n            f\"Create the continuous integration pipeline YAML code to fullfil the requested task.\\n\" +\n            f\"Below you will find some context that may help. Ignore it if it seems irrelevant.\\n\\n\" +\n            f\"Context:\\n{context}\" +\n            f\"\\n\\nTask: {query}\\n\\nYAML Code:\"\n        )\n\nThe `get_message` function formats the prompt in a format compatible with API:\n\n    \n    \n    def get_message(role: str, content: str) -> dict:\n        \"\"\"Generate a message for OpenAI API completion.\"\"\"\n        return {\"role\": role, \"content\": content}\n\nThere are three types of roles that affect how the model reacts:\n\n  * **User** : for the user\u2019s original prompt.\n  * **System** : helps set the behavior of the assistant. While there is some controversy regarding its effectiveness, it appears to be more effective when sent at the end of the messages list.\n  * **Assistant** : represents past responses of the model. The OpenAI API does not have a \u201cmemory\u201d; instead, we must send the model\u2019s previous responses back during each interaction to maintain the conversation flow.\n\nNow for the fun part. The `get_context` function takes the prompt, queries the\ndatabase, and generates a context string until one of these conditions is met:\n\n  * The complete text exceeds `context_tokens_per_query`, the space reserved for context.\n  * The search function retrieves all requested matches.\n  * Matches that have a similarity score below `match_min_score` are ignored.\n\n    \n    \n    import openai\n    \n    def get_context(query: str, max_tokens: int) -> list:\n        \"\"\"Generate message for OpenAI model. Add context until hitting `context_token_limit` limit. Returns prompt string.\"\"\"\n    \n        embeddings = openai.Embedding.create(\n            input=[query],\n            engine=embed_model\n        )\n    \n        # search the database\n        vectors = embeddings['data'][0]['embedding']\n        embeddings = index.query(vectors, top_k=context_cap_per_query, include_metadata=True)\n        matches = embeddings['matches']\n    \n        # filter and aggregate context\n        usable_context = \"\"\n        context_count = 0\n        for i in range(0, len(matches)):\n    \n            source = matches[i]['metadata']['source']\n            if matches[i]['score'] < match_min_score:\n                # skip context with low similarity score\n                continue\n                        \n            context = matches[i]['metadata']['text']\n            token_count = num_tokens_from_string(usable_context + '\\n---\\n' + context)\n    \n            if token_count < context_tokens_per_query:\n                usable_context = usable_context + '\\n---\\n' + context \n                context_count = context_count + 1\n    \n        print(f\"Found {context_count} contexts for your query\")\n    \n        return usable_context\n\nThe next and final function, `complete`, issues the API request to OpenAI and\nreturns the model\u2019s response.\n\n    \n    \n    def complete(messages):\n        \"\"\"Query the OpenAI model. Returns the first answer. \"\"\"\n    \n        res = openai.ChatCompletion.create(\n            model=chat_engine_model,\n            messages=messages,\n            temperature=temperature\n        )\n        return res.choices[0].message.content.strip()\n\nThat\u2019s all; now I only have to deal with the command line arguments and call\nthe functions in the correct order:\n\n    \n    \n    import sys\n    \n    query = sys.argv[1]\n    \n    context = get_context(query, context_tokens_per_query)\n    prompt = get_prompt(query, context)\n    \n    # initialize messages list to send to OpenAI API\n    messages = []\n    messages.append(get_message('user', prompt))\n    messages.append(get_message('system', 'You are a helpful assistant that writes YAML code for Semaphore continuous integration pipelines and explains them. Return YAML code inside code fences.'))\n    \n    if num_tokens_from_messages(messages) >= max_tokens_model:\n        raise Exception('Model token size limit reached') \n    \n    print(\"Working on your query... \")\n    answer = complete(messages)\n    print(\"Answer:\\n\")\n    print(answer)\n    messages.append(get_message('assistant', answer))\n\nIt\u2019s time to run the script and see how it fares:\n\n    \n    \n    $ python complete.py \"Create a CI pipeline that builds and uploads a Docker image to Docker Hub\"\n\nAnd the result is:\n\n    \n    \n    version: v1.0\n    name: Docker Build and Push\n    agent:\n      machine:\n        type: e1-standard-2\n        os_image: ubuntu1804\n    \n    blocks:\n      - name: \"Build and Push Docker Image\"\n        task:\n          jobs:\n            - name: \"Docker Build and Push\"\n              commands:\n                - checkout\n                - docker build -t <dockerhub-username>/<image-name>:<tag> .\n                - echo \"$DOCKERHUB_PASSWORD\" | docker login -u \"$DOCKERHUB_USERNAME\" --password-stdin\n                - docker push <dockerhub-username>/<image-name>:<tag>\n    \n    promotions:\n      - name: Deploy to production\n        pipeline_file: deploy-production.yml\n        auto_promote:\n          when: \"result = 'passed' and branch = 'master'\"\n\nWhich is a completely reasonable answer; the model has inferred the syntax\nfrom the context examples we provided.\n\nYou can find the complete project, ready to run, in the following repository:\n\ntomfern / semaphore-demo-pipelinegpt\n\n## Thoughts on Expanding the Bot\u2019s Capabilities\n\nRemember that I started with a modest goal: creating an assistant to write\nYAML pipelines. With richer content in my vector database, I can generalize\nthe bot to answer any question about Semaphore (or any product \u2014 remember\ncloning the docs into `/tmp`?).\n\nThe key to obtaining good answers is \u2014 unsurprisingly \u2014 quality context.\nMerely uploading every document into the vector database is unlikely to yield\ngood results. The context database should be concise, curated, and tagged with\ndescriptive metadata. Otherwise, we risk filling the token quota in the prompt\nwith unusable context.\n\nSo, in a sense, there is an art \u2014 and a great deal of trial and error \u2014\ninvolved in fine-tuning the bot to meet our needs. We can experiment with the\ncontext limit, remove low-quality content, summarize, and filter out\nirrelevant context by adjusting the similarity score.\n\n### Implementing a Proper Chatbot\n\nYou may have noticed that my bot does not enable us to have actual\nconversation like ChatGPT. We ask one question and get one answer.\n\nConverting the bot into a fully-fledged chatbot is, in principle, not too\nchallenging. We can maintain the conversation by resending previous responses\nto the model with each API request. Prior answers are sent back under the\n\u201cassistant\u201d role. For example:\n\n    \n    \n    messages = []\n    \n    while True:\n    \n        query = input('Type your prompt:\\n')\n        \n        context = get_context(query, context_tokens_per_query)\n        prompt = get_prompt(query, context)\n        messages.append(get_message('user', prompt))\n        messages.append(get_message('system', 'You are a helpful assistant that writes YAML code for Semaphore continuous integration pipelines and explains them. Return YAML code inside code fences.'))\n    \n        if num_tokens_from_messages(messages) >= max_tokens_model:\n            raise Exception('Model token size limit reached') \n    \n        print(\"Working on your query... \")\n        answer = complete(messages)\n        print(\"Answer:\\n\")\n        print(answer)\n        \n        # remove system message and append model's answer\n        messages.pop()  \n        messages.append(get_message('assistant', answer))\n\nUnfortunately, this implementation is rather rudimentary. It will not support\nextended conversations as the token count increases with each interaction.\nSoon enough, we will reach the 4096-token limit for GPT-3, preventing further\ndialogue.\n\nSo, we have to find some way of keeping the request within token limits. A few\nstrategies follow:\n\n  * Delete older messages. While this is the simplest solution, it limits the conversation\u2019s \u201cmemory\u201d to only the most recent messages.\n  * Summarize previous messages. We ask the model to condense earlier messages and substitute them for the original questions and answers. Though this approach increases the cost and lag between queries, it may produce superior outcomes compared to simply deleting past messages.\n  * Set a strict limit on the number of interactions.\n  * Use the GPT-4 model instead which is more powerful and has versions that support 8k (\u201cgpt-4\u201d) tokens. But bear in mind that these are slower and up to 20-30 times more expensive to run.\n  * Use a newer model like \u201cgpt-3.5-turbo-16k\u201d which can handle up to 16k tokens.\n\n## Conclusion\n\nEnhancing the bot\u2019s responses is possible with word embeddings and a good\ncontext databse. To achieve this, we need quality documentation. There is a\nsubstantial amount of trial and error involved in developing a bot that\nseemingly possesses a grasp of the subject matter.\n\nI hope this in-depth exploration of word embeddings and large language models\naids you in building a more potent bot, customized to your requirements.\n\nHappy building!\n\n### Learn CI/CD\n\nLevel up your developer skills to use CI/CD at its max.\n\nStart Learning\n\nNext post Previous post\n\n  *   *   * \n\n### Leave a Reply Cancel reply\n\nYour email address will not be published. Required fields are marked *\n\nComment *\n\nName *\n\nEmail *\n\n\u0394\n\nWriten by:\n\nTomas Fernandez\n\nI picked up most of my skills during the years I worked at IBM. Was a DBA,\ndeveloper, and cloud engineer for a time. After that, I went into freelancing,\nwhere I found the passion for writing. Now, I'm a full-time writer at\nSemaphore.\n\nReviewed by:\n\nDan Ackerson\n\nI picked up most of my soft/hardware troubleshooting skills in the US Army. A\ndecade of Java development drove me to operations, scaling infrastructure to\ncope with the thundering herd. Engineering coach and CTO of Teleclinic.\n\n### CI/CD Weekly Newsletter\n\nTutorials, interviews, and tips for you to become a well-rounded developer.\n\n\u0394\n\n###### Company\n\n  * Our Story\n  * Company\n  * Careers\n  * 2023 Year in Review\n  * Press Kit\n  * Get in touch\n  * Terms of Service\n  * Privacy Policy\n  * Security\n\n###### Product\n\n  * Product Overview\n  * Metrics & Observability\n  * Documentation\n  * Product News\n  * Semaphore Roadmap\n  * System Status\n  * Customers\n  * Premium Support\n  * Sign up\n\n###### Use Cases\n\n  * iOS\n  * Docker and Kubernetes\n  * Android\n\n###### Top Features\n\n  * Test reports\n  * Monorepos\n  * Self-hosted agents\n  * On-Premise\n\n###### Compare\n\n  * vs Jenkins\n  * vs GitHub Actions\n  * vs Travis CI\n  * vs Bitbucket Pipelines\n\n###### Resources\n\n  * Guides\n  * CI/CD Weekly Newsletter\n  * Uncut Podcast\n  * Blog\n  * Write With Us\n\n**Connect with us on Discord**\n\n\u00a9 2024 Rendered Text. All rights reserved.\n\n",
    "links": "[{\"link\": \"http://www.facebook.com/sharer.php?u=https://semaphoreci.com/blog/word-embeddings\", \"text\": \"\"}, {\"link\": \"https://twitter.com/share?url=https://semaphoreci.com/blog/word-embeddings\", \"text\": \"\"}, {\"link\": \"http://www.linkedin.com/shareArticle?mini=true&url=https://semaphoreci.com/blog/word-embeddings\", \"text\": \"\"}, {\"link\": \"http://www.facebook.com/sharer.php?u=https://semaphoreci.com/blog/word-embeddings\", \"text\": \"\"}, {\"link\": \"https://twitter.com/share?url=https://semaphoreci.com/blog/word-embeddings\", \"text\": \"\"}, {\"link\": \"http://www.linkedin.com/shareArticle?mini=true&url=https://semaphoreci.com/blog/word-embeddings\", \"text\": \"\"}, {\"link\": \"https://semaphoreci.com/blog/word-embeddings#respond\", \"text\": \"Cancel reply\"}]"
}