{
    "summary": "Jump to Content\n\n __Guides __API Reference __Changelog v2.0v3.0v4.0\n\n* * *\n\nLog In\n\n __\n\nLog In\n\nMoon (Dark Mode)Sun (Light Mode)\n\nv4.0 __\n\n __Guides __API Reference __Changelog\n\n## Overview\n\n  * What is Humanloop?\n\n## Tutorials\n\n  * Quickstart - Playground\n  * ChatGPT clone in Next.js\n  * Create your first GPT-4 App\n\n## Guides\n\n  *  __Create a project\n    * From Playground\n    * Using the SDK\n  *  __Generate and log data\n    * Generate completions\n    * Generate chat responses\n    * Capture user feedback\n    * Upload historic data\n    * Use your own model\n    * Chain multiple calls\n  *  __Run an experiment\n    * Run an experiment\n    * Run experiments managing your own model\n  * Finetune a model\n  * Manage API keys\n  * Invite collaborators\n  * Deploy to environments\n  *  __Create and use datasets\n    * Create a dataset\n    * Batch generate\n  *  __Evaluate your model\n    * Evaluate models online\n    * Evaluate models offline\n    * Set up evaluations using API\n    * Use LLMs to evaluate logs\n    * Self-hosted evaluations\n    * Evaluating with human feedback\n  *  __Integrate Tools\n    * In the Editor\n    * With the SDK\n    * With the Snippet tool\n    * Link a JSON Schema tool\n    * Set up semantic search\n\n## EXAMPLES\n\n  * Examples\n\n## Key Concepts\n\n  * Key Concepts\n\n## References\n\n  * Postman Workspace\n  * Access Roles\n  * .prompt files\n\nPowered by  __\n\n# ChatGPT clone in Next.js\n\nIn this tutorial, you'll build a simple clone of ChatGPT with Next.js and the\nHumanloop Typescript SDK.\n\n __Suggest Edits\n\nAt the end of this tutorial, you'll have built a simple ChatGPT-style\ninterface using Humanloop as the backend to manage interactions with your\nmodel provider, track user engagement and experiment with model configuration.\n\nIf you just want to leap in, the complete repo for this project is available\non GitHub here.\n\nA simple ChatGPT-style interface using the Humanloop SDK to manage interaction\nwith your model provider, track user engagement, log results and help you\nevaluate and improve your model.\n\n#\n\n1\\. Create a new model in Humanloop\n\nFirst, visit the Humanloop playground. Here, we can play with parameters and\nprompt templates to create a model which will be accessible via the Humanloop\nSDK.\n\n> ## \ud83d\udcd8\n>\n> Model Provider API keys\n>\n> If this is your first time using the Playground, you'll be prompted to enter\n> an OpenAI API key. You can create one by going here.\n\nThe playground is an interactive environment where you can experiment with\nprompt templates to create a model which will be accessible via the Humanloop\nSDK.\n\nLet's try to create a chess tutor. Paste the following _system message_ into\nthe **Chat template** box on the left-hand side.\n\n    \n    \n    You are a chess grandmaster, who is also a friendly and helpful chess instructor.\n    \n    Play a game of chess with the user. Make your own moves in reply to the student. \n    \n    Explain succintly why you made that move. Make your moves in algebraic notation.\n    \n\nIn the **Parameters** section above, select gpt-4 as the model. Click `Save`\nand give this model the name `tutorial-model-1`. You'll need to assign the\nmodel to a project. You can choose an existing project from the dropdown menu\nor creating one with the `create new project` button. To match the code\nsnippets below, create a new project called `chat-tutorial-ts`.\n\nClick the name of your organisation at the top of the page to go to the\nprojects dashboard and select the project you just created. Your new model\nconfig is visible in the table at the bottom of the project dashboard.\n\n#\n\n2\\. Set up a Next.js application\n\nNow, let's turn to building out a simple Next.js application. We'll use the\nHumanloop Typescript SDK to provide programmatic access to the model we just\ncreated.\n\nRun `npx create-next-app@latest` to create a fresh Next.js project. Accept all\nthe default config options in the setup wizard (which includes using\nTypescript, Tailwind, and the Next.js app router). Now `npm run dev` to fire\nup the development server.\n\nNext `npm i humanloop` to install the Humanloop SDK in your project.\n\nEdit `app/page.tsx` to the following. This code stubs out the basic React\ncomponents and state management we need for a chat interface.\n\npage.tsx\n\n    \n    \n    \"use client\";\n    \n    import { ChatMessage } from \"humanloop\";\n    import * as React from \"react\";\n    \n    const { useState } = React;\n    \n    export default function Home() {\n      const [messages, setMessages] = useState<ChatMessage[]>([]);\n      const [inputValue, setInputValue] = useState(\"\");\n    \n      const onSend = async () => {\n        const userMessage: ChatMessage = {\n          role: \"user\",\n          content: inputValue,\n        };\n    \n        setInputValue(\"\");\n    \n        const newMessages = [...messages, userMessage];\n    \n        setMessages(newMessages);\n    \n        // REPLACE ME LATER\n        const res = \"I'm not a language model. I'm just a string. \ud83d\ude1e\";\n        // END REPLACE ME\n    \n        const assistantMessage: ChatMessage = {\n          role: \"assistant\",\n          content: res,\n        };\n    \n        setMessages([...newMessages, assistantMessage]);\n      };\n    \n      const handleKeyDown = (e: React.KeyboardEvent<HTMLInputElement>) => {\n        if (e.key === \"Enter\") {\n          onSend();\n        }\n      };\n    \n      return (\n        <main className=\"flex flex-col items-center min-h-screen p-8 md:p-24\">\n          <h1 className=\"text-2xl font-bold leading-7 text-gray-900 sm:truncate sm:text-3xl sm:tracking-tight\">\n            Chess Tutor\n          </h1>\n          <div className=\"flex-col w-full mt-8\">\n            {messages.map((msg, idx) => (\n              <MessageRow key={idx} msg={msg}></MessageRow>\n            ))}\n    \n            <div className=\"flex w-full\">\n              <div className=\"min-w-[70px] uppercase text-xs text-gray-500 dark:text-gray-300 pt-2\">\n                User\n              </div>\n              <input\n                className=\"w-full px-4 py-1 mr-3 leading-tight text-gray-700 break-words bg-transparent border-none appearance-none dark:text-gray-200 flex-grow-1 focus:outline-none\"\n                type=\"text\"\n                placeholder=\"Type your message here...\"\n                aria-label=\"Prompt\"\n                value={inputValue}\n                onChange={(e) => setInputValue(e.target.value)}\n                onKeyDown={(e) => handleKeyDown(e)}\n              ></input>\n              <button\n                className=\"px-3 font-medium text-gray-500 uppercase border border-gray-300 rounded dark:border-gray-100 dark:text-gray-200 hover:border-blue-500 hover:text-blue-500\"\n                onClick={() => onSend()}\n              >\n                Send\n              </button>\n            </div>\n          </div>\n        </main>\n      );\n    }\n    \n    interface MessageRowProps {\n      msg: ChatMessage;\n    }\n    \n    const MessageRow: React.FC<MessageRowProps> = ({ msg }) => {\n      return (\n        <div className=\"flex pb-4 mb-4 border-b border-gray-300\">\n          <div className=\"min-w-[80px] uppercase text-xs text-gray-500 leading-tight pt-1\">\n            {msg.role}\n          </div>\n          <div className=\"pl-4 whitespace-pre-line\">{msg.content}</div>\n        </div>\n      );\n    };\n    \n    \n\n> ## \ud83d\udea7\n>\n> We shouldn't call the Humanloop SDK from the client's browser as this would\n> require giving out the Humanloop API key, which _you should not do!_\n> Instead, we'll create a simple backend API route in Next.js which can\n> perform the Humanloop requests on the Node server and proxy these back to\n> the client.\n\nCreate a file containing the code below at `app/api/chat/route.ts`. This will\nautomatically create an API route at `/api/chat`. In the call to the Humanloop\nSDK, you'll need to pass the project name you created in step 1.\n\napp/api/chat/route.ts\n\n    \n    \n    import { Humanloop, ChatMessage } from \"humanloop\";\n    \n    if (!process.env.HUMANLOOP_API_KEY) {\n      throw Error(\n        \"no Humanloop API key provided; add one to your .env.local file with: `HUMANLOOP_API_KEY=...\"\n      );\n    }\n    \n    const humanloop = new Humanloop({\n      basePath: \"https://api.humanloop.com/v4\",\n      apiKey: process.env.HUMANLOOP_API_KEY,\n    });\n    \n    export async function POST(req: Request): Promise<Response> {\n      const messages: ChatMessage[] = (await req.json()) as ChatMessage[];\n      console.log(messages);\n    \n      const response = await humanloop.chatDeployed({\n        project: \"chat-tutorial-ts\",\n        messages,\n      });\n    \n      return new Response(JSON.stringify(response.data.data[0].output));\n    }\n    \n\nIn this code, we're calling `humanloop.chatDeployed`. This function is used to\ntarget the model which is actively deployed on your project - in this case it\nshould be the model we set up in step 1. Other related functions in the SDK\nreference (such as `humanloop.chat`) allow you to target a specific model\nconfig (rather than the actively deployed one) or even specify model config\ndirectly in the function call.\n\nWhen we receive a response from Humanloop, we strip out just the text of the\nchat response and send this back to the client via a `Response` object (see\nNext.js - Route Handler docs). The Humanloop SDK response contains much more\ndata besides the raw text, which you can inspect by logging to the console.\n\nFor the above to work, you'll need to ensure that you have a `.env.local` file\nat the root of your project directory with your Humanloop API key. You can\ngenerate a Humanloop API key by clicking your name in the top right of the\nPlayground and selecting API keys. This environment variable will only be\navailable on the Next.js server, not on the client (see Next.js - Environment\nVariables).\n\n.env.local\n\n    \n    \n    HUMANLOOP_API_KEY=...\n    \n\nNow, modify `page.tsx` to use a `fetch` request against the new API route.\n\npage.tsx\n\n    \n    \n      const onSend = async () => {\n        \n    \t\t// REPLACE ME NOW\n    \t\t\n        setMessages(newMessages);\n    \n        const response = await fetch(\"/api/chat\", {\n          method: \"POST\",\n          headers: {\n            \"Content-Type\": \"application/json\",\n          },\n          body: JSON.stringify(newMessages),\n        });\n    \n        const res = await response.json();\n        \n        // END REPLACE ME\n      }\n    \n\nYou should now find that your application works as expected. When we send\nmessages from the client, a GPT response appears beneath (after a delay).\n\nBack in your Humanloop project dashboard you should see datapoints being\nlogged as clients interact with your model.\n\n#\n\n3\\. Streaming tokens\n\n(Note: requires Node version 18+).\n\nYou may notice that model responses can take a while to appear on screen.\nCurrently, our Next.js API route blocks while the entire response is\ngenerated, before finally sending the whole thing back to the client browser\nin one go. For longer generations, this can take some time, particularly with\nlarger models like GPT-4. Other model config settings can impact this too.\n\nTo provide a better user experience, we can deal with this latency by\nstreaming tokens back to the client as they are generated and have them\ndisplay eagerly on the page. The Humanloop SDK wraps the model providers'\nstreaming functionality so that we can achieve this. Let's incorporate\nstreaming tokens into our app next.\n\nEdit the API route at to look like the following. Notice that we have switched\nto using the `humanloop.chatDeployedStream` function, which offers Server Sent\nEvent streaming as new tokens arrive from the model provider.\n\napp/api/chat/route.ts\n\n    \n    \n    import { Humanloop, ChatMessage } from \"humanloop\";\n    \n    if (!process.env.HUMANLOOP_API_KEY) {\n      throw Error(\n        \"no Humanloop API key provided; add one to your .env.local file with: `HUMANLOOP_API_KEY=...\"\n      );\n    }\n    \n    const humanloop = new Humanloop({\n      basePath: \"https://api.humanloop.com/v4\",\n      apiKey: process.env.HUMANLOOP_API_KEY,\n    });\n    \n    export async function POST(req: Request): Promise<Response> {\n      const messages: ChatMessage[] = (await req.json()) as ChatMessage[];\n    \n      const response = await humanloop.chatDeployedStream({\n        project: \"chat-tutorial-ts\",\n        messages,\n      });\n    \n      return new Response(response.data);\n    }\n    \n\nNow, modify the `onSend` function in `page.tsx` to the following. This streams\nthe response body in chunks, updating the UI each time a new chunk arrives.\n\napp/page.tsx\n\n    \n    \n    const onSend = async () => {\n        const userMessage: ChatMessage = {\n          role: \"user\",\n          content: inputValue,\n        };\n    \n        setInputValue(\"\");\n    \n        const newMessages: ChatMessage[] = [\n          ...messages,\n          userMessage,\n          { role: \"assistant\", content: \"\" },\n        ];\n    \n        setMessages(newMessages);\n    \n        const response = await fetch(\"/api/chat\", {\n          method: \"POST\",\n          headers: {\n            \"Content-Type\": \"application/json\",\n          },\n          body: JSON.stringify(newMessages),\n        });\n    \n        if (!response.body) throw Error();\n    \n        const decoder = new TextDecoder();\n        const reader = response.body.getReader();\n        let done = false;\n        while (!done) {\n          const chunk = await reader.read();\n          const value = chunk.value;\n          done = chunk.done;\n          const val = decoder.decode(value);\n          const json_chunks = val\n            .split(\"}{\")\n            .map(\n              (s) =>\n                (s.startsWith(\"{\") ? \"\" : \"{\") + s + (s.endsWith(\"}\") ? \"\" : \"}\")\n            );\n          const tokens = json_chunks.map((s) => JSON.parse(s).output).join(\"\");\n    \n          setMessages((messages) => {\n            const updatedLastMessage = messages.slice(-1)[0];\n    \n            return [\n              ...messages.slice(0, -1),\n              {\n                ...updatedLastMessage,\n                content: updatedLastMessage.content + tokens,\n              },\n            ];\n          });\n        }\n      };\n    \n\nYou should now find that tokens stream onto the screen as soon as they are\navailable.\n\n#\n\n4\\. Add Feedback buttons\n\nWe'll now add feedback buttons to the Assistant chat messages, and submit\nfeedback on those datapoints via the Humanloop API whenever the user clicks\nthe buttons.\n\nModify `page.tsx` to include an id for each message in React state. Note that\nwe'll only have ids for assistant messages, and `null` for user messages.\n\npage.tsx\n\n    \n    \n    // A new type which also includes the Humanloop data_id for a message generated by the model.\n    interface ChatListItem {\n      id: string | null; // null for user messages, string for assistant messages\n      message: ChatMessage;\n    }\n    \n    export default function Home() {\n      const [chatListItems, setChatListItems] =\n          useState<ChatListItem[]>([]); // <- update to use the new type\n      ...\n    \n    \n\nModify the `onSend` function to look like this:\n\npage.tsx\n\n    \n    \n    const onSend = async () => {\n        const userMessage: ChatMessage = {\n          role: \"user\",\n          content: inputValue,\n        };\n    \n        setInputValue(\"\");\n    \n        const newItems: ChatListItem[] = [ // <- modified to update the new list type\n          ...chatListItems,\n          { message: userMessage, id: null },\n          { message: { role: \"assistant\", content: \"\" }, id: null },\n        ];\n    \n        setChatListItems(newItems);\n    \n        const response = await fetch(\"/api/chat\", {\n          method: \"POST\",\n          headers: {\n            \"Content-Type\": \"application/json\",\n          },\n          body: JSON.stringify(newItems.slice(0, -1).map((item) => item.message)), // slice off the final message, which is the currently empty placeholder for the assistant response\n        });\n    \n        if (!response.body) throw Error();\n    \n        const decoder = new TextDecoder();\n        const reader = response.body.getReader();\n        let done = false;\n        while (!done) {\n          const chunk = await reader.read();\n          const value = chunk.value;\n          done = chunk.done;\n          const val = decoder.decode(value);\n          const json_chunks = val\n            .split(\"}{\")\n            .map(\n              (s) =>\n                (s.startsWith(\"{\") ? \"\" : \"{\") + s + (s.endsWith(\"}\") ? \"\" : \"}\")\n            );\n          const tokens = json_chunks.map((s) => JSON.parse(s).output).join(\"\");\n          const id = JSON.parse(json_chunks[0]).id; // <- extract the data id from the streaming response\n    \n          setChatListItems((chatListItems) => {\n            const lastItem = chatListItems.slice(-1)[0];\n            const updatedId = id || lastItem.id; // <- use the id from the streaming response if it's not already set\n            return [\n              ...chatListItems.slice(0, -1),\n              {\n                ...lastItem,\n                message: {\n                  ...lastItem.message,\n                  content: lastItem.message.content + tokens,\n                },\n                id: updatedId, // <- include the id when we update state\n              },\n            ];\n          });\n        }\n    }\n    \n\nNow, modify the `MessageRow` component to become a `ChatItemRow` component\nwhich knows about the id.\n\npage.tsx\n\n    \n    \n    interface ChatItemRowProps {\n      item: ChatListItem;\n    }\n    \n    const ChatItemRow: React.FC<ChatItemRowProps> = ({ item }) => {\n      const onFeedback = async (feedback: string) => {\n        const response = await fetch(\"/api/feedback\", {\n          method: \"POST\",\n          headers: {\n            \"Content-Type\": \"application/json\",\n          },\n          body: JSON.stringify({ id: item.id, value: feedback }),\n        });\n      };\n    \n      return (\n        <div className=\"flex pb-4 mb-4 border-b border-gray-300\">\n          <div className=\"min-w-[80px] uppercase text-xs text-gray-500 dark:text-gray-300 leading-tight pt-1\">\n            {item.message.role}\n          </div>\n          <div className=\"pl-4 whitespace-pre-line\">{item.message.content}</div>\n          <div className=\"grow\" />\n          <div className=\"text-xs\">\n            {item.id !== null && (\n              <div className=\"flex gap-2\">\n                <button\n                  className=\"p-1 bg-gray-100 border-gray-600 rounded hover:bg-gray-200 border-1\"\n                  onClick={() => onFeedback(\"good\")}\n                >\n                  \ud83d\udc4d\n                </button>\n                <button\n                  className=\"p-1 bg-gray-100 border-gray-600 rounded hover:bg-gray-200 border-1\"\n                  onClick={() => onFeedback(\"bad\")}\n                >\n                  \ud83d\udc4e\n                </button>\n              </div>\n            )}\n          </div>\n        </div>\n      );\n    };\n    \n\nAnd finally for `page.tsx`, modify the rendering of the message history to use\nthe new component:\n\npage.tsx\n\n    \n    \n    // OLD\n    // {messages.map((msg, idx) => (\n    //   <MessageRow key={idx} msg={msg}></MessageRow>\n    // ))}\n    \n    // NEW\n    {chatListItems.map((item, idx) => (\n      <ChatItemRow key={idx} item={item}></ChatItemRow>\n    ))}\n    \n\nNext, we need to create a Next.js API route for submitting feedback, similar\nto the one we had for making a `/chat` request. Create a new file at the path\n`app/api/feedback/route.ts` with the following code:\n\napi/feedback/route.ts\n\n    \n    \n    import { Humanloop, ChatMessage } from \"humanloop\";\n    \n    if (!process.env.HUMANLOOP_API_KEY) {\n      throw Error(\n        \"no Humanloop API key provided; add one to your .env.local file with: `HUMANLOOP_API_KEY=...\"\n      );\n    }\n    \n    const humanloop = new Humanloop({\n      apiKey: process.env.HUMANLOOP_API_KEY,\n    });\n    \n    interface FeedbackRequest {\n      id: string;\n      value: string;\n    }\n    \n    export async function POST(req: Request): Promise<Response> {\n      const feedbackRequest: FeedbackRequest = await req.json();\n    \n      await humanloop.feedback({\n        type: \"rating\",\n        data_id: feedbackRequest.id,\n        value: feedbackRequest.value,\n      });\n    \n      return new Response();\n    }\n    \n\nThis code simply proxies the feedback request through the Next.js server. You\nshould now see feedback buttons on the relevant rows in chat.\n\nChat interface with feedback buttons.\n\nWhen you click one of these feedback buttons and visit the project in\nHumanloop, you should see the feedback logged against the datapoint.\n\n#\n\nConclusion\n\nYou've now built a working chat interface and used Humanloop to handle\ninteraction with the model provider and log chats. You used a system message\n(which is invisible to your end user) to make GPT-4 behave like a chess tutor.\nYou also added a way for your app's users to provide feedback which you can\ntrack in Humanloop to help improve your models.\n\nNow that you've seen how to create a simple Humanloop project and build a chat\ninterface on top of it, try visiting the Humanloop project dashboard to view\nthe logs and iterate on your model configs. You can also create experiments to\nlearn which model configs perform best with your users. To learn more about\nthese topics, take a look at our guides below.\n\nAll the code for this project is available at https://github.com/humanloop/hl-\nchatgpt-clone-typescript.\n\n __Updated 2 months ago\n\n* * *\n\n  * __Table of Contents\n  *     * 1\\. Create a new model in Humanloop\n    * 2\\. Set up a Next.js application\n    * 3\\. Streaming tokens\n    * 4\\. Add Feedback buttons\n    * Conclusion\n\n",
    "links": "[{\"link\": \"https://docs.humanloop.com/\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/docs\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/reference\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/changelog\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/login?redirect_uri=/docs/chatgpt-clone-in-nextjs\", \"text\": \"Log In\"}, {\"link\": \"https://docs.humanloop.com/\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/login?redirect_uri=/docs/chatgpt-clone-in-nextjs\", \"text\": \"Log In\"}, {\"link\": \"https://docs.humanloop.com/docs\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/reference\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/changelog\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/docs/what-is-humanloop\", \"text\": \"What is Humanloop?\"}, {\"link\": \"https://docs.humanloop.com/docs/quickstart\", \"text\": \"Quickstart - Playground\"}, {\"link\": \"https://docs.humanloop.com/docs/chatgpt-clone-in-nextjs\", \"text\": \"ChatGPT clone in Next.js\"}, {\"link\": \"https://docs.humanloop.com/docs/create-your-first-gpt-4-app\", \"text\": \"Create your first GPT-4 App\"}, {\"link\": \"https://docs.humanloop.com/docs/projects-2\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/docs/create-a-project-from-the-playground\", \"text\": \"From Playground\"}, {\"link\": \"https://docs.humanloop.com/docs/create-a-project-with-the-sdk\", \"text\": \"Using the SDK\"}, {\"link\": \"https://docs.humanloop.com/docs/generate-and-log-with-the-sdk\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/docs/completion-using-the-sdk\", \"text\": \"Generate completions\"}, {\"link\": \"https://docs.humanloop.com/docs/chat-using-the-sdk\", \"text\": \"Generate chat responses\"}, {\"link\": \"https://docs.humanloop.com/docs/capture-user-feedback-using-the-sdk\", \"text\": \"Capture user feedback\"}, {\"link\": \"https://docs.humanloop.com/docs/upload-historic-data\", \"text\": \"Upload historic data\"}, {\"link\": \"https://docs.humanloop.com/docs/use-your-own-model-provider\", \"text\": \"Use your own model\"}, {\"link\": \"https://docs.humanloop.com/docs/logging-session-traces\", \"text\": \"Chain multiple calls\"}, {\"link\": \"https://docs.humanloop.com/docs/run-an-experiment\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/docs/experiments-from-the-app\", \"text\": \"Run an experiment\"}, {\"link\": \"https://docs.humanloop.com/docs/run-an-experiment-with-your-own-model-provider\", \"text\": \"Run experiments managing your own model\"}, {\"link\": \"https://docs.humanloop.com/docs/finetune-a-model\", \"text\": \"Finetune a model\"}, {\"link\": \"https://docs.humanloop.com/docs/create-and-revoke-api-keys\", \"text\": \"Manage API keys\"}, {\"link\": \"https://docs.humanloop.com/docs/invite-collaborators\", \"text\": \"Invite collaborators\"}, {\"link\": \"https://docs.humanloop.com/docs/deploy-to-an-environment\", \"text\": \"Deploy to environments\"}, {\"link\": \"https://docs.humanloop.com/docs/datasets\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/docs/create-a-dataset\", \"text\": \"Create a dataset\"}, {\"link\": \"https://docs.humanloop.com/docs/batch-generate\", \"text\": \"Batch generate\"}, {\"link\": \"https://docs.humanloop.com/docs/evaluate-your-model\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/docs/evaluate-models-online\", \"text\": \"Evaluate models online\"}, {\"link\": \"https://docs.humanloop.com/docs/evaluate-models-offline\", \"text\": \"Evaluate models offline\"}, {\"link\": \"https://docs.humanloop.com/docs/evaluations-using-api\", \"text\": \"Set up evaluations using API\"}, {\"link\": \"https://docs.humanloop.com/docs/use-llms-to-evaluate-logs\", \"text\": \"Use LLMs to evaluate logs\"}, {\"link\": \"https://docs.humanloop.com/docs/self-hosted-evaluations\", \"text\": \"Self-hosted evaluations\"}, {\"link\": \"https://docs.humanloop.com/docs/evaluating-with-human-feedback\", \"text\": \"Evaluating with human feedback\"}, {\"link\": \"https://docs.humanloop.com/docs/integrate-tools\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/docs/create-a-tool-in-the-editor\", \"text\": \"In the Editor\"}, {\"link\": \"https://docs.humanloop.com/docs/create-a-tool-with-the-sdk\", \"text\": \"With the SDK\"}, {\"link\": \"https://docs.humanloop.com/docs/in-your-prompts\", \"text\": \"With the Snippet tool\"}, {\"link\": \"https://docs.humanloop.com/docs/link-a-jsonschema-tool\", \"text\": \"Link a JSON Schema tool\"}, {\"link\": \"https://docs.humanloop.com/docs/set-up-semantic-search\", \"text\": \"Set up semantic search\"}, {\"link\": \"https://docs.humanloop.com/docs/examples\", \"text\": \"Examples\"}, {\"link\": \"https://docs.humanloop.com/docs/key-concepts\", \"text\": \"Key Concepts\"}, {\"link\": \"https://docs.humanloop.com/docs/postman-workspace\", \"text\": \"Postman Workspace\"}, {\"link\": \"https://docs.humanloop.com/docs/access-roles\", \"text\": \"Access Roles\"}, {\"link\": \"https://docs.humanloop.com/docs/prompt-file-format\", \"text\": \".prompt files\"}, {\"link\": \"https://docs.humanloop.com/edit/chatgpt-clone-in-nextjs\", \"text\": \"\"}, {\"link\": \"https://docs.humanloop.com/reference/sdk\", \"text\": \"SDK reference\"}]",
    "priceAndPlans": "The Wayback Machine -\nhttp://web.archive.org/web/20220927165313/https://docs.humanloop.com/\n\nYour Docusaurus site did not load properly.\n\nA very common reason is a wrong site baseUrl configuration.\n\nCurrent configured baseUrl = / (default value)\n\nWe suggest trying baseUrl = /web/20220927165313/https://docs.humanloop.com/\n\nSkip to main content\n\n **Humanloop Docs**Documentation\n\nAPI Reference\n\n  * Humanloop REST API\n  * Inference API\n\n  * Overview\n\n    * The Humanloop Platform\n  * API Guides\n\n    * Getting Started with the API\n  * Reference\n\n    * Concepts\n\n  *   * Overview\n  * The Humanloop Platform\n\n# The Humanloop Platform\n\nHumanloop makes it easy for developers and ML engineers to build their own\nnatural language AI applications. Our suite of tools lets you really quickly\nget annotated data, train and then deploy models that continuously improve.\n\nYou can use the tools together for an end-to-end experience or integrate with\nother tools you know and love like Hugging Face or SpaCy.\n\n* * *\n\nTeams use Humanloop to add language AI to their applications or automate\nprocesses that require language understanding.\n\nWith Humanloop you can:\n\n  * Use Programmatic labeling and active learning to extremely rapidly get high quality annotated data.\n  * Annotate textual data with teams and have good processes for quality assurance and review.\n  * Automatically train and deploy models on infrastructure that scales to millions of predictions at low latency.\n  * Set up Human-in-the-loop processes with hybrid teams of people and AI models.\n  * Continuously improve your models\n\nNext\n\nGetting Started with the API\n\nDocs\n\n  * Overview\n\nCommunity\n\n  * Twitter\n\nMore\n\n  * Blog\n  * GitHub\n\nCopyright \u00a9 2022 Humanloop.\n\n"
}